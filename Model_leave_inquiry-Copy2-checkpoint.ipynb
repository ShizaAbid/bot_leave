{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first tokenization will be performed then stemming will be performed over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):\n",
    "        \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602, 7)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\dataset.xlsx\")\n",
    "train= train[train['Module']== 'Leave']\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.no</th>\n",
       "      <th>Data</th>\n",
       "      <th>Module</th>\n",
       "      <th>Type</th>\n",
       "      <th>sub_type</th>\n",
       "      <th>sub_type_two</th>\n",
       "      <th>sub_type_three</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>811</td>\n",
       "      <td>I am willing to take leave for today</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>812</td>\n",
       "      <td>I need a sick leave for tomorrow</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>813</td>\n",
       "      <td>I will take leave from 4th march to 8th dec</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>814</td>\n",
       "      <td>I will not join office from 6th to 9th dec</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>815</td>\n",
       "      <td>I am going on annual leaves</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     S.no                                         Data Module          Type  \\\n",
       "810   811         I am willing to take leave for today  Leave  LeaveRequest   \n",
       "811   812             I need a sick leave for tomorrow  Leave  LeaveRequest   \n",
       "812   813  I will take leave from 4th march to 8th dec  Leave  LeaveRequest   \n",
       "813   814   I will not join office from 6th to 9th dec  Leave  LeaveRequest   \n",
       "814   815                  I am going on annual leaves  Leave  LeaveRequest   \n",
       "\n",
       "       sub_type  sub_type_two sub_type_three  \n",
       "810  particular  LeaveRequest            NaN  \n",
       "811  particular  LeaveRequest            NaN  \n",
       "812  particular  LeaveRequest            NaN  \n",
       "813  particular  LeaveRequest            NaN  \n",
       "814  particular  LeaveRequest            NaN  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs[1],\"\\n\\n\",tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 421 samples, validate on 181 samples\n",
      "Epoch 1/30\n",
      "421/421 [==============================] - 7s 16ms/step - loss: 0.5147 - acc: 0.7458 - val_loss: 1.1107 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "421/421 [==============================] - 4s 9ms/step - loss: 0.4619 - acc: 0.7500 - val_loss: 1.2710 - val_acc: 0.7486\n",
      "Epoch 3/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.4447 - acc: 0.7631 - val_loss: 1.1568 - val_acc: 0.7500\n",
      "Epoch 4/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.3889 - acc: 0.8100 - val_loss: 1.3788 - val_acc: 0.5401\n",
      "Epoch 5/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.3251 - acc: 0.8640 - val_loss: 1.5744 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.2563 - acc: 0.9026 - val_loss: 1.2638 - val_acc: 0.6257\n",
      "Epoch 7/30\n",
      "421/421 [==============================] - 4s 9ms/step - loss: 0.2025 - acc: 0.9234 - val_loss: 1.9618 - val_acc: 0.5110\n",
      "Epoch 8/30\n",
      "421/421 [==============================] - 4s 8ms/step - loss: 0.1659 - acc: 0.9460 - val_loss: 1.7419 - val_acc: 0.5801\n",
      "Epoch 9/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.1292 - acc: 0.9537 - val_loss: 1.3892 - val_acc: 0.6340\n",
      "Epoch 10/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.1226 - acc: 0.9572 - val_loss: 2.0094 - val_acc: 0.5373\n",
      "Epoch 11/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.1083 - acc: 0.9673 - val_loss: 1.7875 - val_acc: 0.6022\n",
      "Epoch 12/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0896 - acc: 0.9721 - val_loss: 2.3782 - val_acc: 0.5856\n",
      "Epoch 13/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0889 - acc: 0.9691 - val_loss: 2.7568 - val_acc: 0.5580\n",
      "Epoch 14/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0760 - acc: 0.9762 - val_loss: 2.3504 - val_acc: 0.5511\n",
      "Epoch 15/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0648 - acc: 0.9822 - val_loss: 2.4933 - val_acc: 0.5704\n",
      "Epoch 16/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0579 - acc: 0.9786 - val_loss: 2.4875 - val_acc: 0.5525\n",
      "Epoch 17/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0537 - acc: 0.9840 - val_loss: 3.0313 - val_acc: 0.5594\n",
      "Epoch 18/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0579 - acc: 0.9786 - val_loss: 2.0354 - val_acc: 0.5663\n",
      "Epoch 19/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0450 - acc: 0.9840 - val_loss: 2.8488 - val_acc: 0.5566\n",
      "Epoch 20/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0365 - acc: 0.9899 - val_loss: 2.8928 - val_acc: 0.5580\n",
      "Epoch 21/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0399 - acc: 0.9893 - val_loss: 3.1122 - val_acc: 0.5470\n",
      "Epoch 22/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0376 - acc: 0.9881 - val_loss: 3.2004 - val_acc: 0.5511\n",
      "Epoch 23/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0320 - acc: 0.9857 - val_loss: 3.2563 - val_acc: 0.5608\n",
      "Epoch 24/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0417 - acc: 0.9834 - val_loss: 2.2187 - val_acc: 0.5746\n",
      "Epoch 25/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0357 - acc: 0.9899 - val_loss: 2.9870 - val_acc: 0.5677\n",
      "Epoch 26/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0325 - acc: 0.9917 - val_loss: 3.3880 - val_acc: 0.5608\n",
      "Epoch 27/30\n",
      "421/421 [==============================] - 4s 8ms/step - loss: 0.0315 - acc: 0.9881 - val_loss: 3.2715 - val_acc: 0.5552\n",
      "Epoch 28/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0261 - acc: 0.9929 - val_loss: 2.8674 - val_acc: 0.5331\n",
      "Epoch 29/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0271 - acc: 0.9905 - val_loss: 3.2475 - val_acc: 0.5497\n",
      "Epoch 30/30\n",
      "421/421 [==============================] - 3s 8ms/step - loss: 0.0183 - acc: 0.9941 - val_loss: 3.2281 - val_acc: 0.5691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2751fa6c630>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=30, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " - 6s - loss: 0.5537 - acc: 0.7500\n",
      "Epoch 2/15\n",
      " - 4s - loss: 0.5123 - acc: 0.7423\n",
      "Epoch 3/15\n",
      " - 5s - loss: 0.4409 - acc: 0.7910\n",
      "Epoch 4/15\n",
      " - 4s - loss: 0.3473 - acc: 0.8397\n",
      "Epoch 5/15\n",
      " - 4s - loss: 0.2729 - acc: 0.8913\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.2503 - acc: 0.9097\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.1793 - acc: 0.9400\n",
      "Epoch 8/15\n",
      " - 4s - loss: 0.1750 - acc: 0.9388\n",
      "Epoch 9/15\n",
      " - 5s - loss: 0.1319 - acc: 0.9632\n",
      "Epoch 10/15\n",
      " - 4s - loss: 0.1122 - acc: 0.9697\n",
      "Epoch 11/15\n",
      " - 4s - loss: 0.0949 - acc: 0.9667\n",
      "Epoch 12/15\n",
      " - 4s - loss: 0.0879 - acc: 0.9721\n",
      "Epoch 13/15\n",
      " - 3s - loss: 0.0730 - acc: 0.9762\n",
      "Epoch 14/15\n",
      " - 3s - loss: 0.0651 - acc: 0.9804\n",
      "Epoch 15/15\n",
      " - 3s - loss: 0.0645 - acc: 0.9792\n",
      "Epoch 1/15\n",
      " - 6s - loss: 0.5553 - acc: 0.7500\n",
      "Epoch 2/15\n",
      " - 4s - loss: 0.5123 - acc: 0.7524\n",
      "Epoch 3/15\n",
      " - 4s - loss: 0.4576 - acc: 0.7833\n",
      "Epoch 4/15\n",
      " - 3s - loss: 0.3624 - acc: 0.8456\n",
      "Epoch 5/15\n",
      " - 3s - loss: 0.2748 - acc: 0.8717\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.2537 - acc: 0.8884\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.2128 - acc: 0.9091\n",
      "Epoch 8/15\n",
      " - 3s - loss: 0.1825 - acc: 0.9436\n",
      "Epoch 9/15\n",
      " - 3s - loss: 0.1635 - acc: 0.9495\n",
      "Epoch 10/15\n",
      " - 3s - loss: 0.1308 - acc: 0.9567\n",
      "Epoch 11/15\n",
      " - 3s - loss: 0.1092 - acc: 0.9650\n",
      "Epoch 12/15\n",
      " - 3s - loss: 0.2149 - acc: 0.9198\n",
      "Epoch 13/15\n",
      " - 3s - loss: 0.1125 - acc: 0.9721\n",
      "Epoch 14/15\n",
      " - 3s - loss: 0.1100 - acc: 0.9691\n",
      "Epoch 15/15\n",
      " - 3s - loss: 0.0766 - acc: 0.9762\n",
      "Epoch 1/15\n",
      " - 7s - loss: 0.5514 - acc: 0.7500\n",
      "Epoch 2/15\n",
      " - 4s - loss: 0.4927 - acc: 0.7607\n",
      "Epoch 3/15\n",
      " - 3s - loss: 0.4429 - acc: 0.7892\n",
      "Epoch 4/15\n",
      " - 3s - loss: 0.3596 - acc: 0.8278\n",
      "Epoch 5/15\n",
      " - 3s - loss: 0.2900 - acc: 0.8783\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.2327 - acc: 0.9246\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.2201 - acc: 0.9222\n",
      "Epoch 8/15\n",
      " - 3s - loss: 0.1514 - acc: 0.9679\n",
      "Epoch 9/15\n",
      " - 3s - loss: 0.1300 - acc: 0.9667\n",
      "Epoch 10/15\n",
      " - 3s - loss: 0.1064 - acc: 0.9644\n",
      "Epoch 11/15\n",
      " - 3s - loss: 0.0837 - acc: 0.9721\n",
      "Epoch 12/15\n",
      " - 4s - loss: 0.0960 - acc: 0.9662\n",
      "Epoch 13/15\n",
      " - 2s - loss: 0.0742 - acc: 0.9822\n",
      "Epoch 14/15\n",
      " - 2s - loss: 0.0572 - acc: 0.9786\n",
      "Epoch 15/15\n",
      " - 2s - loss: 0.0530 - acc: 0.9810\n",
      "Epoch 1/15\n",
      " - 5s - loss: 0.5514 - acc: 0.7500\n",
      "Epoch 2/15\n",
      " - 2s - loss: 0.4872 - acc: 0.7494\n",
      "Epoch 3/15\n",
      " - 2s - loss: 0.4062 - acc: 0.8070\n",
      "Epoch 4/15\n",
      " - 2s - loss: 0.3482 - acc: 0.8456\n",
      "Epoch 5/15\n",
      " - 2s - loss: 0.2899 - acc: 0.8735\n",
      "Epoch 6/15\n",
      " - 2s - loss: 0.2344 - acc: 0.9145\n",
      "Epoch 7/15\n",
      " - 2s - loss: 0.1897 - acc: 0.9418\n",
      "Epoch 8/15\n",
      " - 2s - loss: 0.1609 - acc: 0.9537\n",
      "Epoch 9/15\n",
      " - 2s - loss: 0.1505 - acc: 0.9555\n",
      "Epoch 10/15\n",
      " - 2s - loss: 0.1089 - acc: 0.9638\n",
      "Epoch 11/15\n",
      " - 2s - loss: 0.0979 - acc: 0.9673\n",
      "Epoch 12/15\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "m = model()\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=model, epochs=15, verbose=2)\n",
    "kfold = ShuffleSplit(n_splits=15, test_size=0.3, random_state=100)\n",
    "m.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = c.evaluate(test_sequences_matrix,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "c.save('Module.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
