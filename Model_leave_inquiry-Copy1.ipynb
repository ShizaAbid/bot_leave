{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first tokenization will be performed then stemming will be performed over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):\n",
    "        \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 7)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\dataset.xlsx\")\n",
    "train= train[train['Type']== 'Leave_Approval']\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.no</th>\n",
       "      <th>Data</th>\n",
       "      <th>Module</th>\n",
       "      <th>Type</th>\n",
       "      <th>sub_type</th>\n",
       "      <th>sub_type_two</th>\n",
       "      <th>sub_type_three</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>1280</td>\n",
       "      <td>approve person leave on my behalf</td>\n",
       "      <td>Leave</td>\n",
       "      <td>Leave_Approval</td>\n",
       "      <td>particular</td>\n",
       "      <td>approve</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>1281</td>\n",
       "      <td>approve person leave</td>\n",
       "      <td>Leave</td>\n",
       "      <td>Leave_Approval</td>\n",
       "      <td>particular</td>\n",
       "      <td>approve</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>1282</td>\n",
       "      <td>approve person leave request</td>\n",
       "      <td>Leave</td>\n",
       "      <td>Leave_Approval</td>\n",
       "      <td>particular</td>\n",
       "      <td>approve</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>1283</td>\n",
       "      <td>approve person leaves</td>\n",
       "      <td>Leave</td>\n",
       "      <td>Leave_Approval</td>\n",
       "      <td>particular</td>\n",
       "      <td>approve</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>1284</td>\n",
       "      <td>kindly approve person leave request</td>\n",
       "      <td>Leave</td>\n",
       "      <td>Leave_Approval</td>\n",
       "      <td>particular</td>\n",
       "      <td>approve</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      S.no                                 Data Module            Type  \\\n",
       "1279  1280    approve person leave on my behalf  Leave  Leave_Approval   \n",
       "1280  1281                approve person leave   Leave  Leave_Approval   \n",
       "1281  1282         approve person leave request  Leave  Leave_Approval   \n",
       "1282  1283                approve person leaves  Leave  Leave_Approval   \n",
       "1283  1284  kindly approve person leave request  Leave  Leave_Approval   \n",
       "\n",
       "        sub_type sub_type_two sub_type_three  \n",
       "1279  particular      approve            NaN  \n",
       "1280  particular      approve            NaN  \n",
       "1281  particular      approve            NaN  \n",
       "1282  particular      approve            NaN  \n",
       "1283  particular      approve            NaN  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs[1],\"\\n\\n\",tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['sub_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    #model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103 samples, validate on 45 samples\n",
      "Epoch 1/15\n",
      "103/103 [==============================] - 2s 23ms/step - loss: 0.6877 - acc: 0.5146 - val_loss: 0.7007 - val_acc: 0.2667\n",
      "Epoch 2/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6645 - acc: 0.5631 - val_loss: 0.6750 - val_acc: 0.7111\n",
      "Epoch 3/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6416 - acc: 0.7087 - val_loss: 0.6962 - val_acc: 0.2889\n",
      "Epoch 4/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6044 - acc: 0.5728 - val_loss: 0.6359 - val_acc: 0.8000\n",
      "Epoch 5/15\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5604 - acc: 0.7670 - val_loss: 0.5729 - val_acc: 0.8667\n",
      "Epoch 6/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5090 - acc: 0.8641 - val_loss: 0.5169 - val_acc: 0.8889\n",
      "Epoch 7/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.4968 - acc: 0.8058 - val_loss: 0.4660 - val_acc: 0.9556\n",
      "Epoch 8/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.4153 - acc: 0.9612 - val_loss: 0.4222 - val_acc: 0.9778\n",
      "Epoch 9/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3647 - acc: 0.9612 - val_loss: 0.3867 - val_acc: 0.9778\n",
      "Epoch 10/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3095 - acc: 0.9612 - val_loss: 0.3662 - val_acc: 0.9111\n",
      "Epoch 11/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2662 - acc: 0.9515 - val_loss: 0.3148 - val_acc: 0.9333\n",
      "Epoch 12/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2226 - acc: 0.9612 - val_loss: 0.2531 - val_acc: 0.9778\n",
      "Epoch 13/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.1860 - acc: 0.9806 - val_loss: 11.7555 - val_acc: 0.2667\n",
      "Epoch 14/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 5.8276 - acc: 0.6019 - val_loss: 0.2789 - val_acc: 0.9111\n",
      "Epoch 15/15\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 2.4427 - acc: 0.7087 - val_loss: 0.2035 - val_acc: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ea601a470>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=15, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from keras.layers import Dense\\nfrom keras.models import Sequential\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import ShuffleSplit\\n\\n\\n#m = model()\\n# evaluate model with standardized dataset\\nestimator = KerasClassifier(build_fn=model, epochs=5, verbose=2)\\nkfold = ShuffleSplit(n_splits=10, test_size=0.3, random_state=100)\\nm.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "#m = model()\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=model, epochs=5, verbose=2)\n",
    "kfold = ShuffleSplit(n_splits=10, test_size=0.3, random_state=100)\n",
    "m.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = c.evaluate(test_sequences_matrix,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "c.save('Type_Leave.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):\n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            #if token not in stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['sub_type_two'] == 'specific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['TypeLeave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    model.add(LSTM(128, input_shape=(xtrain.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 529 samples, validate on 59 samples\n",
      "Epoch 1/30\n",
      "529/529 [==============================] - 7s 14ms/step - loss: 0.6394 - acc: 0.6125 - val_loss: 0.9413 - val_acc: 0.0678\n",
      "Epoch 2/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.4192 - acc: 0.8866 - val_loss: 0.8663 - val_acc: 0.4576\n",
      "Epoch 3/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.2589 - acc: 0.9263 - val_loss: 0.9486 - val_acc: 0.4746\n",
      "Epoch 4/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.1083 - acc: 0.9716 - val_loss: 1.3649 - val_acc: 0.3898\n",
      "Epoch 5/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.0868 - acc: 0.9679 - val_loss: 0.5542 - val_acc: 0.8475\n",
      "Epoch 6/30\n",
      "529/529 [==============================] - 6s 10ms/step - loss: 0.0671 - acc: 0.9811 - val_loss: 0.2475 - val_acc: 0.9153\n",
      "Epoch 7/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.1227 - acc: 0.9603 - val_loss: 0.3931 - val_acc: 0.8983\n",
      "Epoch 8/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.0511 - acc: 0.9849 - val_loss: 0.0933 - val_acc: 0.9492\n",
      "Epoch 9/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.0418 - acc: 0.9830 - val_loss: 0.1515 - val_acc: 0.9322\n",
      "Epoch 10/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.0339 - acc: 0.9924 - val_loss: 0.0959 - val_acc: 0.9492\n",
      "Epoch 11/30\n",
      "529/529 [==============================] - 6s 10ms/step - loss: 0.0212 - acc: 0.9905 - val_loss: 0.0543 - val_acc: 0.9831\n",
      "Epoch 12/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.0177 - acc: 0.9924 - val_loss: 0.0925 - val_acc: 0.9661\n",
      "Epoch 13/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.0135 - acc: 0.9962 - val_loss: 0.1726 - val_acc: 0.9322\n",
      "Epoch 14/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.0159 - acc: 0.9962 - val_loss: 0.0825 - val_acc: 0.9831\n",
      "Epoch 15/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.0204 - acc: 0.9924 - val_loss: 0.2828 - val_acc: 0.9153\n",
      "Epoch 16/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.0116 - acc: 0.9943 - val_loss: 0.0968 - val_acc: 0.9661\n",
      "Epoch 17/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.0116 - acc: 0.9924 - val_loss: 0.3593 - val_acc: 0.8983\n",
      "Epoch 18/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.0117 - acc: 0.9981 - val_loss: 0.0656 - val_acc: 0.9661\n",
      "Epoch 19/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.0136 - acc: 0.9924 - val_loss: 0.1377 - val_acc: 0.9322\n",
      "Epoch 20/30\n",
      "529/529 [==============================] - 6s 10ms/step - loss: 0.0068 - acc: 0.9981 - val_loss: 0.0129 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.0044 - acc: 0.9981 - val_loss: 0.1406 - val_acc: 0.9492\n",
      "Epoch 22/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 7.0417e-04 - acc: 1.0000 - val_loss: 0.1456 - val_acc: 0.9492\n",
      "Epoch 23/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0266 - val_acc: 0.9831\n",
      "Epoch 24/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 4.9639e-04 - acc: 1.0000 - val_loss: 0.0191 - val_acc: 0.9831\n",
      "Epoch 25/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 1.5404e-04 - acc: 1.0000 - val_loss: 0.0489 - val_acc: 0.9661\n",
      "Epoch 26/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 0.0052 - acc: 0.9962 - val_loss: 0.2938 - val_acc: 0.9492\n",
      "Epoch 27/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 3.8219e-04 - acc: 1.0000 - val_loss: 0.1843 - val_acc: 0.9661\n",
      "Epoch 28/30\n",
      "529/529 [==============================] - 6s 10ms/step - loss: 6.1755e-05 - acc: 1.0000 - val_loss: 0.1519 - val_acc: 0.9661\n",
      "Epoch 29/30\n",
      "529/529 [==============================] - 5s 10ms/step - loss: 4.9445e-04 - acc: 1.0000 - val_loss: 0.3356 - val_acc: 0.9492\n",
      "Epoch 30/30\n",
      "529/529 [==============================] - 6s 11ms/step - loss: 0.0030 - acc: 0.9981 - val_loss: 0.1542 - val_acc: 0.9661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b945d3def0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=30, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.save('Leave_inquiry_specific_Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUERY LEAVE SPECIFIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['sub_type_two'] == 'specific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m = load_model('Leave_inquiry_specific_Model.h5')\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['TypeLeave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how many sick leaves i have taken'\n",
    "query = 'how many leaves i have availed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how many sick leaves have left'\n",
    "query = 'how many sick leaves i can take more'\n",
    "query = 'Can you please inform me about how many sick leaves can i take more'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'mani', 'leav', 'i', 'have', 'avail']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = token_stems(sent)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test = ([list(sen)])\n",
    "sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 3.3748933e-08]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = m.predict(sen_sequences_matrix)\n",
    "print(score)\n",
    "print(max(score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
