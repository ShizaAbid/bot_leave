{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first tokenization will be performed then stemming will be performed over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):\n",
    "        \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "tra\n",
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs[1],\"\\n\\n\",tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['Kind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    model.add(LSTM(50, activation='tanh',dropout=0.1))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dropout(0.01))\n",
    "    #model.add(Dense(20, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))    \n",
    "    \n",
    "    \n",
    "    print(model.add(Dense(4, activation='softmax')))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['mae','acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Train on 397 samples, validate on 100 samples\n",
      "Epoch 1/24\n",
      "397/397 [==============================] - 3s 8ms/step - loss: 0.5529 - mean_absolute_error: 0.3713 - acc: 0.7500 - val_loss: 0.5858 - val_mean_absolute_error: 0.3832 - val_acc: 0.7500\n",
      "Epoch 2/24\n",
      "397/397 [==============================] - 2s 4ms/step - loss: 0.4999 - mean_absolute_error: 0.3321 - acc: 0.7538 - val_loss: 0.9195 - val_mean_absolute_error: 0.4543 - val_acc: 0.5000\n",
      "Epoch 3/24\n",
      "397/397 [==============================] - 2s 4ms/step - loss: 0.4799 - mean_absolute_error: 0.3144 - acc: 0.7513 - val_loss: 0.7011 - val_mean_absolute_error: 0.4149 - val_acc: 0.7500\n",
      "Epoch 4/24\n",
      "397/397 [==============================] - 2s 4ms/step - loss: 0.4599 - mean_absolute_error: 0.3095 - acc: 0.7815 - val_loss: 0.7384 - val_mean_absolute_error: 0.4220 - val_acc: 0.5700\n",
      "Epoch 5/24\n",
      "397/397 [==============================] - 2s 4ms/step - loss: 0.4227 - mean_absolute_error: 0.2967 - acc: 0.8394 - val_loss: 0.7540 - val_mean_absolute_error: 0.4263 - val_acc: 0.6450\n",
      "Epoch 6/24\n",
      "397/397 [==============================] - 2s 4ms/step - loss: 0.3599 - mean_absolute_error: 0.2516 - acc: 0.8715 - val_loss: 0.7176 - val_mean_absolute_error: 0.4151 - val_acc: 0.6700\n",
      "Epoch 7/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.2849 - mean_absolute_error: 0.2017 - acc: 0.9011 - val_loss: 0.6715 - val_mean_absolute_error: 0.4000 - val_acc: 0.6650\n",
      "Epoch 8/24\n",
      "397/397 [==============================] - 2s 4ms/step - loss: 0.2337 - mean_absolute_error: 0.1668 - acc: 0.9125 - val_loss: 0.8096 - val_mean_absolute_error: 0.4098 - val_acc: 0.5900\n",
      "Epoch 9/24\n",
      "397/397 [==============================] - 2s 4ms/step - loss: 0.1915 - mean_absolute_error: 0.1314 - acc: 0.9188 - val_loss: 0.6511 - val_mean_absolute_error: 0.3746 - val_acc: 0.6500\n",
      "Epoch 10/24\n",
      "397/397 [==============================] - 2s 4ms/step - loss: 0.1675 - mean_absolute_error: 0.1224 - acc: 0.9414 - val_loss: 0.6539 - val_mean_absolute_error: 0.3635 - val_acc: 0.6425\n",
      "Epoch 11/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.1518 - mean_absolute_error: 0.1055 - acc: 0.9452 - val_loss: 0.7005 - val_mean_absolute_error: 0.3674 - val_acc: 0.6475\n",
      "Epoch 12/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.1323 - mean_absolute_error: 0.0899 - acc: 0.9528 - val_loss: 0.7021 - val_mean_absolute_error: 0.3564 - val_acc: 0.6650\n",
      "Epoch 13/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.1158 - mean_absolute_error: 0.0795 - acc: 0.9622 - val_loss: 0.6911 - val_mean_absolute_error: 0.3413 - val_acc: 0.6800\n",
      "Epoch 14/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.1047 - mean_absolute_error: 0.0706 - acc: 0.9698 - val_loss: 0.6813 - val_mean_absolute_error: 0.3372 - val_acc: 0.7000\n",
      "Epoch 15/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0914 - mean_absolute_error: 0.0632 - acc: 0.9761 - val_loss: 0.7184 - val_mean_absolute_error: 0.3339 - val_acc: 0.6950\n",
      "Epoch 16/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0797 - mean_absolute_error: 0.0533 - acc: 0.9811 - val_loss: 0.5995 - val_mean_absolute_error: 0.2987 - val_acc: 0.7500\n",
      "Epoch 17/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0698 - mean_absolute_error: 0.0477 - acc: 0.9836 - val_loss: 0.7921 - val_mean_absolute_error: 0.3393 - val_acc: 0.6850\n",
      "Epoch 18/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0616 - mean_absolute_error: 0.0428 - acc: 0.9861 - val_loss: 0.7425 - val_mean_absolute_error: 0.3138 - val_acc: 0.7200\n",
      "Epoch 19/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0538 - mean_absolute_error: 0.0362 - acc: 0.9868 - val_loss: 0.7631 - val_mean_absolute_error: 0.3147 - val_acc: 0.7200\n",
      "Epoch 20/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0508 - mean_absolute_error: 0.0332 - acc: 0.9861 - val_loss: 0.7672 - val_mean_absolute_error: 0.3101 - val_acc: 0.7200\n",
      "Epoch 21/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0463 - mean_absolute_error: 0.0305 - acc: 0.9906 - val_loss: 0.7193 - val_mean_absolute_error: 0.3019 - val_acc: 0.7425\n",
      "Epoch 22/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0405 - mean_absolute_error: 0.0276 - acc: 0.9924 - val_loss: 0.7761 - val_mean_absolute_error: 0.3114 - val_acc: 0.7350\n",
      "Epoch 23/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0344 - mean_absolute_error: 0.0230 - acc: 0.9924 - val_loss: 0.6475 - val_mean_absolute_error: 0.2581 - val_acc: 0.7725\n",
      "Epoch 24/24\n",
      "397/397 [==============================] - 2s 5ms/step - loss: 0.0356 - mean_absolute_error: 0.0254 - acc: 0.9918 - val_loss: 0.7578 - val_mean_absolute_error: 0.3161 - val_acc: 0.7275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f10fd7d9b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=24, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from keras.layers import Dense\\nfrom keras.models import Sequential\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import ShuffleSplit\\n\\n\\nm = model()\\n# evaluate model with standardized dataset\\nestimator = KerasClassifier(build_fn=model, epochs=15, verbose=2)\\nkfold = ShuffleSplit(n_splits=15, test_size=0.3, random_state=100)\\nm.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "m = model()\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=model, epochs=15, verbose=2)\n",
    "kfold = ShuffleSplit(n_splits=15, test_size=0.3, random_state=100)\n",
    "m.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 811us/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = c.evaluate(test_sequences_matrix,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "c.save('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
