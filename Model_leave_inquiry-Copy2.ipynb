{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first tokenization will be performed then stemming will be performed over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):\n",
    "        \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1565, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\dataset.xlsx\")\n",
    "#train= train[train['Module']== 'Personal']\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.no</th>\n",
       "      <th>Data</th>\n",
       "      <th>Module</th>\n",
       "      <th>Type</th>\n",
       "      <th>sub_type</th>\n",
       "      <th>sub_type_two</th>\n",
       "      <th>sub_type_three</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is my department?</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Department</td>\n",
       "      <td>particular</td>\n",
       "      <td>empty</td>\n",
       "      <td>department</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is my department name?</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Department</td>\n",
       "      <td>particular</td>\n",
       "      <td>empty</td>\n",
       "      <td>department</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>In which department I belong?</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Department</td>\n",
       "      <td>particular</td>\n",
       "      <td>empty</td>\n",
       "      <td>department</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>In which department I am working in?</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Department</td>\n",
       "      <td>particular</td>\n",
       "      <td>empty</td>\n",
       "      <td>department</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What is the name of department?</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Department</td>\n",
       "      <td>particular</td>\n",
       "      <td>empty</td>\n",
       "      <td>department</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S.no                                  Data    Module        Type  \\\n",
       "0     1                What is my department?  Personal  Department   \n",
       "1     2           What is my department name?  Personal  Department   \n",
       "2     3         In which department I belong?  Personal  Department   \n",
       "3     4  In which department I am working in?  Personal  Department   \n",
       "4     5       What is the name of department?  Personal  Department   \n",
       "\n",
       "     sub_type sub_type_two sub_type_three  \n",
       "0  particular        empty     department  \n",
       "1  particular        empty     department  \n",
       "2  particular        empty     department  \n",
       "3  particular        empty     department  \n",
       "4  particular        empty     department  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs[1],\"\\n\\n\",tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['sub_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1095 samples, validate on 470 samples\n",
      "Epoch 1/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.3998 - acc: 0.8365 - val_loss: 0.4676 - val_acc: 0.8000\n",
      "Epoch 2/24\n",
      "1095/1095 [==============================] - 5s 5ms/step - loss: 0.0775 - acc: 0.9826 - val_loss: 0.6264 - val_acc: 0.7979\n",
      "Epoch 3/24\n",
      "1095/1095 [==============================] - 5s 5ms/step - loss: 0.0428 - acc: 0.9854 - val_loss: 0.7518 - val_acc: 0.8085\n",
      "Epoch 4/24\n",
      "1095/1095 [==============================] - 5s 5ms/step - loss: 0.0176 - acc: 0.9954 - val_loss: 1.0094 - val_acc: 0.8043\n",
      "Epoch 5/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.0136 - acc: 0.9945 - val_loss: 1.1023 - val_acc: 0.8064\n",
      "Epoch 6/24\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.8551 - val_acc: 0.8000\n",
      "Epoch 7/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 1.4151 - val_acc: 0.8106\n",
      "Epoch 8/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.0017 - acc: 0.9991 - val_loss: 1.2810 - val_acc: 0.8021\n",
      "Epoch 9/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.0017 - acc: 0.9991 - val_loss: 1.3243 - val_acc: 0.8064\n",
      "Epoch 10/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 3.5405e-04 - acc: 1.0000 - val_loss: 1.5003 - val_acc: 0.8043\n",
      "Epoch 11/24\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 5.3591e-05 - acc: 1.0000 - val_loss: 1.7357 - val_acc: 0.8064\n",
      "Epoch 12/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 8.4033e-05 - acc: 1.0000 - val_loss: 1.5173 - val_acc: 0.8106\n",
      "Epoch 13/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 4.8229e-04 - acc: 1.0000 - val_loss: 1.6212 - val_acc: 0.8106\n",
      "Epoch 14/24\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 9.5294e-06 - acc: 1.0000 - val_loss: 1.7332 - val_acc: 0.8128\n",
      "Epoch 15/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 2.4917e-06 - acc: 1.0000 - val_loss: 1.7610 - val_acc: 0.8128\n",
      "Epoch 16/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 1.3503e-06 - acc: 1.0000 - val_loss: 1.8273 - val_acc: 0.8106\n",
      "Epoch 17/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 1.7071e-04 - acc: 1.0000 - val_loss: 1.9782 - val_acc: 0.8085\n",
      "Epoch 18/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 3.5848e-07 - acc: 1.0000 - val_loss: 1.9488 - val_acc: 0.8106\n",
      "Epoch 19/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 2.9373e-07 - acc: 1.0000 - val_loss: 1.9490 - val_acc: 0.8106\n",
      "Epoch 20/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 2.1618e-07 - acc: 1.0000 - val_loss: 2.0055 - val_acc: 0.8106\n",
      "Epoch 21/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 1.4874e-07 - acc: 1.0000 - val_loss: 2.1074 - val_acc: 0.8106\n",
      "Epoch 22/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 1.8313e-06 - acc: 1.0000 - val_loss: 1.7894 - val_acc: 0.8106\n",
      "Epoch 23/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 2.6853e-07 - acc: 1.0000 - val_loss: 1.9542 - val_acc: 0.8128\n",
      "Epoch 24/24\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 1.3202e-07 - acc: 1.0000 - val_loss: 1.8394 - val_acc: 0.8149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x220c2298668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=24, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from keras.layers import Dense\\nfrom keras.models import Sequential\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import ShuffleSplit\\n\\n\\nm = model()\\n# evaluate model with standardized dataset\\nestimator = KerasClassifier(build_fn=model, epochs=15, verbose=2)\\nkfold = ShuffleSplit(n_splits=15, test_size=0.3, random_state=100)\\nm.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "m = model()\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=model, epochs=15, verbose=2)\n",
    "kfold = ShuffleSplit(n_splits=15, test_size=0.3, random_state=100)\n",
    "m.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 0s 755us/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = c.evaluate(test_sequences_matrix,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "c.save('Leave.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
