{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first tokenization will be performed then stemming will be performed over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):\n",
    "        \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545, 7)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\dataset.xlsx\")\n",
    "train= train[train['Module']== 'Leave']\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.no</th>\n",
       "      <th>Data</th>\n",
       "      <th>Module</th>\n",
       "      <th>Type</th>\n",
       "      <th>sub_type</th>\n",
       "      <th>sub_type_two</th>\n",
       "      <th>sub_type_three</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>811</td>\n",
       "      <td>I am willing to take leave for today</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>812</td>\n",
       "      <td>I need a sick leave for tomorrow</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>813</td>\n",
       "      <td>I will take leave from 4th march to 8th dec</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>814</td>\n",
       "      <td>I will not join office from 6th to 9th dec</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>815</td>\n",
       "      <td>I am going on annual leaves</td>\n",
       "      <td>Leave</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>particular</td>\n",
       "      <td>LeaveRequest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     S.no                                         Data Module          Type  \\\n",
       "810   811         I am willing to take leave for today  Leave  LeaveRequest   \n",
       "811   812             I need a sick leave for tomorrow  Leave  LeaveRequest   \n",
       "812   813  I will take leave from 4th march to 8th dec  Leave  LeaveRequest   \n",
       "813   814   I will not join office from 6th to 9th dec  Leave  LeaveRequest   \n",
       "814   815                  I am going on annual leaves  Leave  LeaveRequest   \n",
       "\n",
       "       sub_type  sub_type_two sub_type_three  \n",
       "810  particular  LeaveRequest            NaN  \n",
       "811  particular  LeaveRequest            NaN  \n",
       "812  particular  LeaveRequest            NaN  \n",
       "813  particular  LeaveRequest            NaN  \n",
       "814  particular  LeaveRequest            NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs[1],\"\\n\\n\",tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 436 samples, validate on 109 samples\n",
      "Epoch 1/24\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 0.5118 - acc: 0.7460 - val_loss: 1.0538 - val_acc: 0.7500\n",
      "Epoch 2/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.4678 - acc: 0.7489 - val_loss: 1.1543 - val_acc: 0.7477\n",
      "Epoch 3/24\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 0.4430 - acc: 0.7655 - val_loss: 1.1326 - val_acc: 0.7408\n",
      "Epoch 4/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.3937 - acc: 0.8102 - val_loss: 1.2411 - val_acc: 0.6743\n",
      "Epoch 5/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.3314 - acc: 0.8555 - val_loss: 1.2993 - val_acc: 0.6789\n",
      "Epoch 6/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.2686 - acc: 0.8819 - val_loss: 1.4638 - val_acc: 0.5688\n",
      "Epoch 7/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.2262 - acc: 0.9094 - val_loss: 1.3613 - val_acc: 0.6606\n",
      "Epoch 8/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.1817 - acc: 0.9409 - val_loss: 1.5601 - val_acc: 0.6330\n",
      "Epoch 9/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.1446 - acc: 0.9570 - val_loss: 1.6284 - val_acc: 0.5963\n",
      "Epoch 10/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.1294 - acc: 0.9587 - val_loss: 1.6352 - val_acc: 0.6651\n",
      "Epoch 11/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.1098 - acc: 0.9673 - val_loss: 1.9243 - val_acc: 0.6009\n",
      "Epoch 12/24\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 0.0935 - acc: 0.9690 - val_loss: 2.4225 - val_acc: 0.5505\n",
      "Epoch 13/24\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 0.0743 - acc: 0.9788 - val_loss: 2.2236 - val_acc: 0.5986\n",
      "Epoch 14/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0693 - acc: 0.9759 - val_loss: 2.1406 - val_acc: 0.6170\n",
      "Epoch 15/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0666 - acc: 0.9817 - val_loss: 2.4731 - val_acc: 0.5711\n",
      "Epoch 16/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0588 - acc: 0.9799 - val_loss: 2.6888 - val_acc: 0.5436\n",
      "Epoch 17/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0561 - acc: 0.9839 - val_loss: 2.4998 - val_acc: 0.5619\n",
      "Epoch 18/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0449 - acc: 0.9845 - val_loss: 2.5991 - val_acc: 0.5849\n",
      "Epoch 19/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0461 - acc: 0.9880 - val_loss: 2.6954 - val_acc: 0.5550\n",
      "Epoch 20/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0393 - acc: 0.9897 - val_loss: 2.3121 - val_acc: 0.5665\n",
      "Epoch 21/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0342 - acc: 0.9897 - val_loss: 2.7969 - val_acc: 0.5688\n",
      "Epoch 22/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0314 - acc: 0.9891 - val_loss: 3.0225 - val_acc: 0.5459\n",
      "Epoch 23/24\n",
      "436/436 [==============================] - 3s 7ms/step - loss: 0.0387 - acc: 0.9885 - val_loss: 3.0856 - val_acc: 0.5573\n",
      "Epoch 24/24\n",
      "436/436 [==============================] - 3s 8ms/step - loss: 0.0283 - acc: 0.9914 - val_loss: 2.8277 - val_acc: 0.5665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x275238799b0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=24, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from keras.layers import Dense\\nfrom keras.models import Sequential\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import ShuffleSplit\\n\\n\\nm = model()\\n# evaluate model with standardized dataset\\nestimator = KerasClassifier(build_fn=model, epochs=15, verbose=2)\\nkfold = ShuffleSplit(n_splits=15, test_size=0.3, random_state=100)\\nm.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "m = model()\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=model, epochs=15, verbose=2)\n",
    "kfold = ShuffleSplit(n_splits=15, test_size=0.3, random_state=100)\n",
    "m.fit(cross_val_score(estimator,sequences_matrix,y, cv=kfold))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = c.evaluate(test_sequences_matrix,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "c.save('Module.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
