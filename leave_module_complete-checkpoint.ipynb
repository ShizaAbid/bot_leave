{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import tensorflow as tf\n",
    "    import json\n",
    "    import xlrd\n",
    "    import traceback   \n",
    "    import numpy as np #use to handle numeric data\n",
    "    import nltk #for nlp purpose\n",
    "    import pandas as pd #use for file that we read\n",
    "    import re #to handle regular expression\n",
    "    from keras.models import load_model #To load model\n",
    "    from keras import backend as K #to load the backend library that we are using \n",
    "    from tensorflow.keras.preprocessing import sequence\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from textblob import TextBlob\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from langdetect import detect\n",
    "    import string \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import h5py\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    import datefinder\n",
    "    from dateparser.search import search_dates\n",
    "    import spacy\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import gensim\n",
    "    import inflect\n",
    "    p = inflect.engine()\n",
    "    from dateutil.parser import parse\n",
    "        \n",
    "\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.read_excel(\"D:\\\\bot\\\\botapi\\\\botapi\\\\dataset\\\\Main.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset[Dataset['Module']=='leave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s.no</th>\n",
       "      <th>Data</th>\n",
       "      <th>Module</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>sub_intent_one</th>\n",
       "      <th>sub_intent_two</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>361</td>\n",
       "      <td>Please show my pending leaves</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>362</td>\n",
       "      <td>Please show my all leaves.</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_taken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>363</td>\n",
       "      <td>Please tell me the balance of casual leave</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>364</td>\n",
       "      <td>Please tell me the balance of sick leave</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>365</td>\n",
       "      <td>Please tell me the balance of annual leave</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     s.no                                        Data Module    Quantity  \\\n",
       "360   361               Please show my pending leaves  leave  particular   \n",
       "361   362                  Please show my all leaves.  leave  particular   \n",
       "362   363  Please tell me the balance of casual leave  leave  particular   \n",
       "363   364    Please tell me the balance of sick leave  leave  particular   \n",
       "364   365  Please tell me the balance of annual leave  leave  particular   \n",
       "\n",
       "    sub_intent_one sub_intent_two    Unnamed: 6  \n",
       "360  leave_inquiry       specific  leave_update  \n",
       "361  leave_inquiry       specific   leave_taken  \n",
       "362  leave_inquiry       specific  leave_update  \n",
       "363  leave_inquiry       specific  leave_update  \n",
       "364  leave_inquiry       specific  leave_update  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Dataset['Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first tokenization will be performed then stemming will be performed over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):      \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            #if token not in stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in Data:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs[1],\"\\n\\n\",tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['leave_inquiry', 'LeaveRequest', 'Leave_Approval',\n",
       "       'emotion_LeaveRequest'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset['sub_intent_one'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(Dataset['sub_intent_one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#SVM model \n",
    "from sklearn import svm \n",
    "svm_model = svm.SVC(C=0.4,kernel = 'linear')\n",
    "svm_model.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = \"how many half days have i availed\"\n",
    "query = \"do i have any absent for last month\"\n",
    "query = \"how many leaves have i availed\"\n",
    "query = \"inform me about how many leaves have i taken\"\n",
    "query = \"is my leave approved\"\n",
    "#INQUIRY"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = \"i am feeling sick\"\n",
    "query = \"i dont feel i can work today\"\n",
    "query = \"i dont want to come \"\n",
    "query = \"will office remain close tomorrow\"\n",
    "query = 'who is my leave approving authority'\n",
    "query = 'I won’t be able to come to office today'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = 'i want sick leave for today'\n",
    "query = 'Shiza is not feeling well'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = token_stems(query)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sequence_sen = tok.texts_to_sequences([query])\n",
    "sen = sequence.pad_sequences(sequence_sen , maxlen = max_len)\n",
    "y_pr = clf.predict(sen)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(y_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Algo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(sequences_matrix,y,random_state = 100,test_size =0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(svm_model.predict(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    model.add(LSTM(128, input_shape=(x.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1498 samples, validate on 642 samples\n",
      "Epoch 1/20\n",
      "1498/1498 [==============================] - ETA: 4:47 - loss: 0.5624 - acc: 0.750 - ETA: 2:35 - loss: 0.5600 - acc: 0.750 - ETA: 1:50 - loss: 0.5570 - acc: 0.750 - ETA: 1:28 - loss: 0.5493 - acc: 0.750 - ETA: 1:14 - loss: 0.5478 - acc: 0.753 - ETA: 1:04 - loss: 0.5400 - acc: 0.752 - ETA: 57s - loss: 0.5359 - acc: 0.752 - ETA: 52s - loss: 0.5345 - acc: 0.75 - ETA: 47s - loss: 0.5296 - acc: 0.75 - ETA: 44s - loss: 0.5260 - acc: 0.75 - ETA: 41s - loss: 0.5229 - acc: 0.75 - ETA: 38s - loss: 0.5227 - acc: 0.75 - ETA: 36s - loss: 0.5254 - acc: 0.75 - ETA: 34s - loss: 0.5241 - acc: 0.75 - ETA: 32s - loss: 0.5268 - acc: 0.75 - ETA: 30s - loss: 0.5271 - acc: 0.75 - ETA: 28s - loss: 0.5259 - acc: 0.75 - ETA: 27s - loss: 0.5266 - acc: 0.75 - ETA: 26s - loss: 0.5259 - acc: 0.75 - ETA: 24s - loss: 0.5284 - acc: 0.75 - ETA: 23s - loss: 0.5267 - acc: 0.75 - ETA: 22s - loss: 0.5245 - acc: 0.75 - ETA: 21s - loss: 0.5228 - acc: 0.75 - ETA: 19s - loss: 0.5232 - acc: 0.75 - ETA: 18s - loss: 0.5222 - acc: 0.75 - ETA: 17s - loss: 0.5198 - acc: 0.75 - ETA: 16s - loss: 0.5162 - acc: 0.76 - ETA: 15s - loss: 0.5130 - acc: 0.76 - ETA: 14s - loss: 0.5110 - acc: 0.76 - ETA: 13s - loss: 0.5101 - acc: 0.76 - ETA: 13s - loss: 0.5082 - acc: 0.76 - ETA: 12s - loss: 0.5059 - acc: 0.76 - ETA: 11s - loss: 0.5014 - acc: 0.77 - ETA: 10s - loss: 0.4996 - acc: 0.77 - ETA: 9s - loss: 0.4958 - acc: 0.7754 - ETA: 8s - loss: 0.4949 - acc: 0.775 - ETA: 7s - loss: 0.4921 - acc: 0.777 - ETA: 6s - loss: 0.4900 - acc: 0.779 - ETA: 6s - loss: 0.4857 - acc: 0.781 - ETA: 5s - loss: 0.4804 - acc: 0.784 - ETA: 4s - loss: 0.4765 - acc: 0.787 - ETA: 3s - loss: 0.4731 - acc: 0.788 - ETA: 2s - loss: 0.4700 - acc: 0.790 - ETA: 2s - loss: 0.4674 - acc: 0.791 - ETA: 1s - loss: 0.4647 - acc: 0.792 - ETA: 0s - loss: 0.4614 - acc: 0.794 - 40s 26ms/step - loss: 0.4591 - acc: 0.7956 - val_loss: 0.3284 - val_acc: 0.8711\n",
      "Epoch 2/20\n",
      "1498/1498 [==============================] - ETA: 28s - loss: 0.3356 - acc: 0.85 - ETA: 28s - loss: 0.3574 - acc: 0.83 - ETA: 27s - loss: 0.3426 - acc: 0.84 - ETA: 27s - loss: 0.3524 - acc: 0.83 - ETA: 26s - loss: 0.3589 - acc: 0.83 - ETA: 25s - loss: 0.3476 - acc: 0.84 - ETA: 25s - loss: 0.3504 - acc: 0.84 - ETA: 24s - loss: 0.3390 - acc: 0.84 - ETA: 24s - loss: 0.3314 - acc: 0.84 - ETA: 23s - loss: 0.3427 - acc: 0.84 - ETA: 23s - loss: 0.3403 - acc: 0.84 - ETA: 22s - loss: 0.3361 - acc: 0.84 - ETA: 21s - loss: 0.3367 - acc: 0.84 - ETA: 21s - loss: 0.3303 - acc: 0.84 - ETA: 20s - loss: 0.3342 - acc: 0.84 - ETA: 19s - loss: 0.3320 - acc: 0.84 - ETA: 19s - loss: 0.3337 - acc: 0.84 - ETA: 18s - loss: 0.3329 - acc: 0.84 - ETA: 17s - loss: 0.3279 - acc: 0.84 - ETA: 17s - loss: 0.3262 - acc: 0.85 - ETA: 16s - loss: 0.3201 - acc: 0.85 - ETA: 16s - loss: 0.3223 - acc: 0.85 - ETA: 15s - loss: 0.3197 - acc: 0.85 - ETA: 14s - loss: 0.3226 - acc: 0.85 - ETA: 14s - loss: 0.3210 - acc: 0.85 - ETA: 13s - loss: 0.3186 - acc: 0.85 - ETA: 12s - loss: 0.3161 - acc: 0.85 - ETA: 12s - loss: 0.3112 - acc: 0.85 - ETA: 11s - loss: 0.3114 - acc: 0.85 - ETA: 10s - loss: 0.3110 - acc: 0.85 - ETA: 10s - loss: 0.3066 - acc: 0.85 - ETA: 9s - loss: 0.3079 - acc: 0.8577 - ETA: 8s - loss: 0.3077 - acc: 0.857 - ETA: 8s - loss: 0.3081 - acc: 0.856 - ETA: 7s - loss: 0.3085 - acc: 0.856 - ETA: 7s - loss: 0.3069 - acc: 0.857 - ETA: 6s - loss: 0.3070 - acc: 0.856 - ETA: 5s - loss: 0.3066 - acc: 0.856 - ETA: 5s - loss: 0.3070 - acc: 0.855 - ETA: 4s - loss: 0.3054 - acc: 0.856 - ETA: 3s - loss: 0.3060 - acc: 0.855 - ETA: 3s - loss: 0.3053 - acc: 0.855 - ETA: 2s - loss: 0.3036 - acc: 0.856 - ETA: 1s - loss: 0.3034 - acc: 0.856 - ETA: 1s - loss: 0.3026 - acc: 0.856 - ETA: 0s - loss: 0.3031 - acc: 0.855 - 33s 22ms/step - loss: 0.3027 - acc: 0.8558 - val_loss: 0.2675 - val_acc: 0.8762\n",
      "Epoch 3/20\n",
      "1498/1498 [==============================] - ETA: 29s - loss: 0.2804 - acc: 0.84 - ETA: 28s - loss: 0.2328 - acc: 0.87 - ETA: 26s - loss: 0.2401 - acc: 0.88 - ETA: 26s - loss: 0.2426 - acc: 0.87 - ETA: 25s - loss: 0.2497 - acc: 0.87 - ETA: 24s - loss: 0.2431 - acc: 0.87 - ETA: 24s - loss: 0.2350 - acc: 0.87 - ETA: 23s - loss: 0.2375 - acc: 0.87 - ETA: 23s - loss: 0.2402 - acc: 0.87 - ETA: 22s - loss: 0.2427 - acc: 0.87 - ETA: 22s - loss: 0.2454 - acc: 0.87 - ETA: 21s - loss: 0.2452 - acc: 0.87 - ETA: 20s - loss: 0.2435 - acc: 0.87 - ETA: 20s - loss: 0.2433 - acc: 0.87 - ETA: 19s - loss: 0.2391 - acc: 0.88 - ETA: 19s - loss: 0.2393 - acc: 0.88 - ETA: 18s - loss: 0.2395 - acc: 0.88 - ETA: 18s - loss: 0.2412 - acc: 0.88 - ETA: 17s - loss: 0.2402 - acc: 0.88 - ETA: 16s - loss: 0.2414 - acc: 0.88 - ETA: 16s - loss: 0.2403 - acc: 0.88 - ETA: 15s - loss: 0.2365 - acc: 0.88 - ETA: 14s - loss: 0.2362 - acc: 0.88 - ETA: 14s - loss: 0.2356 - acc: 0.88 - ETA: 13s - loss: 0.2331 - acc: 0.88 - ETA: 12s - loss: 0.2320 - acc: 0.88 - ETA: 12s - loss: 0.2353 - acc: 0.88 - ETA: 11s - loss: 0.2343 - acc: 0.88 - ETA: 11s - loss: 0.2372 - acc: 0.88 - ETA: 10s - loss: 0.2350 - acc: 0.88 - ETA: 9s - loss: 0.2380 - acc: 0.8879 - ETA: 9s - loss: 0.2363 - acc: 0.888 - ETA: 8s - loss: 0.2351 - acc: 0.889 - ETA: 8s - loss: 0.2333 - acc: 0.891 - ETA: 7s - loss: 0.2322 - acc: 0.892 - ETA: 6s - loss: 0.2321 - acc: 0.892 - ETA: 6s - loss: 0.2319 - acc: 0.892 - ETA: 5s - loss: 0.2311 - acc: 0.893 - ETA: 4s - loss: 0.2300 - acc: 0.894 - ETA: 4s - loss: 0.2284 - acc: 0.895 - ETA: 3s - loss: 0.2279 - acc: 0.896 - ETA: 3s - loss: 0.2267 - acc: 0.897 - ETA: 2s - loss: 0.2264 - acc: 0.897 - ETA: 1s - loss: 0.2264 - acc: 0.897 - ETA: 1s - loss: 0.2266 - acc: 0.897 - ETA: 0s - loss: 0.2248 - acc: 0.898 - 33s 22ms/step - loss: 0.2249 - acc: 0.8987 - val_loss: 0.2471 - val_acc: 0.8808\n",
      "Epoch 4/20\n",
      "1498/1498 [==============================] - ETA: 27s - loss: 0.1468 - acc: 0.96 - ETA: 26s - loss: 0.1641 - acc: 0.95 - ETA: 26s - loss: 0.1748 - acc: 0.94 - ETA: 25s - loss: 0.1758 - acc: 0.93 - ETA: 25s - loss: 0.1778 - acc: 0.92 - ETA: 24s - loss: 0.1707 - acc: 0.93 - ETA: 24s - loss: 0.1757 - acc: 0.93 - ETA: 24s - loss: 0.1831 - acc: 0.93 - ETA: 23s - loss: 0.1826 - acc: 0.93 - ETA: 22s - loss: 0.1746 - acc: 0.93 - ETA: 21s - loss: 0.1777 - acc: 0.93 - ETA: 21s - loss: 0.1803 - acc: 0.93 - ETA: 20s - loss: 0.1738 - acc: 0.93 - ETA: 19s - loss: 0.1710 - acc: 0.93 - ETA: 18s - loss: 0.1700 - acc: 0.93 - ETA: 18s - loss: 0.1673 - acc: 0.94 - ETA: 17s - loss: 0.1672 - acc: 0.93 - ETA: 16s - loss: 0.1668 - acc: 0.94 - ETA: 16s - loss: 0.1641 - acc: 0.94 - ETA: 15s - loss: 0.1624 - acc: 0.94 - ETA: 15s - loss: 0.1630 - acc: 0.94 - ETA: 14s - loss: 0.1643 - acc: 0.94 - ETA: 13s - loss: 0.1638 - acc: 0.94 - ETA: 13s - loss: 0.1625 - acc: 0.94 - ETA: 12s - loss: 0.1615 - acc: 0.94 - ETA: 12s - loss: 0.1598 - acc: 0.94 - ETA: 11s - loss: 0.1573 - acc: 0.94 - ETA: 10s - loss: 0.1561 - acc: 0.94 - ETA: 10s - loss: 0.1543 - acc: 0.94 - ETA: 9s - loss: 0.1640 - acc: 0.9409 - ETA: 9s - loss: 0.1651 - acc: 0.940 - ETA: 8s - loss: 0.1662 - acc: 0.940 - ETA: 8s - loss: 0.1659 - acc: 0.940 - ETA: 7s - loss: 0.1667 - acc: 0.940 - ETA: 6s - loss: 0.1656 - acc: 0.940 - ETA: 6s - loss: 0.1655 - acc: 0.941 - ETA: 5s - loss: 0.1644 - acc: 0.941 - ETA: 5s - loss: 0.1631 - acc: 0.942 - ETA: 4s - loss: 0.1621 - acc: 0.942 - ETA: 3s - loss: 0.1610 - acc: 0.943 - ETA: 3s - loss: 0.1583 - acc: 0.944 - ETA: 2s - loss: 0.1581 - acc: 0.944 - ETA: 2s - loss: 0.1570 - acc: 0.945 - ETA: 1s - loss: 0.1561 - acc: 0.946 - ETA: 1s - loss: 0.1560 - acc: 0.946 - ETA: 0s - loss: 0.1544 - acc: 0.947 - 30s 20ms/step - loss: 0.1541 - acc: 0.9476 - val_loss: 0.1792 - val_acc: 0.9361\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1498/1498 [==============================] - ETA: 24s - loss: 0.1567 - acc: 0.93 - ETA: 24s - loss: 0.1638 - acc: 0.92 - ETA: 24s - loss: 0.1287 - acc: 0.95 - ETA: 23s - loss: 0.1222 - acc: 0.95 - ETA: 23s - loss: 0.1207 - acc: 0.95 - ETA: 22s - loss: 0.1217 - acc: 0.95 - ETA: 22s - loss: 0.1244 - acc: 0.95 - ETA: 22s - loss: 0.1234 - acc: 0.95 - ETA: 21s - loss: 0.1231 - acc: 0.95 - ETA: 21s - loss: 0.1205 - acc: 0.95 - ETA: 20s - loss: 0.1145 - acc: 0.96 - ETA: 20s - loss: 0.1128 - acc: 0.96 - ETA: 19s - loss: 0.1160 - acc: 0.96 - ETA: 19s - loss: 0.1153 - acc: 0.96 - ETA: 18s - loss: 0.1139 - acc: 0.96 - ETA: 18s - loss: 0.1149 - acc: 0.96 - ETA: 17s - loss: 0.1136 - acc: 0.96 - ETA: 16s - loss: 0.1114 - acc: 0.96 - ETA: 16s - loss: 0.1099 - acc: 0.96 - ETA: 15s - loss: 0.1118 - acc: 0.96 - ETA: 15s - loss: 0.1126 - acc: 0.96 - ETA: 14s - loss: 0.1130 - acc: 0.96 - ETA: 14s - loss: 0.1160 - acc: 0.96 - ETA: 13s - loss: 0.1155 - acc: 0.96 - ETA: 12s - loss: 0.1132 - acc: 0.96 - ETA: 12s - loss: 0.1136 - acc: 0.96 - ETA: 11s - loss: 0.1154 - acc: 0.96 - ETA: 11s - loss: 0.1145 - acc: 0.96 - ETA: 10s - loss: 0.1138 - acc: 0.96 - ETA: 9s - loss: 0.1142 - acc: 0.9625 - ETA: 9s - loss: 0.1141 - acc: 0.962 - ETA: 8s - loss: 0.1139 - acc: 0.962 - ETA: 8s - loss: 0.1145 - acc: 0.962 - ETA: 7s - loss: 0.1142 - acc: 0.961 - ETA: 6s - loss: 0.1122 - acc: 0.962 - ETA: 6s - loss: 0.1145 - acc: 0.962 - ETA: 5s - loss: 0.1143 - acc: 0.962 - ETA: 5s - loss: 0.1129 - acc: 0.963 - ETA: 4s - loss: 0.1129 - acc: 0.963 - ETA: 4s - loss: 0.1121 - acc: 0.962 - ETA: 3s - loss: 0.1140 - acc: 0.961 - ETA: 2s - loss: 0.1149 - acc: 0.961 - ETA: 2s - loss: 0.1161 - acc: 0.961 - ETA: 1s - loss: 0.1190 - acc: 0.960 - ETA: 1s - loss: 0.1197 - acc: 0.960 - ETA: 0s - loss: 0.1189 - acc: 0.960 - 30s 20ms/step - loss: 0.1190 - acc: 0.9606 - val_loss: 0.3709 - val_acc: 0.8793\n",
      "Epoch 6/20\n",
      "1498/1498 [==============================] - ETA: 26s - loss: 0.0508 - acc: 1.00 - ETA: 26s - loss: 0.0584 - acc: 0.98 - ETA: 25s - loss: 0.0903 - acc: 0.97 - ETA: 25s - loss: 0.0804 - acc: 0.97 - ETA: 24s - loss: 0.0776 - acc: 0.97 - ETA: 24s - loss: 0.0799 - acc: 0.97 - ETA: 23s - loss: 0.0746 - acc: 0.97 - ETA: 22s - loss: 0.0788 - acc: 0.97 - ETA: 22s - loss: 0.0938 - acc: 0.96 - ETA: 21s - loss: 0.0923 - acc: 0.97 - ETA: 20s - loss: 0.0905 - acc: 0.97 - ETA: 20s - loss: 0.0896 - acc: 0.97 - ETA: 19s - loss: 0.0888 - acc: 0.97 - ETA: 19s - loss: 0.0876 - acc: 0.97 - ETA: 18s - loss: 0.0844 - acc: 0.97 - ETA: 17s - loss: 0.0878 - acc: 0.97 - ETA: 17s - loss: 0.0887 - acc: 0.97 - ETA: 16s - loss: 0.0914 - acc: 0.97 - ETA: 16s - loss: 0.0913 - acc: 0.97 - ETA: 15s - loss: 0.0913 - acc: 0.97 - ETA: 14s - loss: 0.0941 - acc: 0.97 - ETA: 14s - loss: 0.0942 - acc: 0.97 - ETA: 13s - loss: 0.0940 - acc: 0.97 - ETA: 13s - loss: 0.0935 - acc: 0.97 - ETA: 12s - loss: 0.0918 - acc: 0.97 - ETA: 11s - loss: 0.0897 - acc: 0.97 - ETA: 11s - loss: 0.0886 - acc: 0.97 - ETA: 10s - loss: 0.0885 - acc: 0.97 - ETA: 10s - loss: 0.0883 - acc: 0.97 - ETA: 9s - loss: 0.0884 - acc: 0.9742 - ETA: 9s - loss: 0.0902 - acc: 0.973 - ETA: 8s - loss: 0.0927 - acc: 0.971 - ETA: 7s - loss: 0.0916 - acc: 0.972 - ETA: 7s - loss: 0.0924 - acc: 0.972 - ETA: 6s - loss: 0.0938 - acc: 0.971 - ETA: 6s - loss: 0.0935 - acc: 0.972 - ETA: 5s - loss: 0.0971 - acc: 0.970 - ETA: 5s - loss: 0.0966 - acc: 0.970 - ETA: 4s - loss: 0.0986 - acc: 0.970 - ETA: 3s - loss: 0.0996 - acc: 0.969 - ETA: 3s - loss: 0.0979 - acc: 0.970 - ETA: 2s - loss: 0.0989 - acc: 0.970 - ETA: 2s - loss: 0.0987 - acc: 0.970 - ETA: 1s - loss: 0.0975 - acc: 0.970 - ETA: 1s - loss: 0.0983 - acc: 0.970 - ETA: 0s - loss: 0.1008 - acc: 0.968 - 30s 20ms/step - loss: 0.1006 - acc: 0.9685 - val_loss: 0.2989 - val_acc: 0.8707\n",
      "Epoch 7/20\n",
      "1498/1498 [==============================] - ETA: 24s - loss: 0.0611 - acc: 0.96 - ETA: 24s - loss: 0.0717 - acc: 0.96 - ETA: 24s - loss: 0.0697 - acc: 0.97 - ETA: 23s - loss: 0.0589 - acc: 0.98 - ETA: 23s - loss: 0.0531 - acc: 0.98 - ETA: 23s - loss: 0.0550 - acc: 0.98 - ETA: 22s - loss: 0.0523 - acc: 0.98 - ETA: 22s - loss: 0.0655 - acc: 0.98 - ETA: 21s - loss: 0.0654 - acc: 0.98 - ETA: 20s - loss: 0.0630 - acc: 0.98 - ETA: 20s - loss: 0.0611 - acc: 0.98 - ETA: 20s - loss: 0.0588 - acc: 0.98 - ETA: 19s - loss: 0.0661 - acc: 0.98 - ETA: 19s - loss: 0.0683 - acc: 0.98 - ETA: 18s - loss: 0.0763 - acc: 0.97 - ETA: 18s - loss: 0.0769 - acc: 0.98 - ETA: 17s - loss: 0.0773 - acc: 0.97 - ETA: 17s - loss: 0.0771 - acc: 0.97 - ETA: 16s - loss: 0.0769 - acc: 0.97 - ETA: 15s - loss: 0.0763 - acc: 0.97 - ETA: 15s - loss: 0.0749 - acc: 0.98 - ETA: 14s - loss: 0.0759 - acc: 0.98 - ETA: 14s - loss: 0.0746 - acc: 0.98 - ETA: 13s - loss: 0.0757 - acc: 0.97 - ETA: 13s - loss: 0.0789 - acc: 0.97 - ETA: 12s - loss: 0.0793 - acc: 0.97 - ETA: 11s - loss: 0.0811 - acc: 0.97 - ETA: 11s - loss: 0.0819 - acc: 0.97 - ETA: 10s - loss: 0.0850 - acc: 0.97 - ETA: 10s - loss: 0.0883 - acc: 0.97 - ETA: 9s - loss: 0.0870 - acc: 0.9740 - ETA: 8s - loss: 0.0875 - acc: 0.973 - ETA: 8s - loss: 0.0864 - acc: 0.974 - ETA: 7s - loss: 0.0870 - acc: 0.974 - ETA: 7s - loss: 0.0867 - acc: 0.974 - ETA: 6s - loss: 0.0880 - acc: 0.974 - ETA: 5s - loss: 0.0876 - acc: 0.973 - ETA: 5s - loss: 0.0858 - acc: 0.974 - ETA: 4s - loss: 0.0857 - acc: 0.974 - ETA: 4s - loss: 0.0855 - acc: 0.974 - ETA: 3s - loss: 0.0858 - acc: 0.973 - ETA: 2s - loss: 0.0852 - acc: 0.974 - ETA: 2s - loss: 0.0845 - acc: 0.974 - ETA: 1s - loss: 0.0846 - acc: 0.973 - ETA: 1s - loss: 0.0857 - acc: 0.973 - ETA: 0s - loss: 0.0849 - acc: 0.973 - 31s 21ms/step - loss: 0.0849 - acc: 0.9740 - val_loss: 0.2507 - val_acc: 0.9128\n",
      "Epoch 8/20\n",
      "1498/1498 [==============================] - ETA: 26s - loss: 0.0344 - acc: 0.98 - ETA: 25s - loss: 0.0612 - acc: 0.97 - ETA: 25s - loss: 0.0477 - acc: 0.98 - ETA: 25s - loss: 0.0688 - acc: 0.97 - ETA: 24s - loss: 0.0608 - acc: 0.97 - ETA: 23s - loss: 0.0709 - acc: 0.97 - ETA: 23s - loss: 0.0737 - acc: 0.96 - ETA: 23s - loss: 0.0932 - acc: 0.96 - ETA: 22s - loss: 0.0877 - acc: 0.96 - ETA: 22s - loss: 0.0821 - acc: 0.96 - ETA: 21s - loss: 0.0837 - acc: 0.96 - ETA: 20s - loss: 0.0791 - acc: 0.96 - ETA: 20s - loss: 0.0761 - acc: 0.97 - ETA: 19s - loss: 0.0726 - acc: 0.97 - ETA: 19s - loss: 0.0729 - acc: 0.97 - ETA: 18s - loss: 0.0708 - acc: 0.97 - ETA: 17s - loss: 0.0678 - acc: 0.97 - ETA: 17s - loss: 0.0680 - acc: 0.97 - ETA: 16s - loss: 0.0676 - acc: 0.97 - ETA: 16s - loss: 0.0656 - acc: 0.97 - ETA: 15s - loss: 0.0664 - acc: 0.97 - ETA: 14s - loss: 0.0643 - acc: 0.97 - ETA: 14s - loss: 0.0655 - acc: 0.97 - ETA: 13s - loss: 0.0674 - acc: 0.97 - ETA: 13s - loss: 0.0679 - acc: 0.97 - ETA: 12s - loss: 0.0685 - acc: 0.97 - ETA: 11s - loss: 0.0680 - acc: 0.97 - ETA: 11s - loss: 0.0664 - acc: 0.97 - ETA: 10s - loss: 0.0654 - acc: 0.97 - ETA: 10s - loss: 0.0641 - acc: 0.97 - ETA: 9s - loss: 0.0655 - acc: 0.9793 - ETA: 8s - loss: 0.0674 - acc: 0.978 - ETA: 8s - loss: 0.0669 - acc: 0.978 - ETA: 7s - loss: 0.0672 - acc: 0.978 - ETA: 7s - loss: 0.0682 - acc: 0.978 - ETA: 6s - loss: 0.0671 - acc: 0.978 - ETA: 5s - loss: 0.0683 - acc: 0.978 - ETA: 5s - loss: 0.0683 - acc: 0.978 - ETA: 4s - loss: 0.0683 - acc: 0.978 - ETA: 4s - loss: 0.0674 - acc: 0.978 - ETA: 3s - loss: 0.0683 - acc: 0.978 - ETA: 2s - loss: 0.0721 - acc: 0.976 - ETA: 2s - loss: 0.0718 - acc: 0.976 - ETA: 1s - loss: 0.0713 - acc: 0.976 - ETA: 1s - loss: 0.0703 - acc: 0.977 - ETA: 0s - loss: 0.0714 - acc: 0.976 - 31s 21ms/step - loss: 0.0705 - acc: 0.9773 - val_loss: 0.1984 - val_acc: 0.9303\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1498/1498 [==============================] - ETA: 26s - loss: 0.1905 - acc: 0.92 - ETA: 26s - loss: 0.1287 - acc: 0.95 - ETA: 26s - loss: 0.1125 - acc: 0.96 - ETA: 25s - loss: 0.0932 - acc: 0.97 - ETA: 25s - loss: 0.0808 - acc: 0.97 - ETA: 24s - loss: 0.0759 - acc: 0.97 - ETA: 24s - loss: 0.0767 - acc: 0.97 - ETA: 23s - loss: 0.0735 - acc: 0.97 - ETA: 22s - loss: 0.0690 - acc: 0.97 - ETA: 22s - loss: 0.0688 - acc: 0.97 - ETA: 21s - loss: 0.0665 - acc: 0.97 - ETA: 20s - loss: 0.0670 - acc: 0.97 - ETA: 20s - loss: 0.0629 - acc: 0.98 - ETA: 19s - loss: 0.0632 - acc: 0.98 - ETA: 19s - loss: 0.0627 - acc: 0.98 - ETA: 18s - loss: 0.0654 - acc: 0.98 - ETA: 17s - loss: 0.0625 - acc: 0.98 - ETA: 17s - loss: 0.0622 - acc: 0.98 - ETA: 16s - loss: 0.0621 - acc: 0.98 - ETA: 16s - loss: 0.0609 - acc: 0.98 - ETA: 15s - loss: 0.0621 - acc: 0.98 - ETA: 14s - loss: 0.0608 - acc: 0.98 - ETA: 14s - loss: 0.0592 - acc: 0.98 - ETA: 13s - loss: 0.0576 - acc: 0.98 - ETA: 13s - loss: 0.0561 - acc: 0.98 - ETA: 12s - loss: 0.0546 - acc: 0.98 - ETA: 11s - loss: 0.0531 - acc: 0.98 - ETA: 11s - loss: 0.0564 - acc: 0.98 - ETA: 10s - loss: 0.0561 - acc: 0.98 - ETA: 10s - loss: 0.0545 - acc: 0.98 - ETA: 9s - loss: 0.0541 - acc: 0.9859 - ETA: 9s - loss: 0.0549 - acc: 0.985 - ETA: 8s - loss: 0.0547 - acc: 0.985 - ETA: 7s - loss: 0.0555 - acc: 0.985 - ETA: 7s - loss: 0.0568 - acc: 0.984 - ETA: 6s - loss: 0.0579 - acc: 0.984 - ETA: 6s - loss: 0.0577 - acc: 0.984 - ETA: 5s - loss: 0.0567 - acc: 0.984 - ETA: 4s - loss: 0.0566 - acc: 0.984 - ETA: 4s - loss: 0.0557 - acc: 0.985 - ETA: 3s - loss: 0.0565 - acc: 0.984 - ETA: 2s - loss: 0.0583 - acc: 0.984 - ETA: 2s - loss: 0.0604 - acc: 0.983 - ETA: 1s - loss: 0.0606 - acc: 0.983 - ETA: 1s - loss: 0.0597 - acc: 0.983 - ETA: 0s - loss: 0.0607 - acc: 0.983 - 31s 21ms/step - loss: 0.0600 - acc: 0.9836 - val_loss: 0.1692 - val_acc: 0.9412\n",
      "Epoch 10/20\n",
      "1498/1498 [==============================] - ETA: 27s - loss: 0.1430 - acc: 0.95 - ETA: 27s - loss: 0.1301 - acc: 0.96 - ETA: 26s - loss: 0.1063 - acc: 0.96 - ETA: 25s - loss: 0.0965 - acc: 0.96 - ETA: 25s - loss: 0.0873 - acc: 0.96 - ETA: 24s - loss: 0.0936 - acc: 0.96 - ETA: 24s - loss: 0.0827 - acc: 0.96 - ETA: 23s - loss: 0.0793 - acc: 0.97 - ETA: 23s - loss: 0.0846 - acc: 0.97 - ETA: 22s - loss: 0.0820 - acc: 0.97 - ETA: 21s - loss: 0.0765 - acc: 0.97 - ETA: 20s - loss: 0.0725 - acc: 0.97 - ETA: 20s - loss: 0.0688 - acc: 0.97 - ETA: 19s - loss: 0.0647 - acc: 0.98 - ETA: 19s - loss: 0.0626 - acc: 0.98 - ETA: 18s - loss: 0.0620 - acc: 0.98 - ETA: 17s - loss: 0.0593 - acc: 0.98 - ETA: 17s - loss: 0.0627 - acc: 0.98 - ETA: 16s - loss: 0.0639 - acc: 0.98 - ETA: 16s - loss: 0.0614 - acc: 0.98 - ETA: 15s - loss: 0.0620 - acc: 0.98 - ETA: 14s - loss: 0.0601 - acc: 0.98 - ETA: 14s - loss: 0.0590 - acc: 0.98 - ETA: 13s - loss: 0.0590 - acc: 0.98 - ETA: 12s - loss: 0.0595 - acc: 0.98 - ETA: 12s - loss: 0.0615 - acc: 0.98 - ETA: 11s - loss: 0.0617 - acc: 0.98 - ETA: 11s - loss: 0.0633 - acc: 0.98 - ETA: 10s - loss: 0.0617 - acc: 0.98 - ETA: 9s - loss: 0.0636 - acc: 0.9826 - ETA: 9s - loss: 0.0661 - acc: 0.981 - ETA: 8s - loss: 0.0661 - acc: 0.981 - ETA: 8s - loss: 0.0647 - acc: 0.982 - ETA: 7s - loss: 0.0644 - acc: 0.982 - ETA: 6s - loss: 0.0633 - acc: 0.982 - ETA: 6s - loss: 0.0620 - acc: 0.983 - ETA: 5s - loss: 0.0622 - acc: 0.983 - ETA: 5s - loss: 0.0611 - acc: 0.983 - ETA: 4s - loss: 0.0603 - acc: 0.983 - ETA: 3s - loss: 0.0604 - acc: 0.983 - ETA: 3s - loss: 0.0593 - acc: 0.983 - ETA: 2s - loss: 0.0584 - acc: 0.984 - ETA: 2s - loss: 0.0578 - acc: 0.984 - ETA: 1s - loss: 0.0568 - acc: 0.984 - ETA: 1s - loss: 0.0572 - acc: 0.983 - ETA: 0s - loss: 0.0575 - acc: 0.983 - 30s 20ms/step - loss: 0.0566 - acc: 0.9838 - val_loss: 0.1235 - val_acc: 0.9556\n",
      "Epoch 11/20\n",
      "1498/1498 [==============================] - ETA: 25s - loss: 0.0094 - acc: 1.00 - ETA: 25s - loss: 0.0138 - acc: 1.00 - ETA: 24s - loss: 0.0171 - acc: 1.00 - ETA: 23s - loss: 0.0174 - acc: 1.00 - ETA: 23s - loss: 0.0430 - acc: 0.99 - ETA: 22s - loss: 0.0380 - acc: 0.99 - ETA: 22s - loss: 0.0342 - acc: 0.99 - ETA: 22s - loss: 0.0316 - acc: 0.99 - ETA: 21s - loss: 0.0299 - acc: 0.99 - ETA: 21s - loss: 0.0280 - acc: 0.99 - ETA: 20s - loss: 0.0278 - acc: 0.99 - ETA: 20s - loss: 0.0319 - acc: 0.99 - ETA: 19s - loss: 0.0328 - acc: 0.99 - ETA: 19s - loss: 0.0335 - acc: 0.99 - ETA: 18s - loss: 0.0349 - acc: 0.99 - ETA: 18s - loss: 0.0348 - acc: 0.99 - ETA: 17s - loss: 0.0429 - acc: 0.98 - ETA: 16s - loss: 0.0439 - acc: 0.98 - ETA: 16s - loss: 0.0425 - acc: 0.98 - ETA: 15s - loss: 0.0427 - acc: 0.98 - ETA: 15s - loss: 0.0435 - acc: 0.98 - ETA: 14s - loss: 0.0442 - acc: 0.98 - ETA: 14s - loss: 0.0450 - acc: 0.98 - ETA: 13s - loss: 0.0436 - acc: 0.98 - ETA: 12s - loss: 0.0423 - acc: 0.98 - ETA: 12s - loss: 0.0410 - acc: 0.98 - ETA: 11s - loss: 0.0420 - acc: 0.98 - ETA: 11s - loss: 0.0412 - acc: 0.98 - ETA: 10s - loss: 0.0401 - acc: 0.98 - ETA: 9s - loss: 0.0391 - acc: 0.9893 - ETA: 9s - loss: 0.0400 - acc: 0.989 - ETA: 8s - loss: 0.0406 - acc: 0.989 - ETA: 8s - loss: 0.0445 - acc: 0.987 - ETA: 7s - loss: 0.0474 - acc: 0.986 - ETA: 6s - loss: 0.0464 - acc: 0.986 - ETA: 6s - loss: 0.0465 - acc: 0.986 - ETA: 5s - loss: 0.0456 - acc: 0.987 - ETA: 5s - loss: 0.0459 - acc: 0.987 - ETA: 4s - loss: 0.0473 - acc: 0.986 - ETA: 4s - loss: 0.0465 - acc: 0.986 - ETA: 3s - loss: 0.0458 - acc: 0.987 - ETA: 2s - loss: 0.0465 - acc: 0.987 - ETA: 2s - loss: 0.0468 - acc: 0.986 - ETA: 1s - loss: 0.0486 - acc: 0.986 - ETA: 1s - loss: 0.0501 - acc: 0.985 - ETA: 0s - loss: 0.0492 - acc: 0.986 - 31s 21ms/step - loss: 0.0487 - acc: 0.9863 - val_loss: 0.1513 - val_acc: 0.9486\n",
      "Epoch 12/20\n",
      "1498/1498 [==============================] - ETA: 26s - loss: 0.0449 - acc: 0.98 - ETA: 26s - loss: 0.0293 - acc: 0.99 - ETA: 26s - loss: 0.0237 - acc: 0.99 - ETA: 26s - loss: 0.0404 - acc: 0.98 - ETA: 25s - loss: 0.0340 - acc: 0.99 - ETA: 24s - loss: 0.0310 - acc: 0.99 - ETA: 24s - loss: 0.0284 - acc: 0.99 - ETA: 23s - loss: 0.0261 - acc: 0.99 - ETA: 23s - loss: 0.0305 - acc: 0.99 - ETA: 22s - loss: 0.0323 - acc: 0.99 - ETA: 21s - loss: 0.0304 - acc: 0.99 - ETA: 20s - loss: 0.0333 - acc: 0.99 - ETA: 20s - loss: 0.0316 - acc: 0.99 - ETA: 19s - loss: 0.0303 - acc: 0.99 - ETA: 19s - loss: 0.0308 - acc: 0.99 - ETA: 18s - loss: 0.0336 - acc: 0.99 - ETA: 17s - loss: 0.0323 - acc: 0.99 - ETA: 17s - loss: 0.0349 - acc: 0.98 - ETA: 16s - loss: 0.0383 - acc: 0.98 - ETA: 16s - loss: 0.0369 - acc: 0.98 - ETA: 15s - loss: 0.0358 - acc: 0.98 - ETA: 14s - loss: 0.0393 - acc: 0.98 - ETA: 14s - loss: 0.0387 - acc: 0.98 - ETA: 13s - loss: 0.0390 - acc: 0.98 - ETA: 13s - loss: 0.0400 - acc: 0.98 - ETA: 12s - loss: 0.0433 - acc: 0.98 - ETA: 12s - loss: 0.0420 - acc: 0.98 - ETA: 11s - loss: 0.0416 - acc: 0.98 - ETA: 10s - loss: 0.0407 - acc: 0.98 - ETA: 10s - loss: 0.0396 - acc: 0.98 - ETA: 9s - loss: 0.0387 - acc: 0.9884 - ETA: 9s - loss: 0.0377 - acc: 0.988 - ETA: 8s - loss: 0.0368 - acc: 0.989 - ETA: 7s - loss: 0.0370 - acc: 0.989 - ETA: 7s - loss: 0.0363 - acc: 0.989 - ETA: 6s - loss: 0.0385 - acc: 0.988 - ETA: 5s - loss: 0.0419 - acc: 0.987 - ETA: 5s - loss: 0.0420 - acc: 0.987 - ETA: 4s - loss: 0.0417 - acc: 0.987 - ETA: 4s - loss: 0.0417 - acc: 0.987 - ETA: 3s - loss: 0.0421 - acc: 0.987 - ETA: 2s - loss: 0.0427 - acc: 0.987 - ETA: 2s - loss: 0.0419 - acc: 0.987 - ETA: 1s - loss: 0.0412 - acc: 0.987 - ETA: 1s - loss: 0.0418 - acc: 0.987 - ETA: 0s - loss: 0.0430 - acc: 0.986 - 31s 21ms/step - loss: 0.0435 - acc: 0.9866 - val_loss: 0.1089 - val_acc: 0.9630\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1498/1498 [==============================] - ETA: 26s - loss: 0.0280 - acc: 0.98 - ETA: 26s - loss: 0.0344 - acc: 0.98 - ETA: 25s - loss: 0.0366 - acc: 0.98 - ETA: 25s - loss: 0.0309 - acc: 0.98 - ETA: 24s - loss: 0.0441 - acc: 0.97 - ETA: 24s - loss: 0.0388 - acc: 0.98 - ETA: 23s - loss: 0.0355 - acc: 0.98 - ETA: 23s - loss: 0.0347 - acc: 0.98 - ETA: 22s - loss: 0.0376 - acc: 0.98 - ETA: 21s - loss: 0.0361 - acc: 0.98 - ETA: 21s - loss: 0.0353 - acc: 0.98 - ETA: 20s - loss: 0.0335 - acc: 0.98 - ETA: 20s - loss: 0.0383 - acc: 0.98 - ETA: 19s - loss: 0.0401 - acc: 0.98 - ETA: 19s - loss: 0.0385 - acc: 0.98 - ETA: 18s - loss: 0.0396 - acc: 0.98 - ETA: 17s - loss: 0.0379 - acc: 0.98 - ETA: 17s - loss: 0.0378 - acc: 0.98 - ETA: 16s - loss: 0.0362 - acc: 0.98 - ETA: 16s - loss: 0.0367 - acc: 0.98 - ETA: 15s - loss: 0.0363 - acc: 0.98 - ETA: 14s - loss: 0.0351 - acc: 0.98 - ETA: 14s - loss: 0.0341 - acc: 0.98 - ETA: 13s - loss: 0.0347 - acc: 0.98 - ETA: 13s - loss: 0.0341 - acc: 0.98 - ETA: 12s - loss: 0.0331 - acc: 0.98 - ETA: 11s - loss: 0.0338 - acc: 0.98 - ETA: 11s - loss: 0.0329 - acc: 0.98 - ETA: 10s - loss: 0.0321 - acc: 0.99 - ETA: 10s - loss: 0.0321 - acc: 0.98 - ETA: 9s - loss: 0.0360 - acc: 0.9892 - ETA: 8s - loss: 0.0366 - acc: 0.989 - ETA: 8s - loss: 0.0366 - acc: 0.988 - ETA: 7s - loss: 0.0371 - acc: 0.989 - ETA: 7s - loss: 0.0376 - acc: 0.988 - ETA: 6s - loss: 0.0368 - acc: 0.989 - ETA: 5s - loss: 0.0360 - acc: 0.989 - ETA: 5s - loss: 0.0357 - acc: 0.989 - ETA: 4s - loss: 0.0350 - acc: 0.989 - ETA: 4s - loss: 0.0343 - acc: 0.990 - ETA: 3s - loss: 0.0343 - acc: 0.989 - ETA: 2s - loss: 0.0346 - acc: 0.989 - ETA: 2s - loss: 0.0347 - acc: 0.989 - ETA: 1s - loss: 0.0360 - acc: 0.988 - ETA: 1s - loss: 0.0353 - acc: 0.989 - ETA: 0s - loss: 0.0346 - acc: 0.989 - 31s 21ms/step - loss: 0.0354 - acc: 0.9892 - val_loss: 0.1140 - val_acc: 0.9618\n",
      "Epoch 14/20\n",
      "1498/1498 [==============================] - ETA: 28s - loss: 0.1331 - acc: 0.95 - ETA: 26s - loss: 0.1029 - acc: 0.96 - ETA: 24s - loss: 0.0707 - acc: 0.97 - ETA: 23s - loss: 0.0546 - acc: 0.98 - ETA: 23s - loss: 0.0471 - acc: 0.98 - ETA: 23s - loss: 0.0404 - acc: 0.98 - ETA: 23s - loss: 0.0359 - acc: 0.98 - ETA: 22s - loss: 0.0353 - acc: 0.98 - ETA: 22s - loss: 0.0324 - acc: 0.98 - ETA: 21s - loss: 0.0325 - acc: 0.98 - ETA: 21s - loss: 0.0308 - acc: 0.98 - ETA: 20s - loss: 0.0292 - acc: 0.99 - ETA: 19s - loss: 0.0297 - acc: 0.98 - ETA: 19s - loss: 0.0280 - acc: 0.99 - ETA: 18s - loss: 0.0282 - acc: 0.98 - ETA: 18s - loss: 0.0271 - acc: 0.99 - ETA: 17s - loss: 0.0273 - acc: 0.98 - ETA: 17s - loss: 0.0267 - acc: 0.99 - ETA: 16s - loss: 0.0257 - acc: 0.99 - ETA: 15s - loss: 0.0321 - acc: 0.98 - ETA: 15s - loss: 0.0333 - acc: 0.98 - ETA: 14s - loss: 0.0323 - acc: 0.98 - ETA: 14s - loss: 0.0313 - acc: 0.98 - ETA: 13s - loss: 0.0304 - acc: 0.99 - ETA: 12s - loss: 0.0294 - acc: 0.99 - ETA: 12s - loss: 0.0285 - acc: 0.99 - ETA: 11s - loss: 0.0300 - acc: 0.99 - ETA: 11s - loss: 0.0307 - acc: 0.99 - ETA: 10s - loss: 0.0312 - acc: 0.99 - ETA: 10s - loss: 0.0317 - acc: 0.98 - ETA: 9s - loss: 0.0311 - acc: 0.9902 - ETA: 8s - loss: 0.0303 - acc: 0.990 - ETA: 8s - loss: 0.0299 - acc: 0.990 - ETA: 7s - loss: 0.0325 - acc: 0.989 - ETA: 7s - loss: 0.0328 - acc: 0.989 - ETA: 6s - loss: 0.0330 - acc: 0.989 - ETA: 5s - loss: 0.0323 - acc: 0.989 - ETA: 5s - loss: 0.0324 - acc: 0.989 - ETA: 4s - loss: 0.0318 - acc: 0.989 - ETA: 4s - loss: 0.0322 - acc: 0.989 - ETA: 3s - loss: 0.0330 - acc: 0.989 - ETA: 2s - loss: 0.0328 - acc: 0.989 - ETA: 2s - loss: 0.0331 - acc: 0.989 - ETA: 1s - loss: 0.0360 - acc: 0.988 - ETA: 1s - loss: 0.0381 - acc: 0.987 - ETA: 0s - loss: 0.0381 - acc: 0.987 - 31s 20ms/step - loss: 0.0376 - acc: 0.9880 - val_loss: 0.2267 - val_acc: 0.9334\n",
      "Epoch 15/20\n",
      "1498/1498 [==============================] - ETA: 25s - loss: 0.0408 - acc: 0.97 - ETA: 25s - loss: 0.0246 - acc: 0.98 - ETA: 25s - loss: 0.0184 - acc: 0.99 - ETA: 25s - loss: 0.0156 - acc: 0.99 - ETA: 24s - loss: 0.0137 - acc: 0.99 - ETA: 23s - loss: 0.0207 - acc: 0.99 - ETA: 23s - loss: 0.0191 - acc: 0.99 - ETA: 23s - loss: 0.0271 - acc: 0.98 - ETA: 22s - loss: 0.0375 - acc: 0.98 - ETA: 21s - loss: 0.0363 - acc: 0.98 - ETA: 21s - loss: 0.0339 - acc: 0.98 - ETA: 20s - loss: 0.0379 - acc: 0.98 - ETA: 20s - loss: 0.0357 - acc: 0.98 - ETA: 19s - loss: 0.0345 - acc: 0.98 - ETA: 19s - loss: 0.0337 - acc: 0.98 - ETA: 18s - loss: 0.0351 - acc: 0.98 - ETA: 17s - loss: 0.0351 - acc: 0.98 - ETA: 17s - loss: 0.0362 - acc: 0.98 - ETA: 16s - loss: 0.0352 - acc: 0.98 - ETA: 16s - loss: 0.0349 - acc: 0.98 - ETA: 15s - loss: 0.0336 - acc: 0.98 - ETA: 14s - loss: 0.0351 - acc: 0.98 - ETA: 14s - loss: 0.0339 - acc: 0.98 - ETA: 13s - loss: 0.0329 - acc: 0.98 - ETA: 12s - loss: 0.0318 - acc: 0.98 - ETA: 12s - loss: 0.0308 - acc: 0.99 - ETA: 11s - loss: 0.0299 - acc: 0.99 - ETA: 11s - loss: 0.0291 - acc: 0.99 - ETA: 10s - loss: 0.0306 - acc: 0.99 - ETA: 9s - loss: 0.0324 - acc: 0.9888 - ETA: 9s - loss: 0.0316 - acc: 0.989 - ETA: 8s - loss: 0.0323 - acc: 0.988 - ETA: 8s - loss: 0.0361 - acc: 0.988 - ETA: 7s - loss: 0.0353 - acc: 0.988 - ETA: 7s - loss: 0.0352 - acc: 0.988 - ETA: 6s - loss: 0.0369 - acc: 0.988 - ETA: 5s - loss: 0.0376 - acc: 0.988 - ETA: 5s - loss: 0.0368 - acc: 0.988 - ETA: 4s - loss: 0.0361 - acc: 0.988 - ETA: 4s - loss: 0.0354 - acc: 0.989 - ETA: 3s - loss: 0.0347 - acc: 0.989 - ETA: 2s - loss: 0.0341 - acc: 0.989 - ETA: 2s - loss: 0.0335 - acc: 0.989 - ETA: 1s - loss: 0.0341 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 0s - loss: 0.0330 - acc: 0.990 - 30s 20ms/step - loss: 0.0326 - acc: 0.9903 - val_loss: 0.0519 - val_acc: 0.9833\n",
      "Epoch 16/20\n",
      "1498/1498 [==============================] - ETA: 24s - loss: 0.0484 - acc: 0.98 - ETA: 24s - loss: 0.0289 - acc: 0.99 - ETA: 24s - loss: 0.0330 - acc: 0.99 - ETA: 24s - loss: 0.0365 - acc: 0.99 - ETA: 23s - loss: 0.0357 - acc: 0.98 - ETA: 22s - loss: 0.0313 - acc: 0.99 - ETA: 22s - loss: 0.0276 - acc: 0.99 - ETA: 21s - loss: 0.0250 - acc: 0.99 - ETA: 20s - loss: 0.0231 - acc: 0.99 - ETA: 20s - loss: 0.0212 - acc: 0.99 - ETA: 19s - loss: 0.0237 - acc: 0.99 - ETA: 19s - loss: 0.0222 - acc: 0.99 - ETA: 18s - loss: 0.0209 - acc: 0.99 - ETA: 18s - loss: 0.0198 - acc: 0.99 - ETA: 17s - loss: 0.0189 - acc: 0.99 - ETA: 17s - loss: 0.0185 - acc: 0.99 - ETA: 16s - loss: 0.0192 - acc: 0.99 - ETA: 16s - loss: 0.0187 - acc: 0.99 - ETA: 15s - loss: 0.0204 - acc: 0.99 - ETA: 15s - loss: 0.0242 - acc: 0.99 - ETA: 14s - loss: 0.0255 - acc: 0.99 - ETA: 13s - loss: 0.0247 - acc: 0.99 - ETA: 13s - loss: 0.0242 - acc: 0.99 - ETA: 12s - loss: 0.0236 - acc: 0.99 - ETA: 12s - loss: 0.0250 - acc: 0.99 - ETA: 11s - loss: 0.0273 - acc: 0.99 - ETA: 11s - loss: 0.0271 - acc: 0.99 - ETA: 10s - loss: 0.0286 - acc: 0.99 - ETA: 10s - loss: 0.0278 - acc: 0.99 - ETA: 9s - loss: 0.0271 - acc: 0.9932 - ETA: 9s - loss: 0.0264 - acc: 0.993 - ETA: 8s - loss: 0.0257 - acc: 0.993 - ETA: 7s - loss: 0.0294 - acc: 0.992 - ETA: 7s - loss: 0.0305 - acc: 0.991 - ETA: 6s - loss: 0.0312 - acc: 0.991 - ETA: 6s - loss: 0.0305 - acc: 0.991 - ETA: 5s - loss: 0.0300 - acc: 0.992 - ETA: 5s - loss: 0.0294 - acc: 0.992 - ETA: 4s - loss: 0.0302 - acc: 0.991 - ETA: 3s - loss: 0.0297 - acc: 0.992 - ETA: 3s - loss: 0.0292 - acc: 0.992 - ETA: 2s - loss: 0.0298 - acc: 0.992 - ETA: 2s - loss: 0.0296 - acc: 0.992 - ETA: 1s - loss: 0.0291 - acc: 0.992 - ETA: 1s - loss: 0.0287 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - 30s 20ms/step - loss: 0.0280 - acc: 0.9927 - val_loss: 0.0632 - val_acc: 0.9825\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1498/1498 [==============================] - ETA: 28s - loss: 0.0544 - acc: 0.98 - ETA: 27s - loss: 0.0426 - acc: 0.98 - ETA: 26s - loss: 0.0653 - acc: 0.97 - ETA: 25s - loss: 0.0499 - acc: 0.98 - ETA: 24s - loss: 0.0409 - acc: 0.98 - ETA: 24s - loss: 0.0349 - acc: 0.98 - ETA: 23s - loss: 0.0321 - acc: 0.99 - ETA: 23s - loss: 0.0286 - acc: 0.99 - ETA: 22s - loss: 0.0262 - acc: 0.99 - ETA: 21s - loss: 0.0258 - acc: 0.99 - ETA: 21s - loss: 0.0244 - acc: 0.99 - ETA: 20s - loss: 0.0232 - acc: 0.99 - ETA: 20s - loss: 0.0224 - acc: 0.99 - ETA: 19s - loss: 0.0213 - acc: 0.99 - ETA: 18s - loss: 0.0202 - acc: 0.99 - ETA: 18s - loss: 0.0191 - acc: 0.99 - ETA: 17s - loss: 0.0183 - acc: 0.99 - ETA: 17s - loss: 0.0222 - acc: 0.99 - ETA: 16s - loss: 0.0224 - acc: 0.99 - ETA: 15s - loss: 0.0243 - acc: 0.99 - ETA: 15s - loss: 0.0271 - acc: 0.99 - ETA: 14s - loss: 0.0261 - acc: 0.99 - ETA: 13s - loss: 0.0252 - acc: 0.99 - ETA: 13s - loss: 0.0267 - acc: 0.99 - ETA: 12s - loss: 0.0281 - acc: 0.99 - ETA: 12s - loss: 0.0272 - acc: 0.99 - ETA: 11s - loss: 0.0269 - acc: 0.99 - ETA: 10s - loss: 0.0261 - acc: 0.99 - ETA: 10s - loss: 0.0267 - acc: 0.99 - ETA: 9s - loss: 0.0262 - acc: 0.9917 - ETA: 9s - loss: 0.0257 - acc: 0.991 - ETA: 8s - loss: 0.0251 - acc: 0.992 - ETA: 7s - loss: 0.0245 - acc: 0.992 - ETA: 7s - loss: 0.0240 - acc: 0.992 - ETA: 6s - loss: 0.0234 - acc: 0.992 - ETA: 6s - loss: 0.0245 - acc: 0.992 - ETA: 5s - loss: 0.0254 - acc: 0.992 - ETA: 5s - loss: 0.0258 - acc: 0.992 - ETA: 4s - loss: 0.0252 - acc: 0.992 - ETA: 3s - loss: 0.0247 - acc: 0.992 - ETA: 3s - loss: 0.0245 - acc: 0.992 - ETA: 2s - loss: 0.0243 - acc: 0.992 - ETA: 2s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.992 - 30s 20ms/step - loss: 0.0256 - acc: 0.9925 - val_loss: 0.0468 - val_acc: 0.9883\n",
      "Epoch 18/20\n",
      "1498/1498 [==============================] - ETA: 27s - loss: 0.0168 - acc: 1.00 - ETA: 26s - loss: 0.0141 - acc: 1.00 - ETA: 25s - loss: 0.0121 - acc: 1.00 - ETA: 25s - loss: 0.0105 - acc: 1.00 - ETA: 25s - loss: 0.0288 - acc: 0.99 - ETA: 24s - loss: 0.0250 - acc: 0.99 - ETA: 23s - loss: 0.0224 - acc: 0.99 - ETA: 23s - loss: 0.0278 - acc: 0.99 - ETA: 22s - loss: 0.0357 - acc: 0.99 - ETA: 22s - loss: 0.0359 - acc: 0.99 - ETA: 21s - loss: 0.0332 - acc: 0.99 - ETA: 20s - loss: 0.0310 - acc: 0.99 - ETA: 20s - loss: 0.0297 - acc: 0.99 - ETA: 19s - loss: 0.0281 - acc: 0.99 - ETA: 18s - loss: 0.0265 - acc: 0.99 - ETA: 18s - loss: 0.0260 - acc: 0.99 - ETA: 17s - loss: 0.0254 - acc: 0.99 - ETA: 17s - loss: 0.0242 - acc: 0.99 - ETA: 16s - loss: 0.0232 - acc: 0.99 - ETA: 16s - loss: 0.0286 - acc: 0.99 - ETA: 15s - loss: 0.0281 - acc: 0.99 - ETA: 14s - loss: 0.0275 - acc: 0.99 - ETA: 14s - loss: 0.0265 - acc: 0.99 - ETA: 13s - loss: 0.0257 - acc: 0.99 - ETA: 13s - loss: 0.0259 - acc: 0.99 - ETA: 12s - loss: 0.0251 - acc: 0.99 - ETA: 11s - loss: 0.0263 - acc: 0.99 - ETA: 11s - loss: 0.0257 - acc: 0.99 - ETA: 10s - loss: 0.0283 - acc: 0.99 - ETA: 10s - loss: 0.0275 - acc: 0.99 - ETA: 9s - loss: 0.0271 - acc: 0.9934 - ETA: 8s - loss: 0.0276 - acc: 0.993 - ETA: 8s - loss: 0.0282 - acc: 0.992 - ETA: 7s - loss: 0.0276 - acc: 0.993 - ETA: 7s - loss: 0.0270 - acc: 0.993 - ETA: 6s - loss: 0.0264 - acc: 0.993 - ETA: 5s - loss: 0.0260 - acc: 0.993 - ETA: 5s - loss: 0.0257 - acc: 0.993 - ETA: 4s - loss: 0.0255 - acc: 0.994 - ETA: 4s - loss: 0.0249 - acc: 0.994 - ETA: 3s - loss: 0.0257 - acc: 0.993 - ETA: 2s - loss: 0.0265 - acc: 0.993 - ETA: 2s - loss: 0.0261 - acc: 0.993 - ETA: 1s - loss: 0.0256 - acc: 0.993 - ETA: 1s - loss: 0.0252 - acc: 0.993 - ETA: 0s - loss: 0.0249 - acc: 0.993 - 31s 21ms/step - loss: 0.0260 - acc: 0.9933 - val_loss: 0.0360 - val_acc: 0.9891\n",
      "Epoch 19/20\n",
      "1498/1498 [==============================] - ETA: 25s - loss: 0.0781 - acc: 0.98 - ETA: 25s - loss: 0.0417 - acc: 0.99 - ETA: 25s - loss: 0.0455 - acc: 0.98 - ETA: 24s - loss: 0.0357 - acc: 0.99 - ETA: 24s - loss: 0.0300 - acc: 0.99 - ETA: 23s - loss: 0.0267 - acc: 0.99 - ETA: 23s - loss: 0.0241 - acc: 0.99 - ETA: 22s - loss: 0.0215 - acc: 0.99 - ETA: 22s - loss: 0.0235 - acc: 0.99 - ETA: 21s - loss: 0.0291 - acc: 0.99 - ETA: 21s - loss: 0.0276 - acc: 0.99 - ETA: 20s - loss: 0.0256 - acc: 0.99 - ETA: 19s - loss: 0.0241 - acc: 0.99 - ETA: 19s - loss: 0.0237 - acc: 0.99 - ETA: 18s - loss: 0.0234 - acc: 0.99 - ETA: 18s - loss: 0.0234 - acc: 0.99 - ETA: 17s - loss: 0.0232 - acc: 0.99 - ETA: 16s - loss: 0.0222 - acc: 0.99 - ETA: 16s - loss: 0.0213 - acc: 0.99 - ETA: 15s - loss: 0.0204 - acc: 0.99 - ETA: 15s - loss: 0.0215 - acc: 0.99 - ETA: 14s - loss: 0.0208 - acc: 0.99 - ETA: 14s - loss: 0.0201 - acc: 0.99 - ETA: 13s - loss: 0.0194 - acc: 0.99 - ETA: 12s - loss: 0.0188 - acc: 0.99 - ETA: 12s - loss: 0.0184 - acc: 0.99 - ETA: 11s - loss: 0.0180 - acc: 0.99 - ETA: 11s - loss: 0.0176 - acc: 0.99 - ETA: 10s - loss: 0.0170 - acc: 0.99 - ETA: 10s - loss: 0.0171 - acc: 0.99 - ETA: 9s - loss: 0.0181 - acc: 0.9945 - ETA: 8s - loss: 0.0182 - acc: 0.994 - ETA: 8s - loss: 0.0178 - acc: 0.994 - ETA: 7s - loss: 0.0175 - acc: 0.994 - ETA: 7s - loss: 0.0171 - acc: 0.994 - ETA: 6s - loss: 0.0171 - acc: 0.995 - ETA: 5s - loss: 0.0168 - acc: 0.995 - ETA: 5s - loss: 0.0196 - acc: 0.994 - ETA: 4s - loss: 0.0193 - acc: 0.994 - ETA: 4s - loss: 0.0193 - acc: 0.994 - ETA: 3s - loss: 0.0191 - acc: 0.994 - ETA: 2s - loss: 0.0187 - acc: 0.994 - ETA: 2s - loss: 0.0183 - acc: 0.994 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0195 - acc: 0.994 - 30s 20ms/step - loss: 0.0193 - acc: 0.9943 - val_loss: 0.0646 - val_acc: 0.9805\n",
      "Epoch 20/20\n",
      "1498/1498 [==============================] - ETA: 26s - loss: 0.0743 - acc: 0.98 - ETA: 26s - loss: 0.0690 - acc: 0.98 - ETA: 26s - loss: 0.0467 - acc: 0.98 - ETA: 28s - loss: 0.0359 - acc: 0.99 - ETA: 27s - loss: 0.0296 - acc: 0.99 - ETA: 26s - loss: 0.0270 - acc: 0.99 - ETA: 25s - loss: 0.0238 - acc: 0.99 - ETA: 24s - loss: 0.0212 - acc: 0.99 - ETA: 24s - loss: 0.0193 - acc: 0.99 - ETA: 23s - loss: 0.0176 - acc: 0.99 - ETA: 22s - loss: 0.0163 - acc: 0.99 - ETA: 22s - loss: 0.0157 - acc: 0.99 - ETA: 21s - loss: 0.0210 - acc: 0.99 - ETA: 20s - loss: 0.0198 - acc: 0.99 - ETA: 20s - loss: 0.0234 - acc: 0.99 - ETA: 19s - loss: 0.0221 - acc: 0.99 - ETA: 18s - loss: 0.0218 - acc: 0.99 - ETA: 18s - loss: 0.0229 - acc: 0.99 - ETA: 17s - loss: 0.0219 - acc: 0.99 - ETA: 16s - loss: 0.0210 - acc: 0.99 - ETA: 16s - loss: 0.0202 - acc: 0.99 - ETA: 15s - loss: 0.0207 - acc: 0.99 - ETA: 14s - loss: 0.0203 - acc: 0.99 - ETA: 14s - loss: 0.0223 - acc: 0.99 - ETA: 13s - loss: 0.0215 - acc: 0.99 - ETA: 13s - loss: 0.0210 - acc: 0.99 - ETA: 12s - loss: 0.0204 - acc: 0.99 - ETA: 11s - loss: 0.0198 - acc: 0.99 - ETA: 11s - loss: 0.0193 - acc: 0.99 - ETA: 10s - loss: 0.0188 - acc: 0.99 - ETA: 9s - loss: 0.0204 - acc: 0.9950 - ETA: 9s - loss: 0.0200 - acc: 0.995 - ETA: 8s - loss: 0.0198 - acc: 0.995 - ETA: 8s - loss: 0.0194 - acc: 0.995 - ETA: 7s - loss: 0.0193 - acc: 0.995 - ETA: 6s - loss: 0.0188 - acc: 0.995 - ETA: 6s - loss: 0.0185 - acc: 0.995 - ETA: 5s - loss: 0.0181 - acc: 0.995 - ETA: 4s - loss: 0.0177 - acc: 0.996 - ETA: 4s - loss: 0.0174 - acc: 0.996 - ETA: 3s - loss: 0.0171 - acc: 0.996 - ETA: 2s - loss: 0.0178 - acc: 0.995 - ETA: 2s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - 32s 21ms/step - loss: 0.0169 - acc: 0.9957 - val_loss: 0.0271 - val_acc: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26ff6fd4048>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m= model()\n",
    "\n",
    "m.fit(sequences_matrix,y,epochs=20, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(sequences_matrix,y,random_state = 100,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pd = m.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = m.evaluate(sequences_matrix,y,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.016674333514872, 0.9955607476635514]\n"
     ]
    }
   ],
   "source": [
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(ytest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from Keras.util import accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accuracy = accuracy_score(y_pd,ytest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accuracy = c.evaluate(test_sequences_matrix,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "m.save('LeaveModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quer For Leave"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m = load_model('LeaveModel.h5')\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= Dataset['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(Dataset['sub_intent_one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"update me about leave approving authority\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i want leave for today\"\n",
    "query = \"i want sick leaves for 2 days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Approve all leaves on my behalf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how many half days have i availed\"\n",
    "query = \"do i have any absent for last month\"\n",
    "query = \"how many leaves have i availed\"\n",
    "query = \"inform me about how many leaves have i taken\"\n",
    "query = \"is my leave approved\"\n",
    "#INQUIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'is office off on 9th november'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is leave encashment'\n",
    "#query = 'can you please tell me about leave encashment procedure'\n",
    "#query = 'i want to encash my leaves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i am feeling sick\"\n",
    "query = \"i dont feel i can work today\"\n",
    "query = \"i dont want to come \"\n",
    "query = \"will office remain close tomorrow\"\n",
    "query = 'who is my leave approving authority'\n",
    "#query = 'I won’t be able to come to office today'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'i want sick leave for today'\n",
    "query = 'Shiza is not feeling well'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'is', 'my', 'leav', 'approv', 'author']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = token_stems(sent)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test = ([list(sen)])\n",
    "sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.1458264e-11 2.4023051e-11 6.7832247e-13 1.0000000e+00]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = m.predict(sen_sequences_matrix)\n",
    "print(score)\n",
    "print(max(score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inquiry 3\n",
      "Request 0\n",
      "Emottional 2\n",
      "Approval 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Inquiry 3\")\n",
    "print(\"Request 0\")\n",
    "print(\"Emottional 2\")\n",
    "print(\"Approval 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_intent = leave(\"i have to go to doctor\") \n",
    "print(leave_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave(name):\n",
    "    sub_intent = leave_module(name)\n",
    "                #for sub_intents\n",
    "    if(sub_intent == 'EmotionlLeave'):\n",
    "        sub_intent_two = emotional_leave(name)\n",
    "                    #return sub_intent_two\n",
    "    elif(sub_intent == 'LeaveApproval'):\n",
    "        sub_intent_two = leave_approval(name)\n",
    "                    #return sub_intent_two\n",
    "    elif(sub_intent == 'LeaveRequest'):\n",
    "        sub_intent_two = classify_leaverequest(name)\n",
    "    else:\n",
    "        sub_intent_two = classify_leaveinquiry(name)\n",
    "    return(sub_intent_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_module(name):\n",
    "        sub_intent = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        docs= train['Data']\n",
    "        tokens = []\n",
    "        for i in docs:\n",
    "            temp = token_stems(i)\n",
    "            tokens.append(temp)\n",
    "\n",
    "        x, y = np.asarray(tokens) , np.asarray(train['Type'])\n",
    "        xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)\n",
    "        max_len=200\n",
    "        max_words = 20000\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "        tok.fit_on_texts(x)\n",
    "        sequences = tok.texts_to_sequences(x)\n",
    "        test_sequences = tok.texts_to_sequences(xtest)\n",
    "\n",
    "        \n",
    "        score = leave_model(name,tok)\n",
    "        print(score)\n",
    "        if((score[0][0] > score [0][1])&(score[0][0] > score [0][2])&(score[0][0] > score [0][3])):\n",
    "            sub_intent = 'LeaveRequest'\n",
    "        elif((score[0][1] > score [0][0])&(score[0][1] > score [0][2])&(score[0][1] > score [0][3])):\n",
    "            sub_intent = 'LeaveApproval'\n",
    "        elif((score[0][2] > score [0][1])&(score[0][2] > score [0][0])&(score[0][2] > score [0][3])):\n",
    "            sub_intent = 'EmotionlLeave'\n",
    "        else:\n",
    "            sub_intent = 'LeaveInquiry'\n",
    "        return sub_intent\n",
    "\n",
    "\n",
    "\n",
    "def leave_model(name,tok):\n",
    "        top_intent = ''\n",
    "        user_response = name\n",
    "        max_len=200\n",
    "        sen = token_stems(user_response)\n",
    "        sen_test = ([list(sen)])\n",
    "        print(sen_test)\n",
    "        sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "        sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)\n",
    "        m = load_model('Leave_Model_final.h5')\n",
    "        score = m.predict(sen_sequences_matrix)\n",
    "        K.clear_session()\n",
    "            #return(\"reach till here\")\n",
    "        print(user_response)\n",
    "        user_response=\"\"\n",
    "        sen=\"\"\n",
    "        sen_test=\"\"\n",
    "        intent=\"\"\n",
    "        Score=\"\"\n",
    "        print(score)\n",
    "        return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def classify_leaverequest(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['Type']== 'LeaveRequest']\n",
    "        text = train['Data']\n",
    "        \n",
    "        half_day = train[train['sub_type_two'] == 'half_day']\n",
    "        leave = train[train['sub_type_two'] == 'leave']\n",
    "        \n",
    "       \n",
    "        gen_half_day = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in half_day['Data']]\n",
    "        gen_leave = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in leave['Data']]\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "        dictionary_half_day = gensim.corpora.Dictionary(gen_half_day)\n",
    "        dictionary_leave = gensim.corpora.Dictionary(gen_leave)\n",
    "       \n",
    "        \n",
    "        corpus_half_day = [dictionary_half_day.doc2bow(gen_half_day) for gen_half_day in gen_half_day]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_half_day  = gensim.models.TfidfModel(corpus_half_day)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_half_day = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\half_day.txt',tf_idf_half_day[corpus_half_day],\n",
    "                                              num_features=len(dictionary_half_day))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_leave = [dictionary_leave.doc2bow(gen_leave) for gen_leave in gen_leave]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave  = gensim.models.TfidfModel(corpus_leave)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave.txt',tf_idf_leave[corpus_leave],\n",
    "                                              num_features=len(dictionary_leave))\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_half_day.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_half_day[query_doc_bow]\n",
    "        half_day = np.max(sims_half_day[query_doc_tf_idf])\n",
    "        print(half_day)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_leave.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave[query_doc_bow]\n",
    "        leave = np.max(sims_leave[query_doc_tf_idf])\n",
    "        print(leave)\n",
    "        \n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "        if(half_day>leave):\n",
    "            return \"half_day\"\n",
    "        else:\n",
    "            return \"leave\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave Inquiry Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "train = train[train['Type'] == 'leave_inquiry']\n",
    "#train = train[train['Module'] == 'Leave']\n",
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            #if token not in stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):  \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['sub_type_two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "\n",
    "#test_sequences = tok.texts_to_sequences(xtest)\n",
    "y = train['sub_type_two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(sequences, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=1,kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-fb199c444cad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[0;32m    148\u001b[0m                          \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                          accept_large_sparse=False)\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "model.fit(sequences,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "\n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    #model.add(LSTM(128, input_shape=(xtrain.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #model.add(LSTM(80, input_shape=(xtrain.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 960 samples, validate on 107 samples\n",
      "Epoch 1/50\n",
      "960/960 [==============================] - ETA: 34s - loss: 0.5007 - acc: 0.80 - ETA: 18s - loss: 0.5000 - acc: 0.80 - ETA: 13s - loss: 0.4993 - acc: 0.80 - ETA: 10s - loss: 0.4986 - acc: 0.80 - ETA: 8s - loss: 0.4976 - acc: 0.8000 - ETA: 7s - loss: 0.4967 - acc: 0.800 - ETA: 6s - loss: 0.4960 - acc: 0.800 - ETA: 6s - loss: 0.4947 - acc: 0.800 - ETA: 5s - loss: 0.4937 - acc: 0.800 - ETA: 4s - loss: 0.4928 - acc: 0.800 - ETA: 4s - loss: 0.4921 - acc: 0.800 - ETA: 4s - loss: 0.4912 - acc: 0.800 - ETA: 3s - loss: 0.4889 - acc: 0.800 - ETA: 3s - loss: 0.4868 - acc: 0.800 - ETA: 3s - loss: 0.4853 - acc: 0.800 - ETA: 2s - loss: 0.4822 - acc: 0.800 - ETA: 2s - loss: 0.4777 - acc: 0.800 - ETA: 2s - loss: 0.4733 - acc: 0.800 - ETA: 2s - loss: 0.4678 - acc: 0.800 - ETA: 1s - loss: 0.4667 - acc: 0.799 - ETA: 1s - loss: 0.4659 - acc: 0.798 - ETA: 1s - loss: 0.4612 - acc: 0.802 - ETA: 1s - loss: 0.4573 - acc: 0.802 - ETA: 1s - loss: 0.4515 - acc: 0.805 - ETA: 0s - loss: 0.4485 - acc: 0.807 - ETA: 0s - loss: 0.4483 - acc: 0.807 - ETA: 0s - loss: 0.4470 - acc: 0.808 - ETA: 0s - loss: 0.4467 - acc: 0.809 - ETA: 0s - loss: 0.4456 - acc: 0.810 - 6s 6ms/step - loss: 0.4437 - acc: 0.8113 - val_loss: 0.4971 - val_acc: 0.6000\n",
      "Epoch 2/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.4277 - acc: 0.793 - ETA: 4s - loss: 0.4008 - acc: 0.806 - ETA: 3s - loss: 0.3992 - acc: 0.812 - ETA: 3s - loss: 0.3807 - acc: 0.839 - ETA: 3s - loss: 0.3796 - acc: 0.855 - ETA: 3s - loss: 0.3855 - acc: 0.858 - ETA: 3s - loss: 0.3779 - acc: 0.867 - ETA: 3s - loss: 0.3766 - acc: 0.864 - ETA: 3s - loss: 0.3725 - acc: 0.866 - ETA: 2s - loss: 0.3718 - acc: 0.863 - ETA: 2s - loss: 0.3733 - acc: 0.859 - ETA: 2s - loss: 0.3716 - acc: 0.857 - ETA: 2s - loss: 0.3700 - acc: 0.856 - ETA: 2s - loss: 0.3738 - acc: 0.852 - ETA: 2s - loss: 0.3757 - acc: 0.850 - ETA: 2s - loss: 0.3728 - acc: 0.851 - ETA: 1s - loss: 0.3692 - acc: 0.852 - ETA: 1s - loss: 0.3637 - acc: 0.854 - ETA: 1s - loss: 0.3624 - acc: 0.855 - ETA: 1s - loss: 0.3627 - acc: 0.855 - ETA: 1s - loss: 0.3575 - acc: 0.859 - ETA: 1s - loss: 0.3566 - acc: 0.861 - ETA: 0s - loss: 0.3549 - acc: 0.863 - ETA: 0s - loss: 0.3523 - acc: 0.865 - ETA: 0s - loss: 0.3520 - acc: 0.866 - ETA: 0s - loss: 0.3478 - acc: 0.868 - ETA: 0s - loss: 0.3459 - acc: 0.870 - ETA: 0s - loss: 0.3386 - acc: 0.873 - ETA: 0s - loss: 0.3354 - acc: 0.875 - 4s 5ms/step - loss: 0.3302 - acc: 0.8765 - val_loss: 0.4320 - val_acc: 0.7252\n",
      "Epoch 3/50\n",
      "960/960 [==============================] - ETA: 3s - loss: 0.2401 - acc: 0.906 - ETA: 3s - loss: 0.2373 - acc: 0.909 - ETA: 3s - loss: 0.2177 - acc: 0.916 - ETA: 3s - loss: 0.2222 - acc: 0.915 - ETA: 3s - loss: 0.2113 - acc: 0.916 - ETA: 3s - loss: 0.2041 - acc: 0.917 - ETA: 3s - loss: 0.2138 - acc: 0.914 - ETA: 2s - loss: 0.2128 - acc: 0.917 - ETA: 2s - loss: 0.2068 - acc: 0.921 - ETA: 2s - loss: 0.2027 - acc: 0.924 - ETA: 2s - loss: 0.1994 - acc: 0.927 - ETA: 2s - loss: 0.1957 - acc: 0.929 - ETA: 2s - loss: 0.1951 - acc: 0.929 - ETA: 2s - loss: 0.1928 - acc: 0.930 - ETA: 2s - loss: 0.1897 - acc: 0.931 - ETA: 1s - loss: 0.1846 - acc: 0.933 - ETA: 1s - loss: 0.1814 - acc: 0.934 - ETA: 1s - loss: 0.1844 - acc: 0.934 - ETA: 1s - loss: 0.1810 - acc: 0.935 - ETA: 1s - loss: 0.1826 - acc: 0.935 - ETA: 1s - loss: 0.1818 - acc: 0.936 - ETA: 1s - loss: 0.1812 - acc: 0.936 - ETA: 0s - loss: 0.1795 - acc: 0.937 - ETA: 0s - loss: 0.1791 - acc: 0.937 - ETA: 0s - loss: 0.1769 - acc: 0.937 - ETA: 0s - loss: 0.1761 - acc: 0.937 - ETA: 0s - loss: 0.1744 - acc: 0.938 - ETA: 0s - loss: 0.1741 - acc: 0.938 - ETA: 0s - loss: 0.1754 - acc: 0.937 - 4s 4ms/step - loss: 0.1761 - acc: 0.9371 - val_loss: 0.2288 - val_acc: 0.8991\n",
      "Epoch 4/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.1253 - acc: 0.956 - ETA: 4s - loss: 0.1078 - acc: 0.965 - ETA: 3s - loss: 0.1078 - acc: 0.964 - ETA: 3s - loss: 0.1169 - acc: 0.960 - ETA: 3s - loss: 0.1168 - acc: 0.961 - ETA: 3s - loss: 0.1162 - acc: 0.960 - ETA: 3s - loss: 0.1083 - acc: 0.963 - ETA: 3s - loss: 0.1127 - acc: 0.960 - ETA: 2s - loss: 0.1172 - acc: 0.959 - ETA: 2s - loss: 0.1183 - acc: 0.958 - ETA: 2s - loss: 0.1191 - acc: 0.957 - ETA: 2s - loss: 0.1209 - acc: 0.956 - ETA: 2s - loss: 0.1165 - acc: 0.957 - ETA: 2s - loss: 0.1179 - acc: 0.956 - ETA: 2s - loss: 0.1197 - acc: 0.955 - ETA: 1s - loss: 0.1233 - acc: 0.953 - ETA: 1s - loss: 0.1204 - acc: 0.955 - ETA: 1s - loss: 0.1203 - acc: 0.955 - ETA: 1s - loss: 0.1184 - acc: 0.956 - ETA: 1s - loss: 0.1185 - acc: 0.955 - ETA: 1s - loss: 0.1166 - acc: 0.956 - ETA: 1s - loss: 0.1184 - acc: 0.955 - ETA: 0s - loss: 0.1187 - acc: 0.955 - ETA: 0s - loss: 0.1172 - acc: 0.956 - ETA: 0s - loss: 0.1156 - acc: 0.956 - ETA: 0s - loss: 0.1158 - acc: 0.956 - ETA: 0s - loss: 0.1143 - acc: 0.957 - ETA: 0s - loss: 0.1140 - acc: 0.957 - ETA: 0s - loss: 0.1141 - acc: 0.957 - 4s 4ms/step - loss: 0.1155 - acc: 0.9560 - val_loss: 0.0537 - val_acc: 0.9963\n",
      "Epoch 5/50\n",
      "960/960 [==============================] - ETA: 3s - loss: 0.1208 - acc: 0.943 - ETA: 3s - loss: 0.1025 - acc: 0.953 - ETA: 3s - loss: 0.0890 - acc: 0.958 - ETA: 3s - loss: 0.0890 - acc: 0.957 - ETA: 3s - loss: 0.0866 - acc: 0.957 - ETA: 3s - loss: 0.0877 - acc: 0.957 - ETA: 3s - loss: 0.0847 - acc: 0.958 - ETA: 3s - loss: 0.0875 - acc: 0.958 - ETA: 2s - loss: 0.0843 - acc: 0.961 - ETA: 2s - loss: 0.0828 - acc: 0.963 - ETA: 2s - loss: 0.0825 - acc: 0.964 - ETA: 2s - loss: 0.0835 - acc: 0.965 - ETA: 2s - loss: 0.0815 - acc: 0.965 - ETA: 2s - loss: 0.0852 - acc: 0.963 - ETA: 2s - loss: 0.0852 - acc: 0.963 - ETA: 1s - loss: 0.0878 - acc: 0.962 - ETA: 1s - loss: 0.0866 - acc: 0.964 - ETA: 1s - loss: 0.0883 - acc: 0.963 - ETA: 1s - loss: 0.0894 - acc: 0.963 - ETA: 1s - loss: 0.0897 - acc: 0.963 - ETA: 1s - loss: 0.0871 - acc: 0.964 - ETA: 1s - loss: 0.0848 - acc: 0.965 - ETA: 0s - loss: 0.0836 - acc: 0.966 - ETA: 0s - loss: 0.0830 - acc: 0.966 - ETA: 0s - loss: 0.0864 - acc: 0.964 - ETA: 0s - loss: 0.0852 - acc: 0.965 - ETA: 0s - loss: 0.0865 - acc: 0.965 - ETA: 0s - loss: 0.0857 - acc: 0.965 - ETA: 0s - loss: 0.0861 - acc: 0.965 - 4s 4ms/step - loss: 0.0854 - acc: 0.9658 - val_loss: 0.0357 - val_acc: 0.9963\n",
      "Epoch 6/50\n",
      "960/960 [==============================] - ETA: 3s - loss: 0.0936 - acc: 0.962 - ETA: 3s - loss: 0.0858 - acc: 0.965 - ETA: 3s - loss: 0.0987 - acc: 0.964 - ETA: 3s - loss: 0.0960 - acc: 0.964 - ETA: 3s - loss: 0.0874 - acc: 0.970 - ETA: 3s - loss: 0.0794 - acc: 0.972 - ETA: 3s - loss: 0.0747 - acc: 0.974 - ETA: 3s - loss: 0.0769 - acc: 0.972 - ETA: 3s - loss: 0.0767 - acc: 0.972 - ETA: 3s - loss: 0.0735 - acc: 0.973 - ETA: 2s - loss: 0.0720 - acc: 0.973 - ETA: 2s - loss: 0.0717 - acc: 0.972 - ETA: 2s - loss: 0.0710 - acc: 0.972 - ETA: 2s - loss: 0.0682 - acc: 0.973 - ETA: 2s - loss: 0.0689 - acc: 0.972 - ETA: 2s - loss: 0.0671 - acc: 0.973 - ETA: 2s - loss: 0.0657 - acc: 0.974 - ETA: 1s - loss: 0.0650 - acc: 0.974 - ETA: 1s - loss: 0.0651 - acc: 0.974 - ETA: 1s - loss: 0.0647 - acc: 0.974 - ETA: 1s - loss: 0.0641 - acc: 0.974 - ETA: 1s - loss: 0.0623 - acc: 0.975 - ETA: 1s - loss: 0.0633 - acc: 0.974 - ETA: 0s - loss: 0.0616 - acc: 0.975 - ETA: 0s - loss: 0.0623 - acc: 0.975 - ETA: 0s - loss: 0.0612 - acc: 0.975 - ETA: 0s - loss: 0.0595 - acc: 0.976 - ETA: 0s - loss: 0.0589 - acc: 0.976 - ETA: 0s - loss: 0.0583 - acc: 0.976 - 5s 5ms/step - loss: 0.0575 - acc: 0.9773 - val_loss: 0.0272 - val_acc: 0.9963\n",
      "Epoch 7/50\n",
      "960/960 [==============================] - ETA: 6s - loss: 0.0390 - acc: 0.981 - ETA: 5s - loss: 0.0355 - acc: 0.987 - ETA: 4s - loss: 0.0341 - acc: 0.987 - ETA: 4s - loss: 0.0337 - acc: 0.987 - ETA: 4s - loss: 0.0346 - acc: 0.988 - ETA: 4s - loss: 0.0310 - acc: 0.989 - ETA: 5s - loss: 0.0332 - acc: 0.988 - ETA: 5s - loss: 0.0331 - acc: 0.988 - ETA: 5s - loss: 0.0333 - acc: 0.988 - ETA: 4s - loss: 0.0366 - acc: 0.986 - ETA: 4s - loss: 0.0358 - acc: 0.986 - ETA: 4s - loss: 0.0359 - acc: 0.986 - ETA: 3s - loss: 0.0352 - acc: 0.987 - ETA: 3s - loss: 0.0361 - acc: 0.987 - ETA: 3s - loss: 0.0340 - acc: 0.987 - ETA: 2s - loss: 0.0326 - acc: 0.988 - ETA: 2s - loss: 0.0323 - acc: 0.989 - ETA: 2s - loss: 0.0310 - acc: 0.989 - ETA: 2s - loss: 0.0322 - acc: 0.988 - ETA: 2s - loss: 0.0315 - acc: 0.989 - ETA: 1s - loss: 0.0328 - acc: 0.988 - ETA: 1s - loss: 0.0318 - acc: 0.988 - ETA: 1s - loss: 0.0309 - acc: 0.989 - ETA: 1s - loss: 0.0313 - acc: 0.988 - ETA: 0s - loss: 0.0321 - acc: 0.989 - ETA: 0s - loss: 0.0324 - acc: 0.989 - ETA: 0s - loss: 0.0327 - acc: 0.989 - ETA: 0s - loss: 0.0326 - acc: 0.989 - ETA: 0s - loss: 0.0320 - acc: 0.989 - 6s 6ms/step - loss: 0.0316 - acc: 0.9900 - val_loss: 0.0375 - val_acc: 0.9963\n",
      "Epoch 8/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0347 - acc: 0.987 - ETA: 4s - loss: 0.0235 - acc: 0.987 - ETA: 4s - loss: 0.0236 - acc: 0.991 - ETA: 4s - loss: 0.0238 - acc: 0.993 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0263 - acc: 0.992 - ETA: 4s - loss: 0.0281 - acc: 0.992 - ETA: 4s - loss: 0.0260 - acc: 0.993 - ETA: 3s - loss: 0.0273 - acc: 0.993 - ETA: 3s - loss: 0.0258 - acc: 0.994 - ETA: 3s - loss: 0.0253 - acc: 0.994 - ETA: 3s - loss: 0.0244 - acc: 0.995 - ETA: 3s - loss: 0.0233 - acc: 0.995 - ETA: 2s - loss: 0.0220 - acc: 0.996 - ETA: 2s - loss: 0.0234 - acc: 0.995 - ETA: 2s - loss: 0.0229 - acc: 0.994 - ETA: 2s - loss: 0.0219 - acc: 0.994 - ETA: 2s - loss: 0.0212 - acc: 0.995 - ETA: 2s - loss: 0.0206 - acc: 0.995 - ETA: 1s - loss: 0.0200 - acc: 0.995 - ETA: 1s - loss: 0.0202 - acc: 0.995 - ETA: 1s - loss: 0.0198 - acc: 0.995 - ETA: 1s - loss: 0.0203 - acc: 0.995 - ETA: 1s - loss: 0.0203 - acc: 0.995 - ETA: 0s - loss: 0.0200 - acc: 0.995 - ETA: 0s - loss: 0.0198 - acc: 0.995 - ETA: 0s - loss: 0.0194 - acc: 0.996 - ETA: 0s - loss: 0.0191 - acc: 0.996 - ETA: 0s - loss: 0.0185 - acc: 0.996 - 6s 6ms/step - loss: 0.0181 - acc: 0.9965 - val_loss: 0.0400 - val_acc: 0.9963\n",
      "Epoch 9/50\n",
      "960/960 [==============================] - ETA: 5s - loss: 0.0086 - acc: 1.000 - ETA: 5s - loss: 0.0103 - acc: 1.000 - ETA: 5s - loss: 0.0129 - acc: 0.997 - ETA: 5s - loss: 0.0121 - acc: 0.998 - ETA: 4s - loss: 0.0121 - acc: 0.998 - ETA: 4s - loss: 0.0106 - acc: 0.999 - ETA: 4s - loss: 0.0108 - acc: 0.999 - ETA: 4s - loss: 0.0107 - acc: 0.999 - ETA: 3s - loss: 0.0099 - acc: 0.999 - ETA: 3s - loss: 0.0096 - acc: 0.999 - ETA: 3s - loss: 0.0099 - acc: 0.998 - ETA: 3s - loss: 0.0096 - acc: 0.999 - ETA: 2s - loss: 0.0094 - acc: 0.999 - ETA: 2s - loss: 0.0088 - acc: 0.999 - ETA: 2s - loss: 0.0095 - acc: 0.999 - ETA: 2s - loss: 0.0092 - acc: 0.999 - ETA: 2s - loss: 0.0130 - acc: 0.998 - ETA: 2s - loss: 0.0127 - acc: 0.998 - ETA: 1s - loss: 0.0124 - acc: 0.998 - ETA: 1s - loss: 0.0124 - acc: 0.998 - ETA: 1s - loss: 0.0120 - acc: 0.998 - ETA: 1s - loss: 0.0131 - acc: 0.997 - ETA: 1s - loss: 0.0134 - acc: 0.997 - ETA: 0s - loss: 0.0129 - acc: 0.997 - ETA: 0s - loss: 0.0125 - acc: 0.998 - ETA: 0s - loss: 0.0121 - acc: 0.998 - ETA: 0s - loss: 0.0118 - acc: 0.998 - ETA: 0s - loss: 0.0119 - acc: 0.998 - ETA: 0s - loss: 0.0122 - acc: 0.997 - 5s 5ms/step - loss: 0.0120 - acc: 0.9979 - val_loss: 0.0326 - val_acc: 0.9963\n",
      "Epoch 10/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0110 - acc: 1.000 - ETA: 4s - loss: 0.0087 - acc: 1.000 - ETA: 4s - loss: 0.0065 - acc: 1.000 - ETA: 3s - loss: 0.0081 - acc: 0.996 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 3s - loss: 0.0070 - acc: 0.997 - ETA: 3s - loss: 0.0065 - acc: 0.998 - ETA: 3s - loss: 0.0066 - acc: 0.998 - ETA: 3s - loss: 0.0061 - acc: 0.998 - ETA: 3s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0054 - acc: 0.999 - ETA: 2s - loss: 0.0055 - acc: 0.999 - ETA: 2s - loss: 0.0058 - acc: 0.999 - ETA: 2s - loss: 0.0059 - acc: 0.999 - ETA: 2s - loss: 0.0056 - acc: 0.999 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - 5s 5ms/step - loss: 0.0093 - acc: 0.9981 - val_loss: 0.0369 - val_acc: 0.9963\n",
      "Epoch 11/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0049 - acc: 1.000 - ETA: 4s - loss: 0.0035 - acc: 1.000 - ETA: 4s - loss: 0.0210 - acc: 0.995 - ETA: 4s - loss: 0.0158 - acc: 0.996 - ETA: 3s - loss: 0.0144 - acc: 0.997 - ETA: 3s - loss: 0.0127 - acc: 0.997 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0124 - acc: 0.997 - ETA: 3s - loss: 0.0114 - acc: 0.997 - ETA: 3s - loss: 0.0105 - acc: 0.997 - ETA: 2s - loss: 0.0103 - acc: 0.997 - ETA: 2s - loss: 0.0097 - acc: 0.998 - ETA: 2s - loss: 0.0092 - acc: 0.998 - ETA: 2s - loss: 0.0091 - acc: 0.998 - ETA: 2s - loss: 0.0089 - acc: 0.998 - ETA: 2s - loss: 0.0092 - acc: 0.997 - ETA: 1s - loss: 0.0088 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.998 - ETA: 1s - loss: 0.0082 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - 5s 5ms/step - loss: 0.0068 - acc: 0.9983 - val_loss: 0.0357 - val_acc: 0.9963\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/960 [==============================] - ETA: 4s - loss: 6.0655e-04 - acc: 1.000 - ETA: 4s - loss: 6.9226e-04 - acc: 1.000 - ETA: 4s - loss: 5.6065e-04 - acc: 1.000 - ETA: 3s - loss: 0.0017 - acc: 1.0000    - ETA: 3s - loss: 0.0017 - acc: 1.000 - ETA: 3s - loss: 0.0015 - acc: 1.000 - ETA: 3s - loss: 0.0025 - acc: 1.000 - ETA: 3s - loss: 0.0029 - acc: 1.000 - ETA: 3s - loss: 0.0032 - acc: 1.000 - ETA: 3s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 2s - loss: 0.0032 - acc: 1.000 - ETA: 2s - loss: 0.0030 - acc: 1.000 - ETA: 2s - loss: 0.0029 - acc: 1.000 - ETA: 2s - loss: 0.0031 - acc: 1.000 - ETA: 1s - loss: 0.0030 - acc: 1.000 - ETA: 1s - loss: 0.0029 - acc: 1.000 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0031 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0053 - acc: 0.999 - 5s 5ms/step - loss: 0.0051 - acc: 0.9992 - val_loss: 0.0289 - val_acc: 0.9963\n",
      "Epoch 13/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0044 - acc: 1.000 - ETA: 4s - loss: 0.0035 - acc: 1.000 - ETA: 4s - loss: 0.0024 - acc: 1.000 - ETA: 3s - loss: 0.0021 - acc: 1.000 - ETA: 3s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0078 - acc: 0.997 - ETA: 3s - loss: 0.0074 - acc: 0.998 - ETA: 3s - loss: 0.0068 - acc: 0.998 - ETA: 3s - loss: 0.0079 - acc: 0.998 - ETA: 3s - loss: 0.0072 - acc: 0.998 - ETA: 3s - loss: 0.0068 - acc: 0.998 - ETA: 3s - loss: 0.0063 - acc: 0.999 - ETA: 3s - loss: 0.0059 - acc: 0.999 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0070 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.999 - ETA: 0s - loss: 0.0053 - acc: 0.999 - ETA: 0s - loss: 0.0051 - acc: 0.999 - ETA: 0s - loss: 0.0051 - acc: 0.999 - ETA: 0s - loss: 0.0049 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - 5s 5ms/step - loss: 0.0046 - acc: 0.9992 - val_loss: 0.0326 - val_acc: 0.9944\n",
      "Epoch 14/50\n",
      "960/960 [==============================] - ETA: 5s - loss: 5.5501e-04 - acc: 1.000 - ETA: 4s - loss: 0.0015 - acc: 1.0000    - ETA: 4s - loss: 0.0014 - acc: 1.000 - ETA: 4s - loss: 0.0013 - acc: 1.000 - ETA: 4s - loss: 0.0016 - acc: 1.000 - ETA: 4s - loss: 0.0015 - acc: 1.000 - ETA: 4s - loss: 0.0017 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 3s - loss: 0.0015 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 3s - loss: 0.0015 - acc: 1.000 - ETA: 3s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0037 - acc: 0.999 - ETA: 0s - loss: 0.0035 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - 5s 5ms/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.0489 - val_acc: 0.9888\n",
      "Epoch 15/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 2.8073e-04 - acc: 1.000 - ETA: 4s - loss: 2.8665e-04 - acc: 1.000 - ETA: 4s - loss: 5.7672e-04 - acc: 1.000 - ETA: 4s - loss: 0.0013 - acc: 1.0000    - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0011 - acc: 1.000 - ETA: 3s - loss: 9.9285e-04 - acc: 1.000 - ETA: 3s - loss: 9.6626e-04 - acc: 1.000 - ETA: 3s - loss: 0.0014 - acc: 1.0000    - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 2s - loss: 0.0012 - acc: 1.000 - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 2s - loss: 0.0012 - acc: 1.000 - ETA: 2s - loss: 0.0012 - acc: 1.000 - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 1.000 - ETA: 1s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - 5s 5ms/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0407 - val_acc: 0.9963\n",
      "Epoch 16/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 5.5955e-05 - acc: 1.000 - ETA: 4s - loss: 5.7441e-04 - acc: 1.000 - ETA: 4s - loss: 5.1574e-04 - acc: 1.000 - ETA: 4s - loss: 0.0012 - acc: 1.0000    - ETA: 3s - loss: 9.4338e-04 - acc: 1.000 - ETA: 3s - loss: 8.4302e-04 - acc: 1.000 - ETA: 3s - loss: 7.7256e-04 - acc: 1.000 - ETA: 3s - loss: 7.5704e-04 - acc: 1.000 - ETA: 3s - loss: 6.8315e-04 - acc: 1.000 - ETA: 3s - loss: 6.6528e-04 - acc: 1.000 - ETA: 2s - loss: 6.3029e-04 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 0.9990    - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0030 - acc: 0.999 - ETA: 2s - loss: 0.0028 - acc: 0.999 - ETA: 2s - loss: 0.0027 - acc: 0.999 - ETA: 2s - loss: 0.0025 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - ETA: 0s - loss: 0.0023 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - 5s 5ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.0331 - val_acc: 0.9963\n",
      "Epoch 17/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 5.8064e-04 - acc: 1.000 - ETA: 4s - loss: 5.9008e-04 - acc: 1.000 - ETA: 4s - loss: 5.4871e-04 - acc: 1.000 - ETA: 4s - loss: 6.0969e-04 - acc: 1.000 - ETA: 3s - loss: 5.1953e-04 - acc: 1.000 - ETA: 3s - loss: 4.6360e-04 - acc: 1.000 - ETA: 3s - loss: 4.6807e-04 - acc: 1.000 - ETA: 3s - loss: 4.2599e-04 - acc: 1.000 - ETA: 3s - loss: 4.1878e-04 - acc: 1.000 - ETA: 3s - loss: 5.0228e-04 - acc: 1.000 - ETA: 2s - loss: 4.9680e-04 - acc: 1.000 - ETA: 2s - loss: 4.9634e-04 - acc: 1.000 - ETA: 2s - loss: 4.7893e-04 - acc: 1.000 - ETA: 2s - loss: 4.7922e-04 - acc: 1.000 - ETA: 2s - loss: 5.0371e-04 - acc: 1.000 - ETA: 2s - loss: 0.0023 - acc: 0.9996    - ETA: 2s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0021 - acc: 0.999 - ETA: 1s - loss: 0.0026 - acc: 0.999 - ETA: 1s - loss: 0.0025 - acc: 0.999 - ETA: 1s - loss: 0.0024 - acc: 0.999 - ETA: 1s - loss: 0.0023 - acc: 0.999 - ETA: 1s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0022 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - ETA: 0s - loss: 0.0021 - acc: 0.999 - 5s 5ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0423 - val_acc: 0.9963\n",
      "Epoch 18/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 6.4118e-04 - acc: 1.000 - ETA: 4s - loss: 7.8312e-04 - acc: 1.000 - ETA: 4s - loss: 6.3448e-04 - acc: 1.000 - ETA: 4s - loss: 5.2856e-04 - acc: 1.000 - ETA: 3s - loss: 4.3846e-04 - acc: 1.000 - ETA: 3s - loss: 4.8248e-04 - acc: 1.000 - ETA: 3s - loss: 4.2395e-04 - acc: 1.000 - ETA: 3s - loss: 4.4400e-04 - acc: 1.000 - ETA: 3s - loss: 4.0985e-04 - acc: 1.000 - ETA: 3s - loss: 4.3488e-04 - acc: 1.000 - ETA: 2s - loss: 4.2428e-04 - acc: 1.000 - ETA: 2s - loss: 5.1728e-04 - acc: 1.000 - ETA: 2s - loss: 4.8750e-04 - acc: 1.000 - ETA: 2s - loss: 7.6378e-04 - acc: 1.000 - ETA: 2s - loss: 7.3206e-04 - acc: 1.000 - ETA: 2s - loss: 6.9772e-04 - acc: 1.000 - ETA: 2s - loss: 6.6970e-04 - acc: 1.000 - ETA: 1s - loss: 7.1436e-04 - acc: 1.000 - ETA: 1s - loss: 6.8076e-04 - acc: 1.000 - ETA: 1s - loss: 6.5901e-04 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 0.9994    - ETA: 1s - loss: 0.0019 - acc: 0.999 - ETA: 1s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0015 - acc: 0.999 - ETA: 0s - loss: 0.0015 - acc: 0.999 - 5s 5ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0356 - val_acc: 0.9944\n",
      "Epoch 19/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 1.8040e-04 - acc: 1.000 - ETA: 4s - loss: 2.5995e-04 - acc: 1.000 - ETA: 4s - loss: 2.4420e-04 - acc: 1.000 - ETA: 3s - loss: 2.0802e-04 - acc: 1.000 - ETA: 3s - loss: 2.4629e-04 - acc: 1.000 - ETA: 3s - loss: 2.2243e-04 - acc: 1.000 - ETA: 3s - loss: 2.5858e-04 - acc: 1.000 - ETA: 3s - loss: 2.8957e-04 - acc: 1.000 - ETA: 3s - loss: 2.7456e-04 - acc: 1.000 - ETA: 3s - loss: 3.2555e-04 - acc: 1.000 - ETA: 3s - loss: 3.0899e-04 - acc: 1.000 - ETA: 2s - loss: 3.0438e-04 - acc: 1.000 - ETA: 2s - loss: 3.2529e-04 - acc: 1.000 - ETA: 2s - loss: 3.4461e-04 - acc: 1.000 - ETA: 2s - loss: 9.3892e-04 - acc: 0.999 - ETA: 2s - loss: 0.0012 - acc: 0.9996    - ETA: 2s - loss: 0.0012 - acc: 0.999 - ETA: 2s - loss: 0.0012 - acc: 0.999 - ETA: 1s - loss: 0.0012 - acc: 0.999 - ETA: 1s - loss: 0.0012 - acc: 0.999 - ETA: 1s - loss: 0.0011 - acc: 0.999 - ETA: 1s - loss: 0.0012 - acc: 0.999 - ETA: 1s - loss: 0.0012 - acc: 0.999 - ETA: 1s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0010 - acc: 0.999 - ETA: 0s - loss: 0.0010 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - 5s 5ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0505 - val_acc: 0.9963\n",
      "Epoch 20/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 2.1398e-04 - acc: 1.000 - ETA: 4s - loss: 1.2225e-04 - acc: 1.000 - ETA: 4s - loss: 1.4094e-04 - acc: 1.000 - ETA: 4s - loss: 1.6034e-04 - acc: 1.000 - ETA: 4s - loss: 1.8434e-04 - acc: 1.000 - ETA: 4s - loss: 1.6539e-04 - acc: 1.000 - ETA: 4s - loss: 1.9871e-04 - acc: 1.000 - ETA: 3s - loss: 1.9227e-04 - acc: 1.000 - ETA: 3s - loss: 2.3560e-04 - acc: 1.000 - ETA: 3s - loss: 2.9640e-04 - acc: 1.000 - ETA: 3s - loss: 3.5324e-04 - acc: 1.000 - ETA: 3s - loss: 3.2631e-04 - acc: 1.000 - ETA: 2s - loss: 3.1565e-04 - acc: 1.000 - ETA: 2s - loss: 2.9573e-04 - acc: 1.000 - ETA: 2s - loss: 3.6504e-04 - acc: 1.000 - ETA: 2s - loss: 3.4719e-04 - acc: 1.000 - ETA: 2s - loss: 3.5199e-04 - acc: 1.000 - ETA: 2s - loss: 3.4232e-04 - acc: 1.000 - ETA: 1s - loss: 3.4539e-04 - acc: 1.000 - ETA: 1s - loss: 3.3499e-04 - acc: 1.000 - ETA: 1s - loss: 3.2094e-04 - acc: 1.000 - ETA: 1s - loss: 3.1109e-04 - acc: 1.000 - ETA: 1s - loss: 3.0257e-04 - acc: 1.000 - ETA: 1s - loss: 2.9303e-04 - acc: 1.000 - ETA: 0s - loss: 2.8910e-04 - acc: 1.000 - ETA: 0s - loss: 2.7936e-04 - acc: 1.000 - ETA: 0s - loss: 2.9273e-04 - acc: 1.000 - ETA: 0s - loss: 2.9680e-04 - acc: 1.000 - ETA: 0s - loss: 2.8832e-04 - acc: 1.000 - 5s 5ms/step - loss: 2.8140e-04 - acc: 1.0000 - val_loss: 0.0389 - val_acc: 0.9963\n",
      "Epoch 21/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0026 - acc: 1.000 - ETA: 4s - loss: 0.0014 - acc: 1.000 - ETA: 4s - loss: 9.9614e-04 - acc: 1.000 - ETA: 4s - loss: 0.0034 - acc: 0.9969    - ETA: 4s - loss: 0.0028 - acc: 0.997 - ETA: 3s - loss: 0.0023 - acc: 0.997 - ETA: 3s - loss: 0.0020 - acc: 0.998 - ETA: 3s - loss: 0.0018 - acc: 0.998 - ETA: 3s - loss: 0.0049 - acc: 0.997 - ETA: 3s - loss: 0.0126 - acc: 0.995 - ETA: 3s - loss: 0.0949 - acc: 0.983 - ETA: 2s - loss: 0.0949 - acc: 0.982 - ETA: 2s - loss: 0.0876 - acc: 0.983 - ETA: 2s - loss: 0.0827 - acc: 0.983 - ETA: 2s - loss: 0.0784 - acc: 0.984 - ETA: 2s - loss: 0.0778 - acc: 0.983 - ETA: 2s - loss: 0.0768 - acc: 0.982 - ETA: 1s - loss: 0.0735 - acc: 0.983 - ETA: 1s - loss: 0.0744 - acc: 0.982 - ETA: 1s - loss: 0.0764 - acc: 0.980 - ETA: 1s - loss: 0.0738 - acc: 0.981 - ETA: 1s - loss: 0.0716 - acc: 0.981 - ETA: 1s - loss: 0.0701 - acc: 0.981 - ETA: 0s - loss: 0.0682 - acc: 0.982 - ETA: 0s - loss: 0.0662 - acc: 0.983 - ETA: 0s - loss: 0.0638 - acc: 0.983 - ETA: 0s - loss: 0.0618 - acc: 0.984 - ETA: 0s - loss: 0.0600 - acc: 0.985 - ETA: 0s - loss: 0.0597 - acc: 0.985 - 5s 5ms/step - loss: 0.0581 - acc: 0.9856 - val_loss: 0.0121 - val_acc: 0.9963\n",
      "Epoch 22/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0203 - acc: 1.000 - ETA: 4s - loss: 0.0159 - acc: 1.000 - ETA: 4s - loss: 0.0119 - acc: 1.000 - ETA: 4s - loss: 0.0104 - acc: 1.000 - ETA: 4s - loss: 0.0108 - acc: 0.998 - ETA: 3s - loss: 0.0117 - acc: 0.999 - ETA: 3s - loss: 0.0105 - acc: 0.999 - ETA: 3s - loss: 0.0099 - acc: 0.999 - ETA: 3s - loss: 0.0103 - acc: 0.999 - ETA: 3s - loss: 0.0103 - acc: 0.999 - ETA: 3s - loss: 0.0099 - acc: 0.999 - ETA: 2s - loss: 0.0096 - acc: 0.999 - ETA: 2s - loss: 0.0098 - acc: 0.999 - ETA: 2s - loss: 0.0097 - acc: 0.999 - ETA: 2s - loss: 0.0095 - acc: 0.999 - ETA: 2s - loss: 0.0093 - acc: 0.999 - ETA: 2s - loss: 0.0100 - acc: 0.998 - ETA: 1s - loss: 0.0095 - acc: 0.999 - ETA: 1s - loss: 0.0091 - acc: 0.999 - ETA: 1s - loss: 0.0092 - acc: 0.999 - ETA: 1s - loss: 0.0089 - acc: 0.999 - ETA: 1s - loss: 0.0085 - acc: 0.999 - ETA: 1s - loss: 0.0083 - acc: 0.999 - ETA: 0s - loss: 0.0083 - acc: 0.999 - ETA: 0s - loss: 0.0081 - acc: 0.999 - ETA: 0s - loss: 0.0080 - acc: 0.999 - ETA: 0s - loss: 0.0077 - acc: 0.999 - ETA: 0s - loss: 0.0077 - acc: 0.999 - ETA: 0s - loss: 0.0075 - acc: 0.999 - 5s 5ms/step - loss: 0.0087 - acc: 0.9990 - val_loss: 0.0236 - val_acc: 0.9963\n",
      "Epoch 23/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0029 - acc: 1.000 - ETA: 4s - loss: 0.0223 - acc: 0.993 - ETA: 4s - loss: 0.0155 - acc: 0.995 - ETA: 4s - loss: 0.0130 - acc: 0.996 - ETA: 4s - loss: 0.0106 - acc: 0.997 - ETA: 3s - loss: 0.0090 - acc: 0.997 - ETA: 3s - loss: 0.0079 - acc: 0.998 - ETA: 3s - loss: 0.0072 - acc: 0.998 - ETA: 3s - loss: 0.0066 - acc: 0.998 - ETA: 3s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0051 - acc: 0.999 - ETA: 2s - loss: 0.0049 - acc: 0.999 - ETA: 2s - loss: 0.0046 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0037 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 0s - loss: 0.0035 - acc: 0.999 - ETA: 0s - loss: 0.0034 - acc: 0.999 - ETA: 0s - loss: 0.0033 - acc: 0.999 - ETA: 0s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - 5s 5ms/step - loss: 0.0030 - acc: 0.9996 - val_loss: 0.0332 - val_acc: 0.9925\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/960 [==============================] - ETA: 4s - loss: 0.0017 - acc: 1.000 - ETA: 4s - loss: 0.0012 - acc: 1.000 - ETA: 4s - loss: 0.0014 - acc: 1.000 - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 3s - loss: 9.0811e-04 - acc: 1.000 - ETA: 3s - loss: 9.2074e-04 - acc: 1.000 - ETA: 3s - loss: 8.8256e-04 - acc: 1.000 - ETA: 3s - loss: 8.2167e-04 - acc: 1.000 - ETA: 3s - loss: 8.3920e-04 - acc: 1.000 - ETA: 2s - loss: 8.7946e-04 - acc: 1.000 - ETA: 2s - loss: 0.0018 - acc: 1.0000    - ETA: 2s - loss: 0.0017 - acc: 1.000 - ETA: 2s - loss: 0.0017 - acc: 1.000 - ETA: 2s - loss: 0.0016 - acc: 1.000 - ETA: 2s - loss: 0.0015 - acc: 1.000 - ETA: 2s - loss: 0.0015 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0030 - acc: 0.999 - ETA: 1s - loss: 0.0029 - acc: 0.999 - ETA: 0s - loss: 0.0028 - acc: 0.999 - ETA: 0s - loss: 0.0027 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0026 - acc: 0.999 - ETA: 0s - loss: 0.0025 - acc: 0.999 - ETA: 0s - loss: 0.0024 - acc: 0.999 - 5s 5ms/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.0272 - val_acc: 0.9963\n",
      "Epoch 25/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0012 - acc: 1.000 - ETA: 4s - loss: 7.9951e-04 - acc: 1.000 - ETA: 4s - loss: 0.0013 - acc: 1.0000    - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 3s - loss: 8.7243e-04 - acc: 1.000 - ETA: 3s - loss: 0.0010 - acc: 1.0000    - ETA: 3s - loss: 0.0014 - acc: 1.000 - ETA: 3s - loss: 0.0013 - acc: 1.000 - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 2s - loss: 0.0011 - acc: 1.000 - ETA: 2s - loss: 9.9929e-04 - acc: 1.000 - ETA: 2s - loss: 9.5478e-04 - acc: 1.000 - ETA: 2s - loss: 9.0949e-04 - acc: 1.000 - ETA: 2s - loss: 8.7095e-04 - acc: 1.000 - ETA: 2s - loss: 8.3237e-04 - acc: 1.000 - ETA: 2s - loss: 8.0470e-04 - acc: 1.000 - ETA: 1s - loss: 8.1539e-04 - acc: 1.000 - ETA: 1s - loss: 7.8316e-04 - acc: 1.000 - ETA: 1s - loss: 7.6315e-04 - acc: 1.000 - ETA: 1s - loss: 7.3828e-04 - acc: 1.000 - ETA: 1s - loss: 7.1651e-04 - acc: 1.000 - ETA: 1s - loss: 6.9584e-04 - acc: 1.000 - ETA: 0s - loss: 7.1583e-04 - acc: 1.000 - ETA: 0s - loss: 7.4366e-04 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 0.9995    - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - 5s 5ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0320 - val_acc: 0.9925\n",
      "Epoch 26/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 8.2544e-04 - acc: 1.000 - ETA: 4s - loss: 0.0016 - acc: 1.0000    - ETA: 4s - loss: 0.0013 - acc: 1.000 - ETA: 3s - loss: 0.0011 - acc: 1.000 - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 3s - loss: 9.9380e-04 - acc: 1.000 - ETA: 3s - loss: 0.0013 - acc: 1.0000    - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0011 - acc: 1.000 - ETA: 3s - loss: 0.0011 - acc: 1.000 - ETA: 2s - loss: 0.0010 - acc: 1.000 - ETA: 2s - loss: 0.0010 - acc: 1.000 - ETA: 2s - loss: 0.0011 - acc: 1.000 - ETA: 2s - loss: 0.0010 - acc: 1.000 - ETA: 2s - loss: 9.8401e-04 - acc: 1.000 - ETA: 2s - loss: 9.3053e-04 - acc: 1.000 - ETA: 1s - loss: 0.0011 - acc: 1.0000    - ETA: 1s - loss: 0.0011 - acc: 1.000 - ETA: 1s - loss: 0.0010 - acc: 1.000 - ETA: 1s - loss: 9.7718e-04 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 0.9997    - ETA: 1s - loss: 0.0018 - acc: 0.999 - ETA: 1s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0018 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0017 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0016 - acc: 0.999 - ETA: 0s - loss: 0.0015 - acc: 0.999 - 5s 5ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.0299 - val_acc: 0.9963\n",
      "Epoch 27/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 0.0045 - acc: 1.000 - ETA: 4s - loss: 0.0025 - acc: 1.000 - ETA: 4s - loss: 0.0019 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 3s - loss: 0.0014 - acc: 1.000 - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 3s - loss: 8.9793e-04 - acc: 1.000 - ETA: 3s - loss: 8.2769e-04 - acc: 1.000 - ETA: 3s - loss: 8.5725e-04 - acc: 1.000 - ETA: 2s - loss: 8.1119e-04 - acc: 1.000 - ETA: 2s - loss: 7.4842e-04 - acc: 1.000 - ETA: 2s - loss: 7.6913e-04 - acc: 1.000 - ETA: 2s - loss: 7.2847e-04 - acc: 1.000 - ETA: 2s - loss: 7.0132e-04 - acc: 1.000 - ETA: 2s - loss: 6.6596e-04 - acc: 1.000 - ETA: 2s - loss: 6.5046e-04 - acc: 1.000 - ETA: 1s - loss: 6.3916e-04 - acc: 1.000 - ETA: 1s - loss: 6.2781e-04 - acc: 1.000 - ETA: 1s - loss: 6.0291e-04 - acc: 1.000 - ETA: 1s - loss: 6.2278e-04 - acc: 1.000 - ETA: 1s - loss: 0.0012 - acc: 0.9997    - ETA: 1s - loss: 0.0012 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0011 - acc: 0.999 - ETA: 0s - loss: 0.0010 - acc: 0.999 - ETA: 0s - loss: 0.0010 - acc: 0.999 - ETA: 0s - loss: 9.9908e-04 - acc: 0.999 - 5s 5ms/step - loss: 9.8205e-04 - acc: 0.9998 - val_loss: 0.0358 - val_acc: 0.9925\n",
      "Epoch 28/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 4.5337e-04 - acc: 1.000 - ETA: 4s - loss: 2.9626e-04 - acc: 1.000 - ETA: 4s - loss: 3.2182e-04 - acc: 1.000 - ETA: 3s - loss: 3.8087e-04 - acc: 1.000 - ETA: 3s - loss: 3.3661e-04 - acc: 1.000 - ETA: 3s - loss: 3.5776e-04 - acc: 1.000 - ETA: 3s - loss: 3.3253e-04 - acc: 1.000 - ETA: 3s - loss: 3.1355e-04 - acc: 1.000 - ETA: 3s - loss: 3.0751e-04 - acc: 1.000 - ETA: 3s - loss: 3.1904e-04 - acc: 1.000 - ETA: 3s - loss: 3.0991e-04 - acc: 1.000 - ETA: 2s - loss: 2.9084e-04 - acc: 1.000 - ETA: 2s - loss: 2.7819e-04 - acc: 1.000 - ETA: 2s - loss: 2.8487e-04 - acc: 1.000 - ETA: 2s - loss: 2.7651e-04 - acc: 1.000 - ETA: 2s - loss: 5.2076e-04 - acc: 1.000 - ETA: 2s - loss: 5.0021e-04 - acc: 1.000 - ETA: 1s - loss: 4.7647e-04 - acc: 1.000 - ETA: 1s - loss: 4.6057e-04 - acc: 1.000 - ETA: 1s - loss: 4.3992e-04 - acc: 1.000 - ETA: 1s - loss: 5.6616e-04 - acc: 1.000 - ETA: 1s - loss: 7.4765e-04 - acc: 1.000 - ETA: 1s - loss: 7.2552e-04 - acc: 1.000 - ETA: 0s - loss: 7.0464e-04 - acc: 1.000 - ETA: 0s - loss: 6.8275e-04 - acc: 1.000 - ETA: 0s - loss: 6.6246e-04 - acc: 1.000 - ETA: 0s - loss: 6.4947e-04 - acc: 1.000 - ETA: 0s - loss: 6.4882e-04 - acc: 1.000 - ETA: 0s - loss: 7.0955e-04 - acc: 1.000 - 5s 5ms/step - loss: 6.8814e-04 - acc: 1.0000 - val_loss: 0.0473 - val_acc: 0.9925\n",
      "Epoch 29/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 1.2167e-04 - acc: 1.000 - ETA: 4s - loss: 1.1598e-04 - acc: 1.000 - ETA: 4s - loss: 3.1566e-04 - acc: 1.000 - ETA: 4s - loss: 2.7101e-04 - acc: 1.000 - ETA: 4s - loss: 2.9187e-04 - acc: 1.000 - ETA: 3s - loss: 2.7764e-04 - acc: 1.000 - ETA: 3s - loss: 2.7298e-04 - acc: 1.000 - ETA: 3s - loss: 2.4433e-04 - acc: 1.000 - ETA: 3s - loss: 2.2743e-04 - acc: 1.000 - ETA: 3s - loss: 2.2672e-04 - acc: 1.000 - ETA: 3s - loss: 2.5442e-04 - acc: 1.000 - ETA: 3s - loss: 2.3630e-04 - acc: 1.000 - ETA: 2s - loss: 3.0107e-04 - acc: 1.000 - ETA: 2s - loss: 3.2669e-04 - acc: 1.000 - ETA: 2s - loss: 5.5268e-04 - acc: 1.000 - ETA: 2s - loss: 5.4439e-04 - acc: 1.000 - ETA: 2s - loss: 5.1593e-04 - acc: 1.000 - ETA: 1s - loss: 4.9934e-04 - acc: 1.000 - ETA: 1s - loss: 4.7962e-04 - acc: 1.000 - ETA: 1s - loss: 5.0207e-04 - acc: 1.000 - ETA: 1s - loss: 4.8587e-04 - acc: 1.000 - ETA: 1s - loss: 4.9960e-04 - acc: 1.000 - ETA: 1s - loss: 4.8282e-04 - acc: 1.000 - ETA: 0s - loss: 4.7743e-04 - acc: 1.000 - ETA: 0s - loss: 4.6724e-04 - acc: 1.000 - ETA: 0s - loss: 4.5165e-04 - acc: 1.000 - ETA: 0s - loss: 4.3801e-04 - acc: 1.000 - ETA: 0s - loss: 4.3156e-04 - acc: 1.000 - ETA: 0s - loss: 4.2298e-04 - acc: 1.000 - 5s 5ms/step - loss: 4.1107e-04 - acc: 1.0000 - val_loss: 0.0398 - val_acc: 0.9925\n",
      "Epoch 30/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 6.8247e-04 - acc: 1.000 - ETA: 4s - loss: 4.0028e-04 - acc: 1.000 - ETA: 4s - loss: 3.0803e-04 - acc: 1.000 - ETA: 4s - loss: 2.5839e-04 - acc: 1.000 - ETA: 4s - loss: 2.1635e-04 - acc: 1.000 - ETA: 3s - loss: 1.9424e-04 - acc: 1.000 - ETA: 3s - loss: 1.7045e-04 - acc: 1.000 - ETA: 3s - loss: 1.6458e-04 - acc: 1.000 - ETA: 3s - loss: 1.5029e-04 - acc: 1.000 - ETA: 3s - loss: 1.4182e-04 - acc: 1.000 - ETA: 3s - loss: 1.5480e-04 - acc: 1.000 - ETA: 2s - loss: 1.4749e-04 - acc: 1.000 - ETA: 2s - loss: 1.8156e-04 - acc: 1.000 - ETA: 2s - loss: 1.7308e-04 - acc: 1.000 - ETA: 2s - loss: 1.8498e-04 - acc: 1.000 - ETA: 2s - loss: 1.7494e-04 - acc: 1.000 - ETA: 2s - loss: 2.0791e-04 - acc: 1.000 - ETA: 1s - loss: 2.0270e-04 - acc: 1.000 - ETA: 1s - loss: 2.1928e-04 - acc: 1.000 - ETA: 1s - loss: 2.3177e-04 - acc: 1.000 - ETA: 1s - loss: 2.5239e-04 - acc: 1.000 - ETA: 1s - loss: 3.4816e-04 - acc: 1.000 - ETA: 1s - loss: 3.3864e-04 - acc: 1.000 - ETA: 0s - loss: 3.4009e-04 - acc: 1.000 - ETA: 0s - loss: 3.3317e-04 - acc: 1.000 - ETA: 0s - loss: 3.2552e-04 - acc: 1.000 - ETA: 0s - loss: 3.1399e-04 - acc: 1.000 - ETA: 0s - loss: 3.1036e-04 - acc: 1.000 - ETA: 0s - loss: 3.0129e-04 - acc: 1.000 - 5s 5ms/step - loss: 2.9941e-04 - acc: 1.0000 - val_loss: 0.0405 - val_acc: 0.9925\n",
      "Epoch 31/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 5.8950e-05 - acc: 1.000 - ETA: 4s - loss: 7.1503e-05 - acc: 1.000 - ETA: 4s - loss: 7.9534e-05 - acc: 1.000 - ETA: 4s - loss: 1.2628e-04 - acc: 1.000 - ETA: 3s - loss: 1.6245e-04 - acc: 1.000 - ETA: 3s - loss: 1.4382e-04 - acc: 1.000 - ETA: 3s - loss: 1.6608e-04 - acc: 1.000 - ETA: 3s - loss: 1.5311e-04 - acc: 1.000 - ETA: 3s - loss: 2.1345e-04 - acc: 1.000 - ETA: 3s - loss: 3.7679e-04 - acc: 1.000 - ETA: 3s - loss: 3.4488e-04 - acc: 1.000 - ETA: 2s - loss: 3.1790e-04 - acc: 1.000 - ETA: 2s - loss: 2.9805e-04 - acc: 1.000 - ETA: 2s - loss: 2.9853e-04 - acc: 1.000 - ETA: 2s - loss: 3.0816e-04 - acc: 1.000 - ETA: 2s - loss: 3.1166e-04 - acc: 1.000 - ETA: 2s - loss: 3.1324e-04 - acc: 1.000 - ETA: 1s - loss: 2.9881e-04 - acc: 1.000 - ETA: 1s - loss: 2.8821e-04 - acc: 1.000 - ETA: 1s - loss: 3.1074e-04 - acc: 1.000 - ETA: 1s - loss: 3.0036e-04 - acc: 1.000 - ETA: 1s - loss: 2.9258e-04 - acc: 1.000 - ETA: 1s - loss: 2.8664e-04 - acc: 1.000 - ETA: 0s - loss: 2.8320e-04 - acc: 1.000 - ETA: 0s - loss: 3.2599e-04 - acc: 1.000 - ETA: 0s - loss: 3.1466e-04 - acc: 1.000 - ETA: 0s - loss: 3.1378e-04 - acc: 1.000 - ETA: 0s - loss: 3.0403e-04 - acc: 1.000 - ETA: 0s - loss: 2.9569e-04 - acc: 1.000 - 5s 5ms/step - loss: 2.8783e-04 - acc: 1.0000 - val_loss: 0.0445 - val_acc: 0.9925\n",
      "Epoch 32/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 4.4824e-04 - acc: 1.000 - ETA: 4s - loss: 3.9226e-04 - acc: 1.000 - ETA: 4s - loss: 2.9810e-04 - acc: 1.000 - ETA: 4s - loss: 2.3366e-04 - acc: 1.000 - ETA: 3s - loss: 3.9550e-04 - acc: 1.000 - ETA: 3s - loss: 3.3297e-04 - acc: 1.000 - ETA: 3s - loss: 2.9291e-04 - acc: 1.000 - ETA: 3s - loss: 2.6609e-04 - acc: 1.000 - ETA: 3s - loss: 2.4549e-04 - acc: 1.000 - ETA: 3s - loss: 2.3181e-04 - acc: 1.000 - ETA: 3s - loss: 2.4516e-04 - acc: 1.000 - ETA: 2s - loss: 2.2975e-04 - acc: 1.000 - ETA: 2s - loss: 2.1265e-04 - acc: 1.000 - ETA: 2s - loss: 1.9818e-04 - acc: 1.000 - ETA: 2s - loss: 1.9496e-04 - acc: 1.000 - ETA: 2s - loss: 1.8578e-04 - acc: 1.000 - ETA: 2s - loss: 1.8233e-04 - acc: 1.000 - ETA: 1s - loss: 1.7858e-04 - acc: 1.000 - ETA: 1s - loss: 1.7538e-04 - acc: 1.000 - ETA: 1s - loss: 1.7171e-04 - acc: 1.000 - ETA: 1s - loss: 1.6421e-04 - acc: 1.000 - ETA: 1s - loss: 1.6367e-04 - acc: 1.000 - ETA: 1s - loss: 1.5858e-04 - acc: 1.000 - ETA: 0s - loss: 1.7725e-04 - acc: 1.000 - ETA: 0s - loss: 2.0030e-04 - acc: 1.000 - ETA: 0s - loss: 1.9398e-04 - acc: 1.000 - ETA: 0s - loss: 1.8918e-04 - acc: 1.000 - ETA: 0s - loss: 1.8372e-04 - acc: 1.000 - ETA: 0s - loss: 1.8084e-04 - acc: 1.000 - 5s 5ms/step - loss: 1.7614e-04 - acc: 1.0000 - val_loss: 0.0427 - val_acc: 0.9925\n",
      "Epoch 33/50\n",
      "960/960 [==============================] - ETA: 5s - loss: 5.1764e-05 - acc: 1.000 - ETA: 4s - loss: 8.3518e-05 - acc: 1.000 - ETA: 4s - loss: 3.7572e-04 - acc: 1.000 - ETA: 4s - loss: 3.4913e-04 - acc: 1.000 - ETA: 4s - loss: 2.9081e-04 - acc: 1.000 - ETA: 4s - loss: 2.5082e-04 - acc: 1.000 - ETA: 4s - loss: 2.2226e-04 - acc: 1.000 - ETA: 3s - loss: 2.0030e-04 - acc: 1.000 - ETA: 3s - loss: 2.2791e-04 - acc: 1.000 - ETA: 3s - loss: 2.0902e-04 - acc: 1.000 - ETA: 3s - loss: 1.9241e-04 - acc: 1.000 - ETA: 3s - loss: 1.8039e-04 - acc: 1.000 - ETA: 3s - loss: 1.6816e-04 - acc: 1.000 - ETA: 2s - loss: 1.6176e-04 - acc: 1.000 - ETA: 2s - loss: 1.6296e-04 - acc: 1.000 - ETA: 2s - loss: 1.5760e-04 - acc: 1.000 - ETA: 2s - loss: 1.5527e-04 - acc: 1.000 - ETA: 2s - loss: 1.4999e-04 - acc: 1.000 - ETA: 2s - loss: 1.4286e-04 - acc: 1.000 - ETA: 1s - loss: 1.3827e-04 - acc: 1.000 - ETA: 1s - loss: 1.3720e-04 - acc: 1.000 - ETA: 1s - loss: 1.4299e-04 - acc: 1.000 - ETA: 1s - loss: 1.3889e-04 - acc: 1.000 - ETA: 1s - loss: 1.4172e-04 - acc: 1.000 - ETA: 0s - loss: 1.3742e-04 - acc: 1.000 - ETA: 0s - loss: 1.4165e-04 - acc: 1.000 - ETA: 0s - loss: 1.5174e-04 - acc: 1.000 - ETA: 0s - loss: 1.4784e-04 - acc: 1.000 - ETA: 0s - loss: 1.4615e-04 - acc: 1.000 - 5s 6ms/step - loss: 1.4178e-04 - acc: 1.0000 - val_loss: 0.0434 - val_acc: 0.9925\n",
      "Epoch 34/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 1.1370e-04 - acc: 1.000 - ETA: 4s - loss: 1.6610e-04 - acc: 1.000 - ETA: 4s - loss: 2.1022e-04 - acc: 1.000 - ETA: 4s - loss: 1.7307e-04 - acc: 1.000 - ETA: 4s - loss: 1.6506e-04 - acc: 1.000 - ETA: 4s - loss: 1.4080e-04 - acc: 1.000 - ETA: 3s - loss: 1.2746e-04 - acc: 1.000 - ETA: 3s - loss: 1.1535e-04 - acc: 1.000 - ETA: 3s - loss: 1.0401e-04 - acc: 1.000 - ETA: 3s - loss: 9.7117e-05 - acc: 1.000 - ETA: 3s - loss: 1.0035e-04 - acc: 1.000 - ETA: 3s - loss: 1.1203e-04 - acc: 1.000 - ETA: 2s - loss: 1.3535e-04 - acc: 1.000 - ETA: 2s - loss: 1.3480e-04 - acc: 1.000 - ETA: 2s - loss: 1.8544e-04 - acc: 1.000 - ETA: 2s - loss: 1.9377e-04 - acc: 1.000 - ETA: 2s - loss: 1.8279e-04 - acc: 1.000 - ETA: 2s - loss: 1.7689e-04 - acc: 1.000 - ETA: 1s - loss: 1.7081e-04 - acc: 1.000 - ETA: 1s - loss: 1.7530e-04 - acc: 1.000 - ETA: 1s - loss: 1.6843e-04 - acc: 1.000 - ETA: 1s - loss: 1.7403e-04 - acc: 1.000 - ETA: 1s - loss: 1.6803e-04 - acc: 1.000 - ETA: 1s - loss: 1.6196e-04 - acc: 1.000 - ETA: 0s - loss: 1.5832e-04 - acc: 1.000 - ETA: 0s - loss: 1.5302e-04 - acc: 1.000 - ETA: 0s - loss: 1.5789e-04 - acc: 1.000 - ETA: 0s - loss: 1.5375e-04 - acc: 1.000 - ETA: 0s - loss: 1.5325e-04 - acc: 1.000 - 5s 5ms/step - loss: 1.7396e-04 - acc: 1.0000 - val_loss: 0.0440 - val_acc: 0.9925\n",
      "Epoch 35/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 5.2313e-05 - acc: 1.000 - ETA: 4s - loss: 5.3761e-05 - acc: 1.000 - ETA: 4s - loss: 4.4071e-05 - acc: 1.000 - ETA: 4s - loss: 3.7756e-05 - acc: 1.000 - ETA: 3s - loss: 3.8070e-05 - acc: 1.000 - ETA: 3s - loss: 1.1066e-04 - acc: 1.000 - ETA: 3s - loss: 9.6911e-05 - acc: 1.000 - ETA: 3s - loss: 9.2040e-05 - acc: 1.000 - ETA: 3s - loss: 1.0953e-04 - acc: 1.000 - ETA: 3s - loss: 1.0286e-04 - acc: 1.000 - ETA: 3s - loss: 1.0277e-04 - acc: 1.000 - ETA: 2s - loss: 1.3431e-04 - acc: 1.000 - ETA: 2s - loss: 1.3039e-04 - acc: 1.000 - ETA: 2s - loss: 1.3809e-04 - acc: 1.000 - ETA: 2s - loss: 1.4280e-04 - acc: 1.000 - ETA: 2s - loss: 1.3655e-04 - acc: 1.000 - ETA: 2s - loss: 1.3605e-04 - acc: 1.000 - ETA: 1s - loss: 1.4105e-04 - acc: 1.000 - ETA: 1s - loss: 1.4498e-04 - acc: 1.000 - ETA: 1s - loss: 1.5244e-04 - acc: 1.000 - ETA: 1s - loss: 1.4650e-04 - acc: 1.000 - ETA: 1s - loss: 1.4703e-04 - acc: 1.000 - ETA: 1s - loss: 1.4446e-04 - acc: 1.000 - ETA: 0s - loss: 1.4041e-04 - acc: 1.000 - ETA: 0s - loss: 1.3555e-04 - acc: 1.000 - ETA: 0s - loss: 1.3175e-04 - acc: 1.000 - ETA: 0s - loss: 1.3037e-04 - acc: 1.000 - ETA: 0s - loss: 1.2853e-04 - acc: 1.000 - ETA: 0s - loss: 1.2871e-04 - acc: 1.000 - 5s 5ms/step - loss: 1.4977e-04 - acc: 1.0000 - val_loss: 0.0481 - val_acc: 0.9925\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/960 [==============================] - ETA: 4s - loss: 6.3722e-06 - acc: 1.000 - ETA: 4s - loss: 5.0110e-05 - acc: 1.000 - ETA: 4s - loss: 5.9085e-05 - acc: 1.000 - ETA: 4s - loss: 8.4086e-05 - acc: 1.000 - ETA: 3s - loss: 9.3677e-05 - acc: 1.000 - ETA: 3s - loss: 1.2394e-04 - acc: 1.000 - ETA: 3s - loss: 1.2902e-04 - acc: 1.000 - ETA: 3s - loss: 1.4696e-04 - acc: 1.000 - ETA: 3s - loss: 1.4055e-04 - acc: 1.000 - ETA: 3s - loss: 1.3011e-04 - acc: 1.000 - ETA: 3s - loss: 1.2833e-04 - acc: 1.000 - ETA: 2s - loss: 1.1873e-04 - acc: 1.000 - ETA: 2s - loss: 1.4795e-04 - acc: 1.000 - ETA: 2s - loss: 1.4010e-04 - acc: 1.000 - ETA: 2s - loss: 1.3573e-04 - acc: 1.000 - ETA: 2s - loss: 1.3212e-04 - acc: 1.000 - ETA: 2s - loss: 2.6417e-04 - acc: 1.000 - ETA: 1s - loss: 2.5259e-04 - acc: 1.000 - ETA: 1s - loss: 2.4097e-04 - acc: 1.000 - ETA: 1s - loss: 2.3459e-04 - acc: 1.000 - ETA: 1s - loss: 2.2882e-04 - acc: 1.000 - ETA: 1s - loss: 3.3713e-04 - acc: 1.000 - ETA: 1s - loss: 3.2313e-04 - acc: 1.000 - ETA: 0s - loss: 3.1075e-04 - acc: 1.000 - ETA: 0s - loss: 2.9901e-04 - acc: 1.000 - ETA: 0s - loss: 2.9575e-04 - acc: 1.000 - ETA: 0s - loss: 2.8601e-04 - acc: 1.000 - ETA: 0s - loss: 2.7601e-04 - acc: 1.000 - ETA: 0s - loss: 2.6748e-04 - acc: 1.000 - 5s 5ms/step - loss: 2.6186e-04 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9888\n",
      "Epoch 37/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 4.3337e-05 - acc: 1.000 - ETA: 4s - loss: 3.5024e-05 - acc: 1.000 - ETA: 4s - loss: 6.0454e-05 - acc: 1.000 - ETA: 4s - loss: 6.0395e-05 - acc: 1.000 - ETA: 4s - loss: 3.8738e-04 - acc: 1.000 - ETA: 3s - loss: 3.7690e-04 - acc: 1.000 - ETA: 3s - loss: 3.2785e-04 - acc: 1.000 - ETA: 3s - loss: 3.0281e-04 - acc: 1.000 - ETA: 3s - loss: 2.8305e-04 - acc: 1.000 - ETA: 3s - loss: 2.5495e-04 - acc: 1.000 - ETA: 3s - loss: 2.5153e-04 - acc: 1.000 - ETA: 2s - loss: 2.3898e-04 - acc: 1.000 - ETA: 2s - loss: 2.2272e-04 - acc: 1.000 - ETA: 2s - loss: 2.4154e-04 - acc: 1.000 - ETA: 2s - loss: 2.2781e-04 - acc: 1.000 - ETA: 2s - loss: 2.2933e-04 - acc: 1.000 - ETA: 2s - loss: 2.2194e-04 - acc: 1.000 - ETA: 1s - loss: 2.1130e-04 - acc: 1.000 - ETA: 1s - loss: 2.0139e-04 - acc: 1.000 - ETA: 1s - loss: 1.9305e-04 - acc: 1.000 - ETA: 1s - loss: 1.8724e-04 - acc: 1.000 - ETA: 1s - loss: 1.8838e-04 - acc: 1.000 - ETA: 1s - loss: 1.8477e-04 - acc: 1.000 - ETA: 0s - loss: 1.8204e-04 - acc: 1.000 - ETA: 0s - loss: 1.9828e-04 - acc: 1.000 - ETA: 0s - loss: 1.9219e-04 - acc: 1.000 - ETA: 0s - loss: 1.8799e-04 - acc: 1.000 - ETA: 0s - loss: 1.9516e-04 - acc: 1.000 - ETA: 0s - loss: 1.9147e-04 - acc: 1.000 - 5s 5ms/step - loss: 2.0886e-04 - acc: 1.0000 - val_loss: 0.0437 - val_acc: 0.9925\n",
      "Epoch 38/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 5.7788e-05 - acc: 1.000 - ETA: 4s - loss: 5.7269e-05 - acc: 1.000 - ETA: 4s - loss: 3.9406e-05 - acc: 1.000 - ETA: 3s - loss: 3.7342e-05 - acc: 1.000 - ETA: 3s - loss: 3.2438e-05 - acc: 1.000 - ETA: 3s - loss: 5.3753e-05 - acc: 1.000 - ETA: 3s - loss: 4.9630e-05 - acc: 1.000 - ETA: 3s - loss: 6.0551e-05 - acc: 1.000 - ETA: 3s - loss: 5.7639e-05 - acc: 1.000 - ETA: 3s - loss: 5.9529e-05 - acc: 1.000 - ETA: 2s - loss: 6.0344e-05 - acc: 1.000 - ETA: 2s - loss: 6.0030e-05 - acc: 1.000 - ETA: 2s - loss: 5.5750e-05 - acc: 1.000 - ETA: 2s - loss: 8.2007e-05 - acc: 1.000 - ETA: 2s - loss: 7.9807e-05 - acc: 1.000 - ETA: 2s - loss: 7.7056e-05 - acc: 1.000 - ETA: 2s - loss: 7.3622e-05 - acc: 1.000 - ETA: 1s - loss: 7.1921e-05 - acc: 1.000 - ETA: 1s - loss: 7.0019e-05 - acc: 1.000 - ETA: 1s - loss: 6.8640e-05 - acc: 1.000 - ETA: 1s - loss: 9.3911e-05 - acc: 1.000 - ETA: 1s - loss: 9.0580e-05 - acc: 1.000 - ETA: 1s - loss: 8.8124e-05 - acc: 1.000 - ETA: 0s - loss: 8.4867e-05 - acc: 1.000 - ETA: 0s - loss: 9.7048e-05 - acc: 1.000 - ETA: 0s - loss: 9.8253e-05 - acc: 1.000 - ETA: 0s - loss: 9.6022e-05 - acc: 1.000 - ETA: 0s - loss: 9.4760e-05 - acc: 1.000 - ETA: 0s - loss: 1.9036e-04 - acc: 1.000 - 5s 5ms/step - loss: 1.8425e-04 - acc: 1.0000 - val_loss: 0.0421 - val_acc: 0.9925\n",
      "Epoch 39/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 1.0962e-04 - acc: 1.000 - ETA: 4s - loss: 1.1285e-04 - acc: 1.000 - ETA: 4s - loss: 1.8594e-04 - acc: 1.000 - ETA: 3s - loss: 1.4136e-04 - acc: 1.000 - ETA: 3s - loss: 1.1502e-04 - acc: 1.000 - ETA: 3s - loss: 1.4737e-04 - acc: 1.000 - ETA: 3s - loss: 1.3460e-04 - acc: 1.000 - ETA: 3s - loss: 1.2131e-04 - acc: 1.000 - ETA: 3s - loss: 1.7248e-04 - acc: 1.000 - ETA: 3s - loss: 2.9278e-04 - acc: 1.000 - ETA: 2s - loss: 2.9164e-04 - acc: 1.000 - ETA: 2s - loss: 2.7158e-04 - acc: 1.000 - ETA: 2s - loss: 3.8596e-04 - acc: 1.000 - ETA: 2s - loss: 3.6316e-04 - acc: 1.000 - ETA: 2s - loss: 3.3948e-04 - acc: 1.000 - ETA: 2s - loss: 3.2240e-04 - acc: 1.000 - ETA: 2s - loss: 3.0499e-04 - acc: 1.000 - ETA: 1s - loss: 2.9100e-04 - acc: 1.000 - ETA: 1s - loss: 2.7730e-04 - acc: 1.000 - ETA: 1s - loss: 2.6416e-04 - acc: 1.000 - ETA: 1s - loss: 2.6311e-04 - acc: 1.000 - ETA: 1s - loss: 2.5454e-04 - acc: 1.000 - ETA: 1s - loss: 2.4607e-04 - acc: 1.000 - ETA: 0s - loss: 2.4663e-04 - acc: 1.000 - ETA: 0s - loss: 2.3810e-04 - acc: 1.000 - ETA: 0s - loss: 2.3183e-04 - acc: 1.000 - ETA: 0s - loss: 2.2679e-04 - acc: 1.000 - ETA: 0s - loss: 2.2851e-04 - acc: 1.000 - ETA: 0s - loss: 2.2173e-04 - acc: 1.000 - 5s 5ms/step - loss: 2.3016e-04 - acc: 1.0000 - val_loss: 0.0652 - val_acc: 0.9925\n",
      "Epoch 40/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 2.9653e-04 - acc: 1.000 - ETA: 4s - loss: 1.6532e-04 - acc: 1.000 - ETA: 4s - loss: 1.6432e-04 - acc: 1.000 - ETA: 3s - loss: 1.2831e-04 - acc: 1.000 - ETA: 3s - loss: 3.2267e-04 - acc: 1.000 - ETA: 3s - loss: 2.8278e-04 - acc: 1.000 - ETA: 3s - loss: 3.5651e-04 - acc: 1.000 - ETA: 3s - loss: 3.9416e-04 - acc: 1.000 - ETA: 3s - loss: 3.5390e-04 - acc: 1.000 - ETA: 3s - loss: 3.2292e-04 - acc: 1.000 - ETA: 2s - loss: 2.9699e-04 - acc: 1.000 - ETA: 2s - loss: 2.7424e-04 - acc: 1.000 - ETA: 2s - loss: 2.6241e-04 - acc: 1.000 - ETA: 2s - loss: 2.5162e-04 - acc: 1.000 - ETA: 2s - loss: 2.3859e-04 - acc: 1.000 - ETA: 2s - loss: 2.3341e-04 - acc: 1.000 - ETA: 2s - loss: 2.2573e-04 - acc: 1.000 - ETA: 1s - loss: 2.1391e-04 - acc: 1.000 - ETA: 1s - loss: 2.0465e-04 - acc: 1.000 - ETA: 1s - loss: 2.1821e-04 - acc: 1.000 - ETA: 1s - loss: 2.1293e-04 - acc: 1.000 - ETA: 1s - loss: 2.0523e-04 - acc: 1.000 - ETA: 1s - loss: 1.9862e-04 - acc: 1.000 - ETA: 0s - loss: 1.9231e-04 - acc: 1.000 - ETA: 0s - loss: 2.3188e-04 - acc: 1.000 - ETA: 0s - loss: 2.5339e-04 - acc: 1.000 - ETA: 0s - loss: 2.7355e-04 - acc: 1.000 - ETA: 0s - loss: 2.6448e-04 - acc: 1.000 - ETA: 0s - loss: 2.5635e-04 - acc: 1.000 - 5s 5ms/step - loss: 2.5037e-04 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 0.9850\n",
      "Epoch 41/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 3.0814e-05 - acc: 1.000 - ETA: 4s - loss: 2.0919e-05 - acc: 1.000 - ETA: 4s - loss: 8.2908e-05 - acc: 1.000 - ETA: 4s - loss: 8.3446e-05 - acc: 1.000 - ETA: 3s - loss: 1.1981e-04 - acc: 1.000 - ETA: 3s - loss: 1.0425e-04 - acc: 1.000 - ETA: 3s - loss: 9.6297e-05 - acc: 1.000 - ETA: 3s - loss: 8.9052e-05 - acc: 1.000 - ETA: 3s - loss: 8.3696e-05 - acc: 1.000 - ETA: 3s - loss: 7.9233e-05 - acc: 1.000 - ETA: 3s - loss: 7.8303e-05 - acc: 1.000 - ETA: 2s - loss: 7.2606e-05 - acc: 1.000 - ETA: 2s - loss: 7.1685e-05 - acc: 1.000 - ETA: 2s - loss: 6.7266e-05 - acc: 1.000 - ETA: 2s - loss: 8.0121e-05 - acc: 1.000 - ETA: 2s - loss: 8.2406e-05 - acc: 1.000 - ETA: 2s - loss: 7.8947e-05 - acc: 1.000 - ETA: 1s - loss: 7.6562e-05 - acc: 1.000 - ETA: 1s - loss: 7.6888e-05 - acc: 1.000 - ETA: 1s - loss: 7.3422e-05 - acc: 1.000 - ETA: 1s - loss: 7.1974e-05 - acc: 1.000 - ETA: 1s - loss: 1.3351e-04 - acc: 1.000 - ETA: 1s - loss: 1.2851e-04 - acc: 1.000 - ETA: 0s - loss: 1.3040e-04 - acc: 1.000 - ETA: 0s - loss: 1.2663e-04 - acc: 1.000 - ETA: 0s - loss: 1.2531e-04 - acc: 1.000 - ETA: 0s - loss: 1.2188e-04 - acc: 1.000 - ETA: 0s - loss: 1.1900e-04 - acc: 1.000 - ETA: 0s - loss: 1.1633e-04 - acc: 1.000 - 5s 5ms/step - loss: 1.1321e-04 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 0.9850\n",
      "Epoch 42/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 1.1748e-04 - acc: 1.000 - ETA: 4s - loss: 8.8184e-05 - acc: 1.000 - ETA: 4s - loss: 2.7243e-04 - acc: 1.000 - ETA: 3s - loss: 2.3539e-04 - acc: 1.000 - ETA: 3s - loss: 1.9802e-04 - acc: 1.000 - ETA: 3s - loss: 1.7030e-04 - acc: 1.000 - ETA: 3s - loss: 1.6528e-04 - acc: 1.000 - ETA: 3s - loss: 2.6878e-04 - acc: 1.000 - ETA: 3s - loss: 2.4882e-04 - acc: 1.000 - ETA: 3s - loss: 2.2664e-04 - acc: 1.000 - ETA: 2s - loss: 2.0819e-04 - acc: 1.000 - ETA: 2s - loss: 1.9413e-04 - acc: 1.000 - ETA: 2s - loss: 2.0573e-04 - acc: 1.000 - ETA: 2s - loss: 1.9739e-04 - acc: 1.000 - ETA: 2s - loss: 1.8776e-04 - acc: 1.000 - ETA: 2s - loss: 1.7905e-04 - acc: 1.000 - ETA: 2s - loss: 1.7032e-04 - acc: 1.000 - ETA: 1s - loss: 1.6297e-04 - acc: 1.000 - ETA: 1s - loss: 1.5487e-04 - acc: 1.000 - ETA: 1s - loss: 1.4816e-04 - acc: 1.000 - ETA: 1s - loss: 1.4432e-04 - acc: 1.000 - ETA: 1s - loss: 1.4014e-04 - acc: 1.000 - ETA: 1s - loss: 1.3520e-04 - acc: 1.000 - ETA: 0s - loss: 1.3033e-04 - acc: 1.000 - ETA: 0s - loss: 1.4657e-04 - acc: 1.000 - ETA: 0s - loss: 1.4170e-04 - acc: 1.000 - ETA: 0s - loss: 1.3906e-04 - acc: 1.000 - ETA: 0s - loss: 1.3741e-04 - acc: 1.000 - ETA: 0s - loss: 1.3312e-04 - acc: 1.000 - 5s 5ms/step - loss: 1.2953e-04 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9850\n",
      "Epoch 43/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 2.7864e-04 - acc: 1.000 - ETA: 4s - loss: 1.5462e-04 - acc: 1.000 - ETA: 4s - loss: 1.2081e-04 - acc: 1.000 - ETA: 4s - loss: 9.1326e-05 - acc: 1.000 - ETA: 3s - loss: 8.0567e-05 - acc: 1.000 - ETA: 3s - loss: 6.9361e-05 - acc: 1.000 - ETA: 3s - loss: 6.5140e-05 - acc: 1.000 - ETA: 3s - loss: 6.9109e-05 - acc: 1.000 - ETA: 3s - loss: 8.0398e-05 - acc: 1.000 - ETA: 3s - loss: 7.8630e-05 - acc: 1.000 - ETA: 3s - loss: 7.6303e-05 - acc: 1.000 - ETA: 2s - loss: 7.4699e-05 - acc: 1.000 - ETA: 2s - loss: 7.3967e-05 - acc: 1.000 - ETA: 2s - loss: 7.1561e-05 - acc: 1.000 - ETA: 2s - loss: 7.3116e-05 - acc: 1.000 - ETA: 2s - loss: 6.8629e-05 - acc: 1.000 - ETA: 2s - loss: 7.4128e-05 - acc: 1.000 - ETA: 1s - loss: 7.0908e-05 - acc: 1.000 - ETA: 1s - loss: 6.7994e-05 - acc: 1.000 - ETA: 1s - loss: 6.5843e-05 - acc: 1.000 - ETA: 1s - loss: 6.4104e-05 - acc: 1.000 - ETA: 1s - loss: 6.3016e-05 - acc: 1.000 - ETA: 1s - loss: 6.3838e-05 - acc: 1.000 - ETA: 0s - loss: 6.8760e-05 - acc: 1.000 - ETA: 0s - loss: 6.8977e-05 - acc: 1.000 - ETA: 0s - loss: 6.8732e-05 - acc: 1.000 - ETA: 0s - loss: 6.7119e-05 - acc: 1.000 - ETA: 0s - loss: 8.4583e-05 - acc: 1.000 - ETA: 0s - loss: 8.2744e-05 - acc: 1.000 - 5s 5ms/step - loss: 8.0219e-05 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 0.9850\n",
      "Epoch 44/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 4.2140e-05 - acc: 1.000 - ETA: 4s - loss: 1.2162e-04 - acc: 1.000 - ETA: 4s - loss: 1.2014e-04 - acc: 1.000 - ETA: 4s - loss: 9.4777e-05 - acc: 1.000 - ETA: 4s - loss: 8.1455e-05 - acc: 1.000 - ETA: 3s - loss: 7.0726e-05 - acc: 1.000 - ETA: 3s - loss: 7.5035e-05 - acc: 1.000 - ETA: 3s - loss: 7.1005e-05 - acc: 1.000 - ETA: 3s - loss: 6.7154e-05 - acc: 1.000 - ETA: 3s - loss: 1.1721e-04 - acc: 1.000 - ETA: 3s - loss: 1.1008e-04 - acc: 1.000 - ETA: 2s - loss: 1.0272e-04 - acc: 1.000 - ETA: 2s - loss: 1.1553e-04 - acc: 1.000 - ETA: 2s - loss: 1.1097e-04 - acc: 1.000 - ETA: 2s - loss: 1.0406e-04 - acc: 1.000 - ETA: 2s - loss: 1.0568e-04 - acc: 1.000 - ETA: 2s - loss: 9.9529e-05 - acc: 1.000 - ETA: 1s - loss: 1.0354e-04 - acc: 1.000 - ETA: 1s - loss: 9.8409e-05 - acc: 1.000 - ETA: 1s - loss: 9.4730e-05 - acc: 1.000 - ETA: 1s - loss: 9.1829e-05 - acc: 1.000 - ETA: 1s - loss: 9.4340e-05 - acc: 1.000 - ETA: 1s - loss: 9.0674e-05 - acc: 1.000 - ETA: 0s - loss: 8.6966e-05 - acc: 1.000 - ETA: 0s - loss: 8.3936e-05 - acc: 1.000 - ETA: 0s - loss: 8.4763e-05 - acc: 1.000 - ETA: 0s - loss: 8.4663e-05 - acc: 1.000 - ETA: 0s - loss: 8.2374e-05 - acc: 1.000 - ETA: 0s - loss: 8.8232e-05 - acc: 1.000 - 5s 5ms/step - loss: 8.6229e-05 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9850\n",
      "Epoch 45/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 2.0552e-04 - acc: 1.000 - ETA: 4s - loss: 1.0616e-04 - acc: 1.000 - ETA: 4s - loss: 7.9119e-05 - acc: 1.000 - ETA: 4s - loss: 7.1578e-05 - acc: 1.000 - ETA: 3s - loss: 6.3022e-05 - acc: 1.000 - ETA: 3s - loss: 6.4393e-05 - acc: 1.000 - ETA: 3s - loss: 5.8717e-05 - acc: 1.000 - ETA: 3s - loss: 5.1829e-05 - acc: 1.000 - ETA: 3s - loss: 4.7134e-05 - acc: 1.000 - ETA: 3s - loss: 4.3757e-05 - acc: 1.000 - ETA: 3s - loss: 4.3181e-05 - acc: 1.000 - ETA: 2s - loss: 4.1172e-05 - acc: 1.000 - ETA: 2s - loss: 6.4281e-05 - acc: 1.000 - ETA: 2s - loss: 6.0202e-05 - acc: 1.000 - ETA: 2s - loss: 6.3307e-05 - acc: 1.000 - ETA: 2s - loss: 6.5468e-05 - acc: 1.000 - ETA: 2s - loss: 6.2155e-05 - acc: 1.000 - ETA: 1s - loss: 5.9930e-05 - acc: 1.000 - ETA: 1s - loss: 5.8473e-05 - acc: 1.000 - ETA: 1s - loss: 6.9128e-05 - acc: 1.000 - ETA: 1s - loss: 7.2661e-05 - acc: 1.000 - ETA: 1s - loss: 7.0716e-05 - acc: 1.000 - ETA: 1s - loss: 6.9097e-05 - acc: 1.000 - ETA: 0s - loss: 6.6567e-05 - acc: 1.000 - ETA: 0s - loss: 6.5071e-05 - acc: 1.000 - ETA: 0s - loss: 6.4710e-05 - acc: 1.000 - ETA: 0s - loss: 6.9687e-05 - acc: 1.000 - ETA: 0s - loss: 6.8013e-05 - acc: 1.000 - ETA: 0s - loss: 7.6354e-05 - acc: 1.000 - 5s 5ms/step - loss: 7.5053e-05 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9850\n",
      "Epoch 46/50\n",
      "960/960 [==============================] - ETA: 5s - loss: 2.8575e-04 - acc: 1.000 - ETA: 4s - loss: 1.4520e-04 - acc: 1.000 - ETA: 4s - loss: 1.0920e-04 - acc: 1.000 - ETA: 4s - loss: 8.4908e-05 - acc: 1.000 - ETA: 4s - loss: 7.3990e-05 - acc: 1.000 - ETA: 4s - loss: 6.2595e-05 - acc: 1.000 - ETA: 4s - loss: 8.4809e-05 - acc: 1.000 - ETA: 4s - loss: 1.0868e-04 - acc: 1.000 - ETA: 4s - loss: 9.9679e-05 - acc: 1.000 - ETA: 3s - loss: 9.3453e-05 - acc: 1.000 - ETA: 3s - loss: 8.6835e-05 - acc: 1.000 - ETA: 3s - loss: 8.0828e-05 - acc: 1.000 - ETA: 3s - loss: 1.2843e-04 - acc: 1.000 - ETA: 3s - loss: 1.2025e-04 - acc: 1.000 - ETA: 2s - loss: 1.1312e-04 - acc: 1.000 - ETA: 2s - loss: 1.0949e-04 - acc: 1.000 - ETA: 2s - loss: 1.1105e-04 - acc: 1.000 - ETA: 2s - loss: 1.0668e-04 - acc: 1.000 - ETA: 2s - loss: 1.0159e-04 - acc: 1.000 - ETA: 1s - loss: 1.0199e-04 - acc: 1.000 - ETA: 1s - loss: 1.0225e-04 - acc: 1.000 - ETA: 1s - loss: 9.8828e-05 - acc: 1.000 - ETA: 1s - loss: 1.0217e-04 - acc: 1.000 - ETA: 1s - loss: 9.8189e-05 - acc: 1.000 - ETA: 0s - loss: 9.5499e-05 - acc: 1.000 - ETA: 0s - loss: 9.2875e-05 - acc: 1.000 - ETA: 0s - loss: 9.0725e-05 - acc: 1.000 - ETA: 0s - loss: 8.8642e-05 - acc: 1.000 - ETA: 0s - loss: 8.6004e-05 - acc: 1.000 - 6s 6ms/step - loss: 8.5365e-05 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9850\n",
      "Epoch 47/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 1.1391e-04 - acc: 1.000 - ETA: 4s - loss: 6.1895e-05 - acc: 1.000 - ETA: 4s - loss: 6.2939e-05 - acc: 1.000 - ETA: 4s - loss: 6.1484e-05 - acc: 1.000 - ETA: 4s - loss: 6.9630e-05 - acc: 1.000 - ETA: 4s - loss: 1.1189e-04 - acc: 1.000 - ETA: 4s - loss: 1.2667e-04 - acc: 1.000 - ETA: 3s - loss: 1.1437e-04 - acc: 1.000 - ETA: 3s - loss: 1.0291e-04 - acc: 1.000 - ETA: 3s - loss: 9.4667e-05 - acc: 1.000 - ETA: 3s - loss: 8.7915e-05 - acc: 1.000 - ETA: 3s - loss: 8.4189e-05 - acc: 1.000 - ETA: 2s - loss: 8.0204e-05 - acc: 1.000 - ETA: 2s - loss: 7.7798e-05 - acc: 1.000 - ETA: 2s - loss: 7.4207e-05 - acc: 1.000 - ETA: 2s - loss: 7.0709e-05 - acc: 1.000 - ETA: 2s - loss: 7.2020e-05 - acc: 1.000 - ETA: 2s - loss: 6.8952e-05 - acc: 1.000 - ETA: 1s - loss: 6.6438e-05 - acc: 1.000 - ETA: 1s - loss: 6.9942e-05 - acc: 1.000 - ETA: 1s - loss: 6.7749e-05 - acc: 1.000 - ETA: 1s - loss: 6.5757e-05 - acc: 1.000 - ETA: 1s - loss: 6.6605e-05 - acc: 1.000 - ETA: 1s - loss: 6.6595e-05 - acc: 1.000 - ETA: 0s - loss: 6.5309e-05 - acc: 1.000 - ETA: 0s - loss: 6.3632e-05 - acc: 1.000 - ETA: 0s - loss: 6.1989e-05 - acc: 1.000 - ETA: 0s - loss: 6.0819e-05 - acc: 1.000 - ETA: 0s - loss: 5.9683e-05 - acc: 1.000 - 5s 6ms/step - loss: 5.9096e-05 - acc: 1.0000 - val_loss: 0.0541 - val_acc: 0.9850\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/960 [==============================] - ETA: 4s - loss: 1.6345e-05 - acc: 1.000 - ETA: 4s - loss: 1.5501e-05 - acc: 1.000 - ETA: 4s - loss: 1.5515e-05 - acc: 1.000 - ETA: 4s - loss: 2.1324e-05 - acc: 1.000 - ETA: 4s - loss: 1.8693e-05 - acc: 1.000 - ETA: 4s - loss: 2.2291e-05 - acc: 1.000 - ETA: 3s - loss: 2.0844e-05 - acc: 1.000 - ETA: 3s - loss: 2.3853e-05 - acc: 1.000 - ETA: 3s - loss: 2.3094e-05 - acc: 1.000 - ETA: 3s - loss: 2.4376e-05 - acc: 1.000 - ETA: 3s - loss: 2.3007e-05 - acc: 1.000 - ETA: 2s - loss: 2.4713e-05 - acc: 1.000 - ETA: 2s - loss: 3.2197e-05 - acc: 1.000 - ETA: 2s - loss: 3.5411e-05 - acc: 1.000 - ETA: 2s - loss: 3.5192e-05 - acc: 1.000 - ETA: 2s - loss: 3.3477e-05 - acc: 1.000 - ETA: 2s - loss: 3.4491e-05 - acc: 1.000 - ETA: 1s - loss: 3.4286e-05 - acc: 1.000 - ETA: 1s - loss: 3.3578e-05 - acc: 1.000 - ETA: 1s - loss: 3.6641e-05 - acc: 1.000 - ETA: 1s - loss: 3.5820e-05 - acc: 1.000 - ETA: 1s - loss: 3.6934e-05 - acc: 1.000 - ETA: 1s - loss: 3.8278e-05 - acc: 1.000 - ETA: 0s - loss: 3.6831e-05 - acc: 1.000 - ETA: 0s - loss: 3.5408e-05 - acc: 1.000 - ETA: 0s - loss: 3.4806e-05 - acc: 1.000 - ETA: 0s - loss: 3.4594e-05 - acc: 1.000 - ETA: 0s - loss: 3.5564e-05 - acc: 1.000 - ETA: 0s - loss: 3.4766e-05 - acc: 1.000 - 5s 5ms/step - loss: 3.4179e-05 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 0.9850\n",
      "Epoch 49/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 1.2140e-04 - acc: 1.000 - ETA: 4s - loss: 6.1243e-05 - acc: 1.000 - ETA: 4s - loss: 4.9994e-05 - acc: 1.000 - ETA: 4s - loss: 4.1737e-05 - acc: 1.000 - ETA: 4s - loss: 3.7635e-05 - acc: 1.000 - ETA: 3s - loss: 3.2107e-05 - acc: 1.000 - ETA: 3s - loss: 3.5897e-05 - acc: 1.000 - ETA: 3s - loss: 4.1729e-05 - acc: 1.000 - ETA: 3s - loss: 5.1640e-05 - acc: 1.000 - ETA: 3s - loss: 4.8428e-05 - acc: 1.000 - ETA: 3s - loss: 4.4594e-05 - acc: 1.000 - ETA: 2s - loss: 4.8082e-05 - acc: 1.000 - ETA: 2s - loss: 4.5784e-05 - acc: 1.000 - ETA: 2s - loss: 4.4166e-05 - acc: 1.000 - ETA: 2s - loss: 4.2949e-05 - acc: 1.000 - ETA: 2s - loss: 4.3063e-05 - acc: 1.000 - ETA: 2s - loss: 4.4808e-05 - acc: 1.000 - ETA: 1s - loss: 4.9851e-05 - acc: 1.000 - ETA: 1s - loss: 4.9175e-05 - acc: 1.000 - ETA: 1s - loss: 4.9417e-05 - acc: 1.000 - ETA: 1s - loss: 4.8389e-05 - acc: 1.000 - ETA: 1s - loss: 4.7696e-05 - acc: 1.000 - ETA: 1s - loss: 4.7699e-05 - acc: 1.000 - ETA: 0s - loss: 4.6091e-05 - acc: 1.000 - ETA: 0s - loss: 4.4974e-05 - acc: 1.000 - ETA: 0s - loss: 4.3777e-05 - acc: 1.000 - ETA: 0s - loss: 4.2259e-05 - acc: 1.000 - ETA: 0s - loss: 4.4714e-05 - acc: 1.000 - ETA: 0s - loss: 4.3496e-05 - acc: 1.000 - 5s 5ms/step - loss: 4.3288e-05 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 0.9850\n",
      "Epoch 50/50\n",
      "960/960 [==============================] - ETA: 4s - loss: 1.0291e-05 - acc: 1.000 - ETA: 4s - loss: 1.7756e-05 - acc: 1.000 - ETA: 4s - loss: 2.3584e-05 - acc: 1.000 - ETA: 4s - loss: 3.2606e-05 - acc: 1.000 - ETA: 4s - loss: 2.7004e-05 - acc: 1.000 - ETA: 3s - loss: 3.6299e-05 - acc: 1.000 - ETA: 3s - loss: 3.2126e-05 - acc: 1.000 - ETA: 3s - loss: 3.0826e-05 - acc: 1.000 - ETA: 3s - loss: 3.0359e-05 - acc: 1.000 - ETA: 3s - loss: 3.8051e-05 - acc: 1.000 - ETA: 3s - loss: 4.2311e-05 - acc: 1.000 - ETA: 2s - loss: 4.0723e-05 - acc: 1.000 - ETA: 2s - loss: 3.8159e-05 - acc: 1.000 - ETA: 2s - loss: 4.4266e-05 - acc: 1.000 - ETA: 2s - loss: 4.6798e-05 - acc: 1.000 - ETA: 2s - loss: 4.4655e-05 - acc: 1.000 - ETA: 2s - loss: 4.4193e-05 - acc: 1.000 - ETA: 1s - loss: 4.2186e-05 - acc: 1.000 - ETA: 1s - loss: 4.0543e-05 - acc: 1.000 - ETA: 1s - loss: 4.0095e-05 - acc: 1.000 - ETA: 1s - loss: 3.9664e-05 - acc: 1.000 - ETA: 1s - loss: 3.8762e-05 - acc: 1.000 - ETA: 1s - loss: 3.7825e-05 - acc: 1.000 - ETA: 0s - loss: 3.9612e-05 - acc: 1.000 - ETA: 0s - loss: 3.8207e-05 - acc: 1.000 - ETA: 0s - loss: 3.6942e-05 - acc: 1.000 - ETA: 0s - loss: 3.7993e-05 - acc: 1.000 - ETA: 0s - loss: 3.7223e-05 - acc: 1.000 - ETA: 0s - loss: 4.0416e-05 - acc: 1.000 - 5s 5ms/step - loss: 3.9518e-05 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2490edfd588>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "c.save('Leave_Inquiry_Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "train = train[train['Type'] == 'leave_inquiry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m = load_model('Leave_Inquiry_Model.h5')\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['sub_type_two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"update me about leave approving authority\"\n",
    "#query = 'is today a holiday'\n",
    "#query = 'how many leaves have i taken'\n",
    "#query = \"can you please tell me about my leave encashment\"\n",
    "#query = \"is my leave approved\"\n",
    "#query = \"will office open tomorrow\"\n",
    "#query = \"will office remain close tomorrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENRAL LEAVES 1#\n",
    "query = \"is office open today\"\n",
    "query = \"is this saturday on\"\n",
    "query = \"is tomorrow a working day\"\n",
    "query = \"Can you please inform me if today is working day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAVE APPROVING AUTHORITY 2#\n",
    "query = 'who is my leave approving authroity'\n",
    "query = 'kindly update me about my leave approveing authority'\n",
    "query = \"update me about leave approving authority\"\n",
    "query = 'Can you please update me about my leave approving authority'\n",
    "query = \"inform me who will approve my leaves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECIFIC 3#\n",
    "query = 'how many leaves have i taken'\n",
    "query = 'how many leaves can i take more'\n",
    "query = 'how many sick leaves are left of mine'\n",
    "query = 'can you please tell me how many sick leaves can i take more'\n",
    "query = 'update me about how many sick leaves can i take more'\n",
    "query = 'how many leaves has ms.shiza has taken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAVE ENCASHMENT 0#\n",
    "query = \"can you please tell me about leave encashment procedure\"\n",
    "query = \"how much will i recieve when i will encash my leaves\"\n",
    "query = \"what is the procedure of leave encashment\"\n",
    "query = \"can you please tell me the procedure of leave encashment\"\n",
    "query = \"update me with the procedure of leave encashment\"\n",
    "query = \"inform me about the procedure of leave encashment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAVE STATUS 4#\n",
    "query = \"is my leave approved\"\n",
    "query = \"kindly update me about if my leave is approved or not\"\n",
    "query = \"has my sick leave approved\"\n",
    "query = \"inform me about if my sick leave is approved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECIAL CASES\n",
    "query = 'is my leave approved'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'my', 'leav', 'approv']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = token_stems(sent)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test = ([list(sen)])\n",
    "sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.0927594e-06 9.6479249e-05 1.9483917e-04 5.2777606e-05 9.9965179e-01]]\n",
      "0.9996518\n"
     ]
    }
   ],
   "source": [
    "score = m.predict(sen_sequences_matrix)\n",
    "print(score)\n",
    "print(max(score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave approve authority 2 2\n",
      "genral 1 0\n",
      "specific 3 3\n",
      "encashment 0 4 0\n",
      "approval 4 4\n"
     ]
    }
   ],
   "source": [
    "print(\"leave approve authority 2 2\")\n",
    "print('genral 1 0')\n",
    "print('specific 3 3')\n",
    "print('encashment 0 4 0')\n",
    "print('approval 4 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def leave_inquiry_module(name):\n",
    "        sub_intent = ''\n",
    "        train= pd.read_excel(\"leave_final.xlsx\")\n",
    "        train = train[train['Type'] == 'leave_inquiry']\n",
    "        docs= train['Data']\n",
    "        tokens = []\n",
    "        for i in docs:\n",
    "            temp = token_stems(i)\n",
    "            tokens.append(temp)\n",
    "\n",
    "        x, y = np.asarray(tokens) , np.asarray(train['sub_type_two'])\n",
    "        xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)\n",
    "        max_len=200\n",
    "        max_words = 20000\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "        tok.fit_on_texts(x)\n",
    "        sequences = tok.texts_to_sequences(x)\n",
    "        test_sequences = tok.texts_to_sequences(xtest)\n",
    "\n",
    "        \n",
    "        score = leave_inquiry_model(name,tok)\n",
    "        print(score)\n",
    "        if((score[0][0] > score [0][1])&(score[0][0] > score [0][2])&(score[0][0] > score [0][3])&(score[0][0] > score [0][4])):\n",
    "            sub_intent = 'LeaveEncashment'\n",
    "        elif((score[0][1] > score [0][0])&(score[0][1] > score [0][2])&(score[0][1] > score [0][3])&(score[0][1] > score [0][4])):\n",
    "            sub_intent = 'LeaveGenral'\n",
    "        elif((score[0][2] > score [0][1])&(score[0][2] > score [0][0])&(score[0][2] > score [0][3])&(score[0][2] > score [0][4])):\n",
    "            sub_intent = 'LeaveReporting'\n",
    "        elif((score[0][3] > score [0][1])&(score[0][3] > score [0][0])&(score[0][3] > score [0][2])&(score[0][3] > score [0][4])):\n",
    "            sub_intent = 'Leave_Specific'+',\"' + leave_inquiry_specific(name) + '\"'\n",
    "        else:\n",
    "            sub_intent = 'LeaveApproval'\n",
    "        return sub_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def leave_inquiry_model(name,tok):\n",
    "        top_intent = ''\n",
    "        user_response = name\n",
    "        max_len=200\n",
    "        sen = token_stems(user_response)\n",
    "        sen_test = ([list(sen)])\n",
    "        print(sen_test)\n",
    "        sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "        sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)\n",
    "        m = load_model('Leave_Inquiry_Model.h5')\n",
    "        score = m.predict(sen_sequences_matrix)\n",
    "        K.clear_session()\n",
    "        print(user_response)\n",
    "        user_response=\"\"\n",
    "        sen=\"\"\n",
    "        sen_test=\"\"\n",
    "        intent=\"\"\n",
    "        Score=\"\"\n",
    "        print(score)\n",
    "        return(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave Inquiry Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def classify_leaveinquiry(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['Type']== 'leave_inquiry']\n",
    "        text = train['Data']\n",
    "        \n",
    "        status = train[train['sub_type_two'] == 'status']\n",
    "        specific = train[train['sub_type_two'] == 'specific']\n",
    "        encashment = train[train['sub_type_two'] == 'encashment']\n",
    "        reporting = train[train['sub_type_two'] == 'reporting']\n",
    "        genral = train[train['sub_type_two'] == 'genral']\n",
    "       \n",
    "        gen_status = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in status['Data']]\n",
    "        gen_specific = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in specific['Data']]\n",
    "        gen_encashment = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in encashment['Data']]\n",
    "        gen_reporting = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in reporting['Data']]\n",
    "        gen_genral = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in genral['Data']]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        dictionary_status = gensim.corpora.Dictionary(gen_status)\n",
    "        dictionary_specific = gensim.corpora.Dictionary(gen_specific)\n",
    "        dictionary_encashment = gensim.corpora.Dictionary(gen_encashment)\n",
    "        dictionary_reporting = gensim.corpora.Dictionary(gen_reporting)\n",
    "        dictionary_genral = gensim.corpora.Dictionary(gen_genral)\n",
    "        \n",
    "        corpus_status = [dictionary_status.doc2bow(gen_status) for gen_status in gen_status]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_status  = gensim.models.TfidfModel(corpus_status)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_status = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\status.txt',tf_idf_status[corpus_status],\n",
    "                                              num_features=len(dictionary_status))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_specific = [dictionary_specific.doc2bow(gen_specific) for gen_specific in gen_specific]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_specific  = gensim.models.TfidfModel(corpus_specific)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_specific = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\specific.txt',tf_idf_specific[corpus_specific],\n",
    "                                              num_features=len(dictionary_specific))\n",
    "\n",
    "        \n",
    "        \n",
    "        corpus_encashment = [dictionary_encashment.doc2bow(gen_encashment) for gen_encashment in gen_encashment]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_encashment  = gensim.models.TfidfModel(corpus_encashment)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_encashment = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\encashment.txt',tf_idf_encashment[corpus_encashment],\n",
    "                                              num_features=len(dictionary_encashment))\n",
    "        \n",
    "        \n",
    "        corpus_reporting = [dictionary_reporting.doc2bow(gen_reporting) for gen_reporting in gen_reporting]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_reporting  = gensim.models.TfidfModel(corpus_reporting)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_reporting = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\reporting.txt',tf_idf_reporting[corpus_reporting],\n",
    "                                              num_features=len(dictionary_reporting))\n",
    "        \n",
    "        \n",
    "        corpus_genral = [dictionary_genral.doc2bow(gen_genral) for gen_genral in gen_genral]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_genral  = gensim.models.TfidfModel(corpus_genral)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_genral = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\genral.txt',tf_idf_genral[corpus_genral],\n",
    "                                              num_features=len(dictionary_genral))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_status.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_status[query_doc_bow]\n",
    "        status = np.max(sims_status[query_doc_tf_idf])\n",
    "        print(status)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_specific.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_specific[query_doc_bow]\n",
    "        specific = np.max(sims_specific[query_doc_tf_idf])\n",
    "        print(specific)\n",
    "        \n",
    "        query_doc_bow = dictionary_encashment.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_encashment[query_doc_bow]\n",
    "        encashment = np.max(sims_encashment[query_doc_tf_idf])\n",
    "        print(encashment)\n",
    "        \n",
    "        query_doc_bow = dictionary_reporting.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_reporting[query_doc_bow]\n",
    "        reporting = np.max(sims_reporting[query_doc_tf_idf])\n",
    "        print(reporting)\n",
    "        \n",
    "        query_doc_bow = dictionary_genral.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_genral[query_doc_bow]\n",
    "        genral = np.max(sims_genral[query_doc_tf_idf])\n",
    "        print(genral)\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "        if((status>specific)&(status>encashment)&(status>reporting)&(status>genral)):\n",
    "            return \"Leave_status\"\n",
    "        elif((specific>status)&(specific>encashment)&(specific>reporting)&(specific>genral)):\n",
    "            subintent = leave_inquiry_specific(name)\n",
    "            return subintent\n",
    "            #here specific work will come\n",
    "        elif((encashment>status)&(specific<encashment)&(encashment>reporting)&(encashment>genral)):\n",
    "            return \"Leave_encashment\"\n",
    "        elif((reporting>status)&(reporting>specific)&(encashment<reporting)&(reporting>genral)):\n",
    "            return \"Leave_reporting\"\n",
    "        else:\n",
    "            return \"Genral_holiday\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'can you please inform me about how many leaves can i take'\n",
    "name = 'do i have sick leaves'\n",
    "name = 'how many sick leaves have left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8260476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9849832\n",
      "leave_taken\n"
     ]
    }
   ],
   "source": [
    "print(leave_inquiry_specific(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_inquiry_specific(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['sub_type_two']== 'specific']\n",
    "        text = train['Data']\n",
    "        \n",
    "        leave_update = train[train['TypeLeave'] == 'leave_update']\n",
    "        leave_taken = train[train['TypeLeave'] == 'leave_taken']\n",
    "        \n",
    "       \n",
    "        gen_leave_update = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_update['Data']]\n",
    "        gen_leave_taken = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in leave_taken['Data']]\n",
    "\n",
    "        \n",
    "        dictionary_leave_update = gensim.corpora.Dictionary(gen_leave_update)\n",
    "        dictionary_leave_taken = gensim.corpora.Dictionary(gen_leave_taken)\n",
    "       \n",
    "\n",
    "        \n",
    "        corpus_leave_update = [dictionary_leave_update.doc2bow(gen_leave_update) for gen_leave_update in gen_leave_update]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave_update  = gensim.models.TfidfModel(corpus_leave_update)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave_update = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave_update.txt',tf_idf_leave_update[corpus_leave_update],\n",
    "                                              num_features=len(dictionary_leave_update))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_leave_taken = [dictionary_leave_taken.doc2bow(gen_leave_taken) for gen_leave_taken in gen_leave_taken]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave_taken  = gensim.models.TfidfModel(corpus_leave_taken)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave_taken = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave_taken.txt',tf_idf_leave_taken[corpus_leave_taken],\n",
    "                                              num_features=len(dictionary_leave_taken))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_leave_update.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave_update[query_doc_bow]\n",
    "        leave_update = np.max(sims_leave_update[query_doc_tf_idf])\n",
    "        print(leave_update)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_leave_taken.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave_taken[query_doc_bow]\n",
    "        leave_taken = np.max(sims_leave_taken[query_doc_tf_idf])\n",
    "        print(leave_taken)\n",
    "        \n",
    "\n",
    "       \n",
    "        if(leave_update > leave_taken):\n",
    "            sub_type = \"leave_update\"\n",
    "        else:\n",
    "            sub_type = \"leave_taken\"\n",
    "  \n",
    "        return(sub_type)\n",
    "        \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotional_leave(name):\n",
    "        query = name\n",
    "        train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        sick = train[train[\"sub_type_two\"] == 'sick']\n",
    "        casual = train[train[\"sub_type_two\"] == 'casual']\n",
    "\n",
    "\n",
    "        #raw_documents = train['Leave Data Description']\n",
    "        #print(\"Number of sick documents:\",len(sick))\n",
    "        #print(\"Number of casual documents:\",len(casual))\n",
    "        gen_docs_s = [[w.lower() for w in word_tokenize(text)] \n",
    "                for text in sick['Data']]\n",
    "        gen_docs_c = [[w.lower() for w in word_tokenize(text)] \n",
    "                for text in casual['Data']]\n",
    "        dictionary_s = gensim.corpora.Dictionary(gen_docs_s)\n",
    "        dictionary_c = gensim.corpora.Dictionary(gen_docs_c)\n",
    "        corpus_s = [dictionary_s.doc2bow(gen_doc_s) for gen_doc_s in gen_docs_s]\n",
    "        #print(corpus_l)\n",
    "\n",
    "        corpus_c = [dictionary_c.doc2bow(gen_doc_c) for gen_doc_c in gen_docs_c]\n",
    "        #print(corpus_i)\n",
    "        \n",
    "        tf_idf_s = gensim.models.TfidfModel(corpus_s)\n",
    "        tf_idf_c = gensim.models.TfidfModel(corpus_c)\n",
    "        sims_s = gensim.similarities.Similarity('D:\\\\gensim\\\\sick.txt',tf_idf_s[corpus_s],\n",
    "                                          num_features=len(dictionary_s))\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #print(query_doc)\n",
    "        query_doc_bow = dictionary_s.doc2bow(query_doc)\n",
    "        #print(query_doc_bow)\n",
    "        query_doc_tf_idf = tf_idf_s[query_doc_bow]\n",
    "        sick = np.max(sims_s[query_doc_tf_idf])\n",
    "        print(\"sick:\",np.max(sims_s[query_doc_tf_idf]))\n",
    "        sims_c = gensim.similarities.Similarity('D:\\gensim\\\\casual.txt',tf_idf_c[corpus_c],\n",
    "                                          num_features=len(dictionary_c))\n",
    "        query_doc_bow = dictionary_c.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_c[query_doc_bow]\n",
    "        casual = np.max(sims_c[query_doc_tf_idf])\n",
    "        print(\"Casual:\",np.max(sims_c[query_doc_tf_idf]))\n",
    "        if(casual>sick):\n",
    "            return('CasualLeave')\n",
    "        else:\n",
    "            return('Indirect_SickLeave')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_approval(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"leave_final.xlsx\")\n",
    "        train = train[train['Type'] == 'Leave_Approval']\n",
    "        text = train['Data']\n",
    "        approve = train[train['sub_type_two'] == 'approve']\n",
    "        reject = train[train['sub_type_two'] == 'reject']\n",
    "       \n",
    "        gen_approve = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in approve['Data']]\n",
    "        gen_reject = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in reject['Data']]\n",
    "\n",
    "        \n",
    "        dictionary_approve = gensim.corpora.Dictionary(gen_approve)\n",
    "        dictionary_reject = gensim.corpora.Dictionary(gen_reject)\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_approve = [dictionary_approve.doc2bow(gen_approve) for gen_approve in gen_approve]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_approve  = gensim.models.TfidfModel(corpus_approve)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_approve = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\approve.txt',tf_idf_approve[corpus_approve],\n",
    "                                              num_features=len(dictionary_approve))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_reject = [dictionary_reject.doc2bow(gen_reject) for gen_reject in gen_reject]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_reject  = gensim.models.TfidfModel(corpus_reject)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_reject = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\reject.txt',tf_idf_reject[corpus_reject],\n",
    "                                              num_features=len(dictionary_reject))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_approve.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_approve[query_doc_bow]\n",
    "        approve = np.max(sims_approve[query_doc_tf_idf])\n",
    "        print(approve)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_reject.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_reject[query_doc_bow]\n",
    "        reject = np.max(sims_reject[query_doc_tf_idf])\n",
    "        print(reject)\n",
    "\n",
    "       \n",
    "        if(approve > reject):\n",
    "            sub_type = sub_type + \"approve\"\n",
    "        else:\n",
    "            sub_type = sub_type + \"reject\"\n",
    "\n",
    "        return(sub_type)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEAVE ENCASHMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave_encashment_policy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sub_type = ''\n",
    "train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "leave_encashment = train[train['TypeLeave'] == 'leave_encashment']\n",
    "leave_encashment_policy = train[train['TypeLeave'] == 'leave_encashment_policy']\n",
    "leave_encashment_procedure = train[train['TypeLeave'] == 'leave_encashment_procedure']\n",
    "    \n",
    "gen_docs_leave_encashment = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_encashment['Data']]\n",
    "gen_docs_leave_encashment_policy = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_encashment_policy['Data']]\n",
    "gen_docs_leave_encashment_procedure = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_encashment_procedure['Data']]\n",
    "    \n",
    "dictionary_leave_encashment  = gensim.corpora.Dictionary(gen_docs_leave_encashment )\n",
    "dictionary_leave_encashment_policy = gensim.corpora.Dictionary(gen_docs_leave_encashment_policy)\n",
    "dictionary_leave_encashment_procedure = gensim.corpora.Dictionary(gen_docs_leave_encashment_procedure)\n",
    "    \n",
    "corpus_leave_encashment = [dictionary_leave_encashment.doc2bow(gen_doc_leave_encashment) for gen_doc_leave_encashment in gen_docs_leave_encashment]\n",
    "corpus_leave_encashment_policy = [dictionary_leave_encashment_policy.doc2bow(gen_doc_leave_encashment_policy) for gen_doc_leave_encashment_policy in gen_docs_leave_encashment_policy]\n",
    "corpus_leave_encashment_procedure = [dictionary_leave_encashment_procedure.doc2bow(gen_doc_leave_encashment_procedure) for gen_doc_leave_encashment_procedure in gen_docs_leave_encashment_procedure]\n",
    "\n",
    "tf_idf_leave_encashment = gensim.models.TfidfModel(corpus_leave_encashment)\n",
    "tf_idf_leave_encashment_policy = gensim.models.TfidfModel(corpus_leave_encashment_policy)\n",
    "tf_idf_leave_encashment_procedure = gensim.models.TfidfModel(corpus_leave_encashment_procedure )\n",
    "    \n",
    "sims_leave_encashment = gensim.similarities.Similarity('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\Gensim_data\\\\leave_encashment.txt',tf_idf_leave_encashment[corpus_leave_encashment],\n",
    "                                num_features=len(dictionary_leave_encashment))\n",
    "sims_leave_encashment_policy = gensim.similarities.Similarity('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\Gensim_data\\\\leave_encashment_policy.txt',tf_idf_leave_encashment_policy[corpus_leave_encashment_policy],\n",
    "                                num_features=len(dictionary_leave_encashment_policy))\n",
    "sims_leave_encashment_procedure = gensim.similarities.Similarity('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\Gensim_data\\\\leave_encashment_procedure.txt',tf_idf_leave_encashment_procedure[corpus_leave_encashment_procedure],\n",
    "                                num_features=len(dictionary_leave_encashment_procedure))\n",
    "    \n",
    "leave_encashment_gensim={1:sims_leave_encashment,2:tf_idf_leave_encashment,3:dictionary_leave_encashment,4:sims_leave_encashment_policy,5:tf_idf_leave_encashment_policy,6:dictionary_leave_encashment_policy,7:sims_leave_encashment_procedure,8:tf_idf_leave_encashment_procedure,9:dictionary_leave_encashment_procedure}\n",
    "\n",
    "name = 'how can i encash my leaves'\n",
    "name = 'inform me how can i encash my leaves'\n",
    "name = 'what is leave encashment'\n",
    "name = 'tell me about leave encashment'\n",
    "leave_encashment_classification(name,leave_encashment_gensim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_encashment_classification(name,leave_encashment_gensim):\n",
    "    query = name\n",
    "    sims_leave_encashment = leave_encashment_gensim[1]\n",
    "    tf_idf_leave_encashment  = leave_encashment_gensim[2]\n",
    "    dictionary_leave_encashment = leave_encashment_gensim[3]\n",
    "    sims_leave_encashment_policy = leave_encashment_gensim[4]\n",
    "    tf_idf_leave_encashment_policy = leave_encashment_gensim[5]\n",
    "    dictionary_leave_encashment_policy = leave_encashment_gensim[6]\n",
    "    sims_leave_encashment_procedure = leave_encashment_gensim[7]\n",
    "    tf_idf_leave_encashment_procedure = leave_encashment_gensim[8]\n",
    "    dictionary_leave_encashment_procedure = leave_encashment_gensim[9]\n",
    "    \n",
    "    query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "    \n",
    "    query_doc_bow = dictionary_leave_encashment.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf_leave_encashment[query_doc_bow]\n",
    "    leave_encashment = np.max(sims_leave_encashment[query_doc_tf_idf])\n",
    "    \n",
    "    query_doc_bow = dictionary_leave_encashment_policy.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf_leave_encashment_policy[query_doc_bow]\n",
    "    leave_encashment_policy = np.max(sims_leave_encashment_policy[query_doc_tf_idf])\n",
    "    \n",
    "    query_doc_bow = dictionary_leave_encashment_procedure.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf_leave_encashment_procedure[query_doc_bow]\n",
    "    leave_encashment_procedure = np.max(sims_leave_encashment_procedure[query_doc_tf_idf])\n",
    "    \n",
    "    if((leave_encashment > leave_encashment_policy)&(leave_encashment > leave_encashment_procedure)):\n",
    "        print('leave_encashment')\n",
    "    elif((leave_encashment_policy>leave_encashment)&(leave_encashment_policy>leave_encashment_procedure)):\n",
    "        print('leave_encashment_policy')\n",
    "    else:\n",
    "        print('leave_encashment_procedure')\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave Encashment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\datasets\\\\leave_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['sub_type_two'] == 'encashment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['TypeLeave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 262 samples, validate on 30 samples\n",
      "Epoch 1/50\n",
      "262/262 [==============================] - ETA: 9s - loss: 0.6370 - acc: 0.666 - ETA: 4s - loss: 0.6350 - acc: 0.666 - ETA: 2s - loss: 0.6323 - acc: 0.666 - ETA: 1s - loss: 0.6295 - acc: 0.666 - ETA: 1s - loss: 0.6265 - acc: 0.666 - ETA: 0s - loss: 0.6214 - acc: 0.666 - ETA: 0s - loss: 0.6175 - acc: 0.666 - ETA: 0s - loss: 0.6108 - acc: 0.666 - 3s 10ms/step - loss: 0.6086 - acc: 0.6667 - val_loss: 0.6766 - val_acc: 0.6667\n",
      "Epoch 2/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.4975 - acc: 0.791 - ETA: 0s - loss: 0.4788 - acc: 0.822 - ETA: 0s - loss: 0.4807 - acc: 0.805 - ETA: 0s - loss: 0.4581 - acc: 0.825 - ETA: 0s - loss: 0.4272 - acc: 0.839 - ETA: 0s - loss: 0.4181 - acc: 0.842 - ETA: 0s - loss: 0.4511 - acc: 0.815 - ETA: 0s - loss: 0.4707 - acc: 0.791 - 1s 5ms/step - loss: 0.4684 - acc: 0.7901 - val_loss: 0.6674 - val_acc: 0.6667\n",
      "Epoch 3/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.3242 - acc: 0.854 - ETA: 0s - loss: 0.3412 - acc: 0.833 - ETA: 0s - loss: 0.3361 - acc: 0.854 - ETA: 0s - loss: 0.3283 - acc: 0.864 - ETA: 0s - loss: 0.3301 - acc: 0.860 - ETA: 0s - loss: 0.3332 - acc: 0.854 - ETA: 0s - loss: 0.3374 - acc: 0.851 - ETA: 0s - loss: 0.3400 - acc: 0.843 - 1s 5ms/step - loss: 0.3406 - acc: 0.8435 - val_loss: 1.0458 - val_acc: 0.3333\n",
      "Epoch 4/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.3677 - acc: 0.833 - ETA: 0s - loss: 0.3524 - acc: 0.833 - ETA: 0s - loss: 0.3325 - acc: 0.843 - ETA: 0s - loss: 0.3141 - acc: 0.856 - ETA: 0s - loss: 0.3339 - acc: 0.835 - ETA: 0s - loss: 0.3180 - acc: 0.843 - ETA: 0s - loss: 0.3080 - acc: 0.854 - ETA: 0s - loss: 0.2995 - acc: 0.862 - 1s 5ms/step - loss: 0.3060 - acc: 0.8575 - val_loss: 0.8669 - val_acc: 0.3333\n",
      "Epoch 5/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.3873 - acc: 0.781 - ETA: 0s - loss: 0.3326 - acc: 0.849 - ETA: 0s - loss: 0.2943 - acc: 0.885 - ETA: 0s - loss: 0.2876 - acc: 0.880 - ETA: 0s - loss: 0.2828 - acc: 0.877 - ETA: 0s - loss: 0.2715 - acc: 0.878 - ETA: 0s - loss: 0.2802 - acc: 0.872 - ETA: 0s - loss: 0.2861 - acc: 0.867 - 1s 5ms/step - loss: 0.2884 - acc: 0.8651 - val_loss: 0.5775 - val_acc: 0.7333\n",
      "Epoch 6/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.3620 - acc: 0.822 - ETA: 0s - loss: 0.3398 - acc: 0.838 - ETA: 0s - loss: 0.2924 - acc: 0.881 - ETA: 0s - loss: 0.2797 - acc: 0.880 - ETA: 0s - loss: 0.2995 - acc: 0.854 - ETA: 0s - loss: 0.2920 - acc: 0.857 - ETA: 0s - loss: 0.2855 - acc: 0.864 - ETA: 0s - loss: 0.2757 - acc: 0.871 - 1s 5ms/step - loss: 0.2712 - acc: 0.8740 - val_loss: 1.1185 - val_acc: 0.3333\n",
      "Epoch 7/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.1907 - acc: 0.895 - ETA: 0s - loss: 0.2414 - acc: 0.875 - ETA: 0s - loss: 0.2667 - acc: 0.854 - ETA: 0s - loss: 0.2625 - acc: 0.864 - ETA: 0s - loss: 0.2673 - acc: 0.854 - ETA: 0s - loss: 0.2622 - acc: 0.859 - ETA: 0s - loss: 0.2543 - acc: 0.867 - ETA: 0s - loss: 0.2465 - acc: 0.868 - 1s 5ms/step - loss: 0.2475 - acc: 0.8664 - val_loss: 0.6655 - val_acc: 0.5333\n",
      "Epoch 8/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.1743 - acc: 0.958 - ETA: 0s - loss: 0.1948 - acc: 0.942 - ETA: 0s - loss: 0.2089 - acc: 0.913 - ETA: 0s - loss: 0.2227 - acc: 0.898 - ETA: 0s - loss: 0.2230 - acc: 0.891 - ETA: 0s - loss: 0.2171 - acc: 0.892 - ETA: 0s - loss: 0.2082 - acc: 0.897 - ETA: 0s - loss: 0.2142 - acc: 0.894 - 1s 5ms/step - loss: 0.2142 - acc: 0.8957 - val_loss: 0.6055 - val_acc: 0.6222\n",
      "Epoch 9/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.1336 - acc: 0.916 - ETA: 0s - loss: 0.1736 - acc: 0.880 - ETA: 0s - loss: 0.1620 - acc: 0.906 - ETA: 0s - loss: 0.1585 - acc: 0.906 - ETA: 0s - loss: 0.1838 - acc: 0.889 - ETA: 0s - loss: 0.1728 - acc: 0.906 - ETA: 0s - loss: 0.1719 - acc: 0.910 - ETA: 0s - loss: 0.1731 - acc: 0.907 - 1s 5ms/step - loss: 0.1744 - acc: 0.9097 - val_loss: 1.1724 - val_acc: 0.5111\n",
      "Epoch 10/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.1918 - acc: 0.916 - ETA: 0s - loss: 0.1690 - acc: 0.906 - ETA: 0s - loss: 0.1496 - acc: 0.927 - ETA: 0s - loss: 0.1469 - acc: 0.934 - ETA: 0s - loss: 0.1609 - acc: 0.922 - ETA: 0s - loss: 0.1541 - acc: 0.935 - ETA: 0s - loss: 0.1507 - acc: 0.943 - ETA: 0s - loss: 0.1435 - acc: 0.947 - 1s 5ms/step - loss: 0.1436 - acc: 0.9466 - val_loss: 0.4858 - val_acc: 0.7556\n",
      "Epoch 11/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.1017 - acc: 0.989 - ETA: 0s - loss: 0.0906 - acc: 0.984 - ETA: 0s - loss: 0.0968 - acc: 0.968 - ETA: 0s - loss: 0.1040 - acc: 0.971 - ETA: 0s - loss: 0.0948 - acc: 0.977 - ETA: 0s - loss: 0.0884 - acc: 0.980 - ETA: 0s - loss: 0.0906 - acc: 0.980 - ETA: 0s - loss: 0.0953 - acc: 0.977 - 1s 5ms/step - loss: 0.1001 - acc: 0.9758 - val_loss: 0.3994 - val_acc: 0.8778\n",
      "Epoch 12/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.2680 - acc: 0.927 - ETA: 0s - loss: 0.1471 - acc: 0.963 - ETA: 0s - loss: 0.1268 - acc: 0.961 - ETA: 0s - loss: 0.1190 - acc: 0.963 - ETA: 0s - loss: 0.1201 - acc: 0.960 - ETA: 0s - loss: 0.1235 - acc: 0.965 - ETA: 0s - loss: 0.1411 - acc: 0.955 - ETA: 0s - loss: 0.1342 - acc: 0.958 - 1s 5ms/step - loss: 0.1313 - acc: 0.9593 - val_loss: 0.4002 - val_acc: 0.8222\n",
      "Epoch 13/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0571 - acc: 0.989 - ETA: 0s - loss: 0.0597 - acc: 0.994 - ETA: 0s - loss: 0.0628 - acc: 0.989 - ETA: 0s - loss: 0.0692 - acc: 0.987 - ETA: 0s - loss: 0.0679 - acc: 0.989 - ETA: 0s - loss: 0.0654 - acc: 0.989 - ETA: 0s - loss: 0.0644 - acc: 0.991 - ETA: 0s - loss: 0.0625 - acc: 0.990 - 1s 5ms/step - loss: 0.0615 - acc: 0.9911 - val_loss: 0.4741 - val_acc: 0.7333\n",
      "Epoch 14/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0367 - acc: 1.000 - ETA: 0s - loss: 0.0430 - acc: 1.000 - ETA: 0s - loss: 0.0593 - acc: 0.993 - ETA: 0s - loss: 0.0533 - acc: 0.994 - ETA: 0s - loss: 0.0519 - acc: 0.995 - ETA: 0s - loss: 0.0475 - acc: 0.996 - ETA: 0s - loss: 0.0501 - acc: 0.995 - ETA: 0s - loss: 0.0462 - acc: 0.996 - 1s 5ms/step - loss: 0.0479 - acc: 0.9936 - val_loss: 0.3783 - val_acc: 0.7778\n",
      "Epoch 15/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0471 - acc: 1.000 - ETA: 0s - loss: 0.0385 - acc: 1.000 - ETA: 0s - loss: 0.0356 - acc: 1.000 - ETA: 0s - loss: 0.0509 - acc: 0.994 - ETA: 0s - loss: 0.0471 - acc: 0.995 - ETA: 0s - loss: 0.0432 - acc: 0.996 - ETA: 0s - loss: 0.0422 - acc: 0.994 - ETA: 0s - loss: 0.0398 - acc: 0.994 - 1s 5ms/step - loss: 0.0391 - acc: 0.9949 - val_loss: 0.4820 - val_acc: 0.7556\n",
      "Epoch 16/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0254 - acc: 1.000 - ETA: 0s - loss: 0.0334 - acc: 1.000 - ETA: 0s - loss: 0.0341 - acc: 1.000 - ETA: 0s - loss: 0.0471 - acc: 0.994 - ETA: 0s - loss: 0.0425 - acc: 0.995 - ETA: 0s - loss: 0.0382 - acc: 0.996 - ETA: 0s - loss: 0.0346 - acc: 0.997 - ETA: 0s - loss: 0.0365 - acc: 0.992 - 1s 5ms/step - loss: 0.0360 - acc: 0.9924 - val_loss: 0.3363 - val_acc: 0.9111\n",
      "Epoch 17/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0141 - acc: 1.000 - ETA: 0s - loss: 0.0219 - acc: 1.000 - ETA: 0s - loss: 0.0235 - acc: 1.000 - ETA: 0s - loss: 0.0242 - acc: 1.000 - ETA: 0s - loss: 0.0274 - acc: 0.995 - ETA: 0s - loss: 0.0381 - acc: 0.989 - ETA: 0s - loss: 0.0342 - acc: 0.991 - ETA: 0s - loss: 0.0335 - acc: 0.992 - 1s 5ms/step - loss: 0.0349 - acc: 0.9898 - val_loss: 0.2477 - val_acc: 0.9111\n",
      "Epoch 18/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0854 - acc: 0.958 - ETA: 0s - loss: 0.0477 - acc: 0.979 - ETA: 0s - loss: 0.0383 - acc: 0.986 - ETA: 0s - loss: 0.0341 - acc: 0.989 - ETA: 0s - loss: 0.0328 - acc: 0.991 - ETA: 0s - loss: 0.0288 - acc: 0.993 - ETA: 0s - loss: 0.0268 - acc: 0.994 - ETA: 0s - loss: 0.0335 - acc: 0.992 - 1s 5ms/step - loss: 0.0330 - acc: 0.9924 - val_loss: 0.4840 - val_acc: 0.7556\n",
      "Epoch 19/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0184 - acc: 1.000 - ETA: 0s - loss: 0.0153 - acc: 1.000 - ETA: 0s - loss: 0.0129 - acc: 1.000 - ETA: 0s - loss: 0.0156 - acc: 1.000 - ETA: 0s - loss: 0.0148 - acc: 1.000 - ETA: 0s - loss: 0.0135 - acc: 1.000 - ETA: 0s - loss: 0.0224 - acc: 0.997 - ETA: 0s - loss: 0.0215 - acc: 0.997 - 1s 5ms/step - loss: 0.0222 - acc: 0.9975 - val_loss: 0.4570 - val_acc: 0.7556\n",
      "Epoch 20/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0292 - acc: 1.000 - ETA: 0s - loss: 0.0187 - acc: 1.000 - ETA: 0s - loss: 0.0139 - acc: 1.000 - ETA: 0s - loss: 0.0325 - acc: 0.994 - ETA: 0s - loss: 0.0293 - acc: 0.995 - ETA: 0s - loss: 0.0251 - acc: 0.996 - ETA: 0s - loss: 0.0224 - acc: 0.997 - ETA: 0s - loss: 0.0207 - acc: 0.997 - 1s 5ms/step - loss: 0.0202 - acc: 0.9975 - val_loss: 0.3530 - val_acc: 0.8889\n",
      "Epoch 21/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0192 - acc: 1.000 - ETA: 0s - loss: 0.0555 - acc: 0.989 - ETA: 0s - loss: 0.0394 - acc: 0.993 - ETA: 0s - loss: 0.0318 - acc: 0.994 - ETA: 0s - loss: 0.0285 - acc: 0.995 - ETA: 0s - loss: 0.0250 - acc: 0.996 - ETA: 0s - loss: 0.0233 - acc: 0.997 - ETA: 0s - loss: 0.0209 - acc: 0.997 - 1s 5ms/step - loss: 0.0205 - acc: 0.9975 - val_loss: 0.3730 - val_acc: 0.8556\n",
      "Epoch 22/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0092 - acc: 1.000 - ETA: 0s - loss: 0.0059 - acc: 1.000 - ETA: 0s - loss: 0.0043 - acc: 1.000 - ETA: 0s - loss: 0.0261 - acc: 0.994 - ETA: 0s - loss: 0.0284 - acc: 0.991 - ETA: 0s - loss: 0.0261 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.994 - 1s 5ms/step - loss: 0.0207 - acc: 0.9949 - val_loss: 0.4018 - val_acc: 0.8556\n",
      "Epoch 23/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0844 - acc: 0.979 - ETA: 0s - loss: 0.0489 - acc: 0.989 - ETA: 0s - loss: 0.0362 - acc: 0.993 - ETA: 0s - loss: 0.0274 - acc: 0.994 - ETA: 0s - loss: 0.0227 - acc: 0.995 - ETA: 0s - loss: 0.0192 - acc: 0.996 - ETA: 0s - loss: 0.0170 - acc: 0.997 - ETA: 0s - loss: 0.0156 - acc: 0.997 - 1s 5ms/step - loss: 0.0152 - acc: 0.9975 - val_loss: 0.2586 - val_acc: 0.9556\n",
      "Epoch 24/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0044 - acc: 1.000 - ETA: 0s - loss: 0.0403 - acc: 0.989 - ETA: 0s - loss: 0.0469 - acc: 0.979 - ETA: 0s - loss: 0.0373 - acc: 0.984 - ETA: 0s - loss: 0.0308 - acc: 0.987 - ETA: 0s - loss: 0.0274 - acc: 0.989 - ETA: 0s - loss: 0.0246 - acc: 0.991 - ETA: 0s - loss: 0.0225 - acc: 0.992 - 1s 5ms/step - loss: 0.0220 - acc: 0.9924 - val_loss: 0.3171 - val_acc: 0.9556\n",
      "Epoch 25/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 0s - loss: 0.0026 - acc: 1.000 - ETA: 0s - loss: 0.0042 - acc: 1.000 - ETA: 0s - loss: 0.0046 - acc: 1.000 - ETA: 0s - loss: 0.0206 - acc: 0.995 - ETA: 0s - loss: 0.0188 - acc: 0.996 - ETA: 0s - loss: 0.0166 - acc: 0.997 - ETA: 0s - loss: 0.0148 - acc: 0.997 - 1s 5ms/step - loss: 0.0145 - acc: 0.9975 - val_loss: 0.3141 - val_acc: 0.9111\n",
      "Epoch 26/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0061 - acc: 1.000 - ETA: 0s - loss: 0.0079 - acc: 1.000 - ETA: 0s - loss: 0.0060 - acc: 1.000 - ETA: 0s - loss: 0.0056 - acc: 1.000 - ETA: 0s - loss: 0.0046 - acc: 1.000 - ETA: 0s - loss: 0.0040 - acc: 1.000 - ETA: 0s - loss: 0.0035 - acc: 1.000 - ETA: 0s - loss: 0.0034 - acc: 1.000 - 1s 5ms/step - loss: 0.0124 - acc: 0.9975 - val_loss: 0.6019 - val_acc: 0.8889\n",
      "Epoch 27/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0208 - acc: 1.000 - ETA: 0s - loss: 0.0292 - acc: 0.989 - ETA: 0s - loss: 0.0205 - acc: 0.993 - ETA: 0s - loss: 0.0163 - acc: 0.994 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0126 - acc: 0.996 - ETA: 0s - loss: 0.0176 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.994 - 1s 5ms/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.3843 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0114 - acc: 1.000 - ETA: 0s - loss: 0.0076 - acc: 1.000 - ETA: 0s - loss: 0.0059 - acc: 1.000 - ETA: 0s - loss: 0.0047 - acc: 1.000 - ETA: 0s - loss: 0.0045 - acc: 1.000 - ETA: 0s - loss: 0.0057 - acc: 1.000 - ETA: 0s - loss: 0.0055 - acc: 1.000 - ETA: 0s - loss: 0.0108 - acc: 0.997 - 1s 5ms/step - loss: 0.0106 - acc: 0.9975 - val_loss: 0.3836 - val_acc: 0.8778\n",
      "Epoch 29/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0035 - acc: 1.000 - ETA: 0s - loss: 0.0018 - acc: 1.000 - ETA: 0s - loss: 0.0165 - acc: 0.993 - ETA: 0s - loss: 0.0126 - acc: 0.994 - ETA: 0s - loss: 0.0134 - acc: 0.993 - ETA: 0s - loss: 0.0119 - acc: 0.994 - ETA: 0s - loss: 0.0107 - acc: 0.995 - ETA: 0s - loss: 0.0105 - acc: 0.996 - 1s 6ms/step - loss: 0.0104 - acc: 0.9962 - val_loss: 0.4038 - val_acc: 0.8778\n",
      "Epoch 30/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0010 - acc: 1.000 - ETA: 0s - loss: 0.0168 - acc: 0.993 - ETA: 0s - loss: 0.0162 - acc: 0.994 - ETA: 0s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - 1s 5ms/step - loss: 0.0092 - acc: 0.9975 - val_loss: 0.3906 - val_acc: 0.8889\n",
      "Epoch 31/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 6.7457e-04 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.0000    - ETA: 0s - loss: 0.0188 - acc: 0.993 - ETA: 0s - loss: 0.0145 - acc: 0.994 - ETA: 0s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0129 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - 1s 5ms/step - loss: 0.0104 - acc: 0.9975 - val_loss: 0.4030 - val_acc: 0.8889\n",
      "Epoch 32/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0297 - acc: 0.989 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0155 - acc: 0.994 - ETA: 0s - loss: 0.0129 - acc: 0.995 - ETA: 0s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - 1s 5ms/step - loss: 0.0098 - acc: 0.9975 - val_loss: 0.3126 - val_acc: 0.8667\n",
      "Epoch 33/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0027 - acc: 1.000 - ETA: 0s - loss: 0.0373 - acc: 0.989 - ETA: 0s - loss: 0.0261 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0127 - acc: 0.997 - ETA: 0s - loss: 0.0112 - acc: 0.997 - 1s 5ms/step - loss: 0.0110 - acc: 0.9975 - val_loss: 0.4271 - val_acc: 0.8889\n",
      "Epoch 34/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0171 - acc: 0.994 - ETA: 0s - loss: 0.0140 - acc: 0.995 - ETA: 0s - loss: 0.0119 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - 1s 5ms/step - loss: 0.0097 - acc: 0.9975 - val_loss: 0.5034 - val_acc: 0.8889\n",
      "Epoch 35/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0028 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0019 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0074 - acc: 0.997 - 1s 5ms/step - loss: 0.0072 - acc: 0.9975 - val_loss: 0.5027 - val_acc: 0.8889\n",
      "Epoch 36/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0099 - acc: 0.995 - ETA: 0s - loss: 0.0120 - acc: 0.994 - ETA: 0s - loss: 0.0104 - acc: 0.995 - ETA: 0s - loss: 0.0094 - acc: 0.996 - 1s 6ms/step - loss: 0.0092 - acc: 0.9962 - val_loss: 0.6478 - val_acc: 0.8667\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/262 [==============================] - ETA: 1s - loss: 1.7710e-04 - acc: 1.000 - ETA: 0s - loss: 4.7963e-04 - acc: 1.000 - ETA: 0s - loss: 0.0151 - acc: 0.9931    - ETA: 0s - loss: 0.0121 - acc: 0.994 - ETA: 0s - loss: 0.0117 - acc: 0.995 - ETA: 0s - loss: 0.0099 - acc: 0.996 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0076 - acc: 0.997 - 1s 6ms/step - loss: 0.0075 - acc: 0.9975 - val_loss: 0.5579 - val_acc: 0.8667\n",
      "Epoch 38/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0038 - acc: 1.000 - ETA: 1s - loss: 0.0023 - acc: 1.000 - ETA: 0s - loss: 0.0196 - acc: 0.993 - ETA: 0s - loss: 0.0155 - acc: 0.994 - ETA: 0s - loss: 0.0124 - acc: 0.995 - ETA: 0s - loss: 0.0104 - acc: 0.996 - ETA: 0s - loss: 0.0092 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - 1s 5ms/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.7064 - val_acc: 0.8444\n",
      "Epoch 39/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0011 - acc: 1.000 - ETA: 0s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 9.4126e-04 - acc: 1.000 - ETA: 0s - loss: 0.0098 - acc: 0.9948    - ETA: 0s - loss: 0.0079 - acc: 0.995 - ETA: 0s - loss: 0.0069 - acc: 0.996 - ETA: 0s - loss: 0.0101 - acc: 0.995 - ETA: 0s - loss: 0.0089 - acc: 0.996 - 1s 5ms/step - loss: 0.0087 - acc: 0.9962 - val_loss: 0.6069 - val_acc: 0.8444\n",
      "Epoch 40/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 7.6302e-04 - acc: 1.000 - ETA: 0s - loss: 5.2338e-04 - acc: 1.000 - ETA: 0s - loss: 9.3109e-04 - acc: 1.000 - ETA: 0s - loss: 0.0018 - acc: 1.0000    - ETA: 0s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 0.0012 - acc: 1.000 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - 1s 5ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.7097 - val_acc: 0.8444\n",
      "Epoch 41/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 0.0139 - acc: 0.989 - ETA: 0s - loss: 0.0074 - acc: 0.994 - ETA: 0s - loss: 0.0052 - acc: 0.996 - ETA: 0s - loss: 0.0039 - acc: 0.997 - ETA: 0s - loss: 0.0117 - acc: 0.993 - ETA: 0s - loss: 0.0099 - acc: 0.994 - ETA: 0s - loss: 0.0085 - acc: 0.995 - ETA: 0s - loss: 0.0075 - acc: 0.996 - 1s 5ms/step - loss: 0.0073 - acc: 0.9962 - val_loss: 0.7228 - val_acc: 0.8444\n",
      "Epoch 42/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 1.0720e-04 - acc: 1.000 - ETA: 0s - loss: 4.5937e-04 - acc: 1.000 - ETA: 0s - loss: 4.0689e-04 - acc: 1.000 - ETA: 0s - loss: 3.5060e-04 - acc: 1.000 - ETA: 0s - loss: 5.7874e-04 - acc: 1.000 - ETA: 0s - loss: 6.6633e-04 - acc: 1.000 - ETA: 0s - loss: 0.0063 - acc: 0.9970    - ETA: 0s - loss: 0.0055 - acc: 0.997 - 1s 5ms/step - loss: 0.0054 - acc: 0.9975 - val_loss: 0.7121 - val_acc: 0.8444\n",
      "Epoch 43/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0010 - acc: 1.000 - ETA: 0s - loss: 7.2922e-04 - acc: 1.000 - ETA: 0s - loss: 0.0099 - acc: 0.9931    - ETA: 0s - loss: 0.0268 - acc: 0.989 - ETA: 0s - loss: 0.0215 - acc: 0.991 - ETA: 0s - loss: 0.0184 - acc: 0.993 - ETA: 0s - loss: 0.0158 - acc: 0.994 - ETA: 0s - loss: 0.0140 - acc: 0.994 - 1s 5ms/step - loss: 0.0136 - acc: 0.9949 - val_loss: 0.7478 - val_acc: 0.8444\n",
      "Epoch 44/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 2.8936e-04 - acc: 1.000 - ETA: 0s - loss: 5.7873e-04 - acc: 1.000 - ETA: 0s - loss: 0.0096 - acc: 0.9931    - ETA: 0s - loss: 0.0076 - acc: 0.994 - ETA: 0s - loss: 0.0130 - acc: 0.991 - ETA: 0s - loss: 0.0109 - acc: 0.993 - ETA: 0s - loss: 0.0094 - acc: 0.994 - ETA: 0s - loss: 0.0082 - acc: 0.994 - 1s 5ms/step - loss: 0.0080 - acc: 0.9949 - val_loss: 0.7709 - val_acc: 0.8667\n",
      "Epoch 45/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 2.8924e-04 - acc: 1.000 - ETA: 0s - loss: 1.4534e-04 - acc: 1.000 - ETA: 0s - loss: 1.1322e-04 - acc: 1.000 - ETA: 0s - loss: 0.0111 - acc: 0.9948    - ETA: 0s - loss: 0.0089 - acc: 0.995 - ETA: 0s - loss: 0.0076 - acc: 0.996 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - 1s 5ms/step - loss: 0.0058 - acc: 0.9975 - val_loss: 0.8488 - val_acc: 0.8667\n",
      "Epoch 46/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 1.9447e-04 - acc: 1.000 - ETA: 0s - loss: 1.4234e-04 - acc: 1.000 - ETA: 0s - loss: 0.0087 - acc: 0.9931    - ETA: 0s - loss: 0.0065 - acc: 0.994 - ETA: 0s - loss: 0.0104 - acc: 0.991 - ETA: 0s - loss: 0.0114 - acc: 0.989 - ETA: 0s - loss: 0.0100 - acc: 0.991 - ETA: 0s - loss: 0.0089 - acc: 0.992 - 1s 5ms/step - loss: 0.0087 - acc: 0.9924 - val_loss: 0.6412 - val_acc: 0.8444\n",
      "Epoch 47/50\n",
      "262/262 [==============================] - ETA: 0s - loss: 0.0046 - acc: 1.000 - ETA: 0s - loss: 0.0028 - acc: 1.000 - ETA: 0s - loss: 0.0020 - acc: 1.000 - ETA: 0s - loss: 0.0018 - acc: 1.000 - ETA: 0s - loss: 0.0067 - acc: 0.995 - ETA: 0s - loss: 0.0058 - acc: 0.996 - ETA: 0s - loss: 0.0074 - acc: 0.995 - ETA: 0s - loss: 0.0068 - acc: 0.996 - 1s 5ms/step - loss: 0.0067 - acc: 0.9962 - val_loss: 0.7125 - val_acc: 0.8667\n",
      "Epoch 48/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 3.2542e-04 - acc: 1.000 - ETA: 0s - loss: 3.4835e-04 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.0000    - ETA: 0s - loss: 0.0024 - acc: 1.000 - ETA: 0s - loss: 0.0020 - acc: 1.000 - ETA: 0s - loss: 0.0016 - acc: 1.000 - ETA: 0s - loss: 0.0064 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - 1s 5ms/step - loss: 0.0058 - acc: 0.9975 - val_loss: 0.7378 - val_acc: 0.8444\n",
      "Epoch 49/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 1.6105e-05 - acc: 1.000 - ETA: 0s - loss: 0.0057 - acc: 1.0000    - ETA: 0s - loss: 0.0040 - acc: 1.000 - ETA: 0s - loss: 0.0030 - acc: 1.000 - ETA: 0s - loss: 0.0091 - acc: 0.995 - ETA: 0s - loss: 0.0076 - acc: 0.996 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - 1s 5ms/step - loss: 0.0058 - acc: 0.9975 - val_loss: 0.7664 - val_acc: 0.8444\n",
      "Epoch 50/50\n",
      "262/262 [==============================] - ETA: 1s - loss: 5.2667e-04 - acc: 1.000 - ETA: 1s - loss: 0.0106 - acc: 0.9896    - ETA: 0s - loss: 0.0071 - acc: 0.993 - ETA: 0s - loss: 0.0061 - acc: 0.994 - ETA: 0s - loss: 0.0049 - acc: 0.995 - ETA: 0s - loss: 0.0092 - acc: 0.993 - ETA: 0s - loss: 0.0079 - acc: 0.994 - ETA: 0s - loss: 0.0069 - acc: 0.994 - 1s 5ms/step - loss: 0.0068 - acc: 0.9949 - val_loss: 0.7894 - val_acc: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18f28f6bfd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.save('Encashment_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\datasets\\\\leave_final.xlsx')\n",
    "train = train[train['sub_type_two'] == 'encashment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m = load_model('Encashment_Model.h5')\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['TypeLeave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How many Leave allowance will I get?'\n",
    "query = 'Do i have any leaves that can be encashed'\n",
    "query = 'How much i will get if i cash my leaves'\n",
    "query = 'can i encash my sick leaves '\n",
    "query = 'can my sick leaves be encashed '\n",
    "query = 'can i encash my hajj leaves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is leave encashment policy'\n",
    "query = 'tell me about my leave encashment '\n",
    "query = 'what is leave encashment policy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'can you tell me the procedure of leave encashment'\n",
    "query = 'tell me about encashment of leaves '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tell', 'me', 'about', 'encash', 'of', 'leav']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = token_stems(sent)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test = ([list(sen)])\n",
    "sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.7503523e-03 9.9643707e-01 8.1264909e-04]]\n",
      "0.9964371\n"
     ]
    }
   ],
   "source": [
    "score = m.predict(sen_sequences_matrix)\n",
    "print(score)\n",
    "print(max(score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave allowance 0\n",
      "policy 1\n",
      "procedure 2\n"
     ]
    }
   ],
   "source": [
    "print('leave allowance 0')\n",
    "print('policy 1')\n",
    "print('procedure 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
