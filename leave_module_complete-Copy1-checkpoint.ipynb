{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import tensorflow as tf\n",
    "    import json\n",
    "    import xlrd\n",
    "    import traceback   \n",
    "    import numpy as np #use to handle numeric data\n",
    "    import nltk #for nlp purpose\n",
    "    import pandas as pd #use for file that we read\n",
    "    import re #to handle regular expression\n",
    "    from keras.models import load_model #To load model\n",
    "    from keras import backend as K #to load the backend library that we are using \n",
    "    from tensorflow.keras.preprocessing import sequence\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from textblob import TextBlob\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from langdetect import detect\n",
    "    import string \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import h5py\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    import datefinder\n",
    "    from dateparser.search import search_dates\n",
    "    import spacy\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import gensim\n",
    "    import inflect\n",
    "    p = inflect.engine()\n",
    "    from dateutil.parser import parse\n",
    "        \n",
    "\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('tell')\n",
    "stopwords.append('list')\n",
    "stopwords.append('find')\n",
    "stopwords.append('send')\n",
    "stopwords.append('show')\n",
    "stopwords.append('provide')\n",
    "stopwords.append('bring')\n",
    "stopwords.append('update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first tokenization will be performed then stemming will be performed over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):\n",
    "        \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            #if token not in stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "#train = train[train['Module'] == 'Leave']\n",
    "#train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs[1],\"\\n\\n\",tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    model.add(LSTM(128, input_shape=(xtrain.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1273 samples, validate on 319 samples\n",
      "Epoch 1/50\n",
      "1273/1273 [==============================] - ETA: 1:36 - loss: 0.5621 - acc: 0.750 - ETA: 52s - loss: 0.5587 - acc: 0.750 - ETA: 37s - loss: 0.5558 - acc: 0.75 - ETA: 29s - loss: 0.5546 - acc: 0.75 - ETA: 25s - loss: 0.5544 - acc: 0.75 - ETA: 21s - loss: 0.5550 - acc: 0.75 - ETA: 19s - loss: 0.5554 - acc: 0.75 - ETA: 17s - loss: 0.5546 - acc: 0.75 - ETA: 16s - loss: 0.5514 - acc: 0.75 - ETA: 14s - loss: 0.5471 - acc: 0.75 - ETA: 13s - loss: 0.5483 - acc: 0.75 - ETA: 12s - loss: 0.5497 - acc: 0.75 - ETA: 12s - loss: 0.5496 - acc: 0.75 - ETA: 11s - loss: 0.5490 - acc: 0.75 - ETA: 10s - loss: 0.5486 - acc: 0.75 - ETA: 9s - loss: 0.5487 - acc: 0.7500 - ETA: 9s - loss: 0.5483 - acc: 0.750 - ETA: 8s - loss: 0.5483 - acc: 0.750 - ETA: 8s - loss: 0.5477 - acc: 0.750 - ETA: 7s - loss: 0.5466 - acc: 0.750 - ETA: 7s - loss: 0.5443 - acc: 0.750 - ETA: 6s - loss: 0.5422 - acc: 0.751 - ETA: 6s - loss: 0.5424 - acc: 0.751 - ETA: 5s - loss: 0.5404 - acc: 0.751 - ETA: 5s - loss: 0.5381 - acc: 0.753 - ETA: 5s - loss: 0.5349 - acc: 0.753 - ETA: 4s - loss: 0.5332 - acc: 0.753 - ETA: 4s - loss: 0.5308 - acc: 0.755 - ETA: 3s - loss: 0.5285 - acc: 0.757 - ETA: 3s - loss: 0.5245 - acc: 0.759 - ETA: 3s - loss: 0.5222 - acc: 0.759 - ETA: 2s - loss: 0.5202 - acc: 0.760 - ETA: 2s - loss: 0.5161 - acc: 0.763 - ETA: 1s - loss: 0.5151 - acc: 0.762 - ETA: 1s - loss: 0.5123 - acc: 0.764 - ETA: 1s - loss: 0.5111 - acc: 0.765 - ETA: 0s - loss: 0.5084 - acc: 0.766 - ETA: 0s - loss: 0.5061 - acc: 0.766 - ETA: 0s - loss: 0.5027 - acc: 0.768 - 14s 11ms/step - loss: 0.4999 - acc: 0.7700 - val_loss: 0.0677 - val_acc: 0.9835\n",
      "Epoch 2/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.4115 - acc: 0.79 - ETA: 10s - loss: 0.3487 - acc: 0.83 - ETA: 11s - loss: 0.4444 - acc: 0.81 - ETA: 10s - loss: 0.4491 - acc: 0.80 - ETA: 10s - loss: 0.4309 - acc: 0.81 - ETA: 9s - loss: 0.4213 - acc: 0.8125 - ETA: 9s - loss: 0.4171 - acc: 0.811 - ETA: 9s - loss: 0.4103 - acc: 0.812 - ETA: 9s - loss: 0.4044 - acc: 0.813 - ETA: 8s - loss: 0.4027 - acc: 0.812 - ETA: 8s - loss: 0.3981 - acc: 0.813 - ETA: 8s - loss: 0.3946 - acc: 0.814 - ETA: 8s - loss: 0.3946 - acc: 0.815 - ETA: 7s - loss: 0.3918 - acc: 0.817 - ETA: 7s - loss: 0.3932 - acc: 0.814 - ETA: 7s - loss: 0.3919 - acc: 0.814 - ETA: 6s - loss: 0.3923 - acc: 0.811 - ETA: 6s - loss: 0.3914 - acc: 0.813 - ETA: 6s - loss: 0.3901 - acc: 0.812 - ETA: 5s - loss: 0.3884 - acc: 0.812 - ETA: 5s - loss: 0.3844 - acc: 0.814 - ETA: 5s - loss: 0.3822 - acc: 0.815 - ETA: 4s - loss: 0.3857 - acc: 0.813 - ETA: 4s - loss: 0.3853 - acc: 0.814 - ETA: 4s - loss: 0.3837 - acc: 0.815 - ETA: 3s - loss: 0.3784 - acc: 0.817 - ETA: 3s - loss: 0.3783 - acc: 0.816 - ETA: 3s - loss: 0.3778 - acc: 0.816 - ETA: 3s - loss: 0.3757 - acc: 0.816 - ETA: 2s - loss: 0.3742 - acc: 0.816 - ETA: 2s - loss: 0.3754 - acc: 0.816 - ETA: 2s - loss: 0.3747 - acc: 0.815 - ETA: 1s - loss: 0.3729 - acc: 0.815 - ETA: 1s - loss: 0.3700 - acc: 0.816 - ETA: 1s - loss: 0.3678 - acc: 0.817 - ETA: 1s - loss: 0.3684 - acc: 0.816 - ETA: 0s - loss: 0.3652 - acc: 0.818 - ETA: 0s - loss: 0.3638 - acc: 0.818 - ETA: 0s - loss: 0.3633 - acc: 0.819 - 12s 9ms/step - loss: 0.3617 - acc: 0.8207 - val_loss: 0.0555 - val_acc: 0.9757\n",
      "Epoch 3/50\n",
      "1273/1273 [==============================] - ETA: 10s - loss: 0.2836 - acc: 0.84 - ETA: 11s - loss: 0.3032 - acc: 0.83 - ETA: 11s - loss: 0.2955 - acc: 0.84 - ETA: 11s - loss: 0.2749 - acc: 0.85 - ETA: 11s - loss: 0.2723 - acc: 0.86 - ETA: 10s - loss: 0.2844 - acc: 0.85 - ETA: 10s - loss: 0.2900 - acc: 0.85 - ETA: 10s - loss: 0.2811 - acc: 0.85 - ETA: 9s - loss: 0.2830 - acc: 0.8542 - ETA: 9s - loss: 0.2792 - acc: 0.854 - ETA: 9s - loss: 0.2759 - acc: 0.858 - ETA: 8s - loss: 0.2804 - acc: 0.854 - ETA: 8s - loss: 0.2743 - acc: 0.859 - ETA: 8s - loss: 0.2718 - acc: 0.861 - ETA: 8s - loss: 0.2731 - acc: 0.860 - ETA: 7s - loss: 0.2719 - acc: 0.860 - ETA: 7s - loss: 0.2761 - acc: 0.860 - ETA: 7s - loss: 0.2794 - acc: 0.860 - ETA: 6s - loss: 0.2801 - acc: 0.859 - ETA: 6s - loss: 0.2788 - acc: 0.860 - ETA: 6s - loss: 0.2811 - acc: 0.860 - ETA: 5s - loss: 0.2811 - acc: 0.860 - ETA: 5s - loss: 0.2813 - acc: 0.861 - ETA: 5s - loss: 0.2797 - acc: 0.861 - ETA: 4s - loss: 0.2787 - acc: 0.862 - ETA: 4s - loss: 0.2793 - acc: 0.862 - ETA: 4s - loss: 0.2787 - acc: 0.862 - ETA: 3s - loss: 0.2760 - acc: 0.865 - ETA: 3s - loss: 0.2769 - acc: 0.864 - ETA: 3s - loss: 0.2760 - acc: 0.866 - ETA: 2s - loss: 0.2762 - acc: 0.866 - ETA: 2s - loss: 0.2757 - acc: 0.866 - ETA: 2s - loss: 0.2754 - acc: 0.867 - ETA: 1s - loss: 0.2751 - acc: 0.868 - ETA: 1s - loss: 0.2746 - acc: 0.868 - ETA: 1s - loss: 0.2759 - acc: 0.868 - ETA: 0s - loss: 0.2749 - acc: 0.869 - ETA: 0s - loss: 0.2726 - acc: 0.870 - ETA: 0s - loss: 0.2693 - acc: 0.873 - 14s 11ms/step - loss: 0.2677 - acc: 0.8745 - val_loss: 0.0609 - val_acc: 0.9639\n",
      "Epoch 4/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.1631 - acc: 0.96 - ETA: 13s - loss: 0.1869 - acc: 0.95 - ETA: 13s - loss: 0.1794 - acc: 0.94 - ETA: 12s - loss: 0.1681 - acc: 0.94 - ETA: 12s - loss: 0.1760 - acc: 0.94 - ETA: 12s - loss: 0.1759 - acc: 0.94 - ETA: 11s - loss: 0.1687 - acc: 0.94 - ETA: 11s - loss: 0.1674 - acc: 0.94 - ETA: 10s - loss: 0.1690 - acc: 0.94 - ETA: 10s - loss: 0.1821 - acc: 0.93 - ETA: 9s - loss: 0.1798 - acc: 0.9389 - ETA: 9s - loss: 0.1808 - acc: 0.936 - ETA: 9s - loss: 0.1754 - acc: 0.939 - ETA: 8s - loss: 0.1790 - acc: 0.938 - ETA: 8s - loss: 0.1865 - acc: 0.934 - ETA: 8s - loss: 0.1917 - acc: 0.934 - ETA: 7s - loss: 0.1912 - acc: 0.934 - ETA: 7s - loss: 0.1901 - acc: 0.935 - ETA: 6s - loss: 0.1941 - acc: 0.935 - ETA: 6s - loss: 0.1936 - acc: 0.936 - ETA: 6s - loss: 0.1892 - acc: 0.939 - ETA: 5s - loss: 0.1912 - acc: 0.938 - ETA: 5s - loss: 0.1927 - acc: 0.937 - ETA: 5s - loss: 0.1916 - acc: 0.938 - ETA: 4s - loss: 0.1930 - acc: 0.936 - ETA: 4s - loss: 0.1908 - acc: 0.937 - ETA: 4s - loss: 0.1878 - acc: 0.938 - ETA: 3s - loss: 0.1849 - acc: 0.940 - ETA: 3s - loss: 0.1831 - acc: 0.940 - ETA: 3s - loss: 0.1806 - acc: 0.941 - ETA: 2s - loss: 0.1784 - acc: 0.942 - ETA: 2s - loss: 0.1768 - acc: 0.943 - ETA: 2s - loss: 0.1733 - acc: 0.944 - ETA: 1s - loss: 0.1707 - acc: 0.945 - ETA: 1s - loss: 0.1729 - acc: 0.944 - ETA: 1s - loss: 0.1728 - acc: 0.944 - ETA: 0s - loss: 0.1729 - acc: 0.944 - ETA: 0s - loss: 0.1722 - acc: 0.943 - ETA: 0s - loss: 0.1728 - acc: 0.943 - 14s 11ms/step - loss: 0.1722 - acc: 0.9430 - val_loss: 0.0498 - val_acc: 0.9702\n",
      "Epoch 5/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.1025 - acc: 0.96 - ETA: 12s - loss: 0.1011 - acc: 0.96 - ETA: 11s - loss: 0.0991 - acc: 0.96 - ETA: 11s - loss: 0.1029 - acc: 0.96 - ETA: 11s - loss: 0.0960 - acc: 0.96 - ETA: 10s - loss: 0.0913 - acc: 0.97 - ETA: 10s - loss: 0.0864 - acc: 0.97 - ETA: 10s - loss: 0.0829 - acc: 0.97 - ETA: 9s - loss: 0.0808 - acc: 0.9757 - ETA: 9s - loss: 0.0784 - acc: 0.976 - ETA: 9s - loss: 0.0809 - acc: 0.975 - ETA: 8s - loss: 0.0971 - acc: 0.966 - ETA: 8s - loss: 0.1013 - acc: 0.965 - ETA: 8s - loss: 0.0985 - acc: 0.966 - ETA: 7s - loss: 0.1009 - acc: 0.966 - ETA: 7s - loss: 0.1064 - acc: 0.963 - ETA: 7s - loss: 0.1140 - acc: 0.958 - ETA: 6s - loss: 0.1196 - acc: 0.957 - ETA: 6s - loss: 0.1174 - acc: 0.958 - ETA: 6s - loss: 0.1162 - acc: 0.959 - ETA: 6s - loss: 0.1151 - acc: 0.960 - ETA: 5s - loss: 0.1143 - acc: 0.960 - ETA: 5s - loss: 0.1139 - acc: 0.959 - ETA: 5s - loss: 0.1140 - acc: 0.960 - ETA: 4s - loss: 0.1167 - acc: 0.959 - ETA: 4s - loss: 0.1167 - acc: 0.959 - ETA: 4s - loss: 0.1173 - acc: 0.960 - ETA: 3s - loss: 0.1169 - acc: 0.960 - ETA: 3s - loss: 0.1159 - acc: 0.960 - ETA: 3s - loss: 0.1136 - acc: 0.961 - ETA: 2s - loss: 0.1128 - acc: 0.961 - ETA: 2s - loss: 0.1133 - acc: 0.961 - ETA: 2s - loss: 0.1124 - acc: 0.961 - ETA: 1s - loss: 0.1117 - acc: 0.962 - ETA: 1s - loss: 0.1107 - acc: 0.962 - ETA: 1s - loss: 0.1104 - acc: 0.962 - ETA: 0s - loss: 0.1123 - acc: 0.961 - ETA: 0s - loss: 0.1139 - acc: 0.960 - ETA: 0s - loss: 0.1135 - acc: 0.960 - 14s 11ms/step - loss: 0.1120 - acc: 0.9617 - val_loss: 0.0520 - val_acc: 0.9702\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0761 - acc: 0.98 - ETA: 11s - loss: 0.0560 - acc: 0.98 - ETA: 11s - loss: 0.0503 - acc: 0.98 - ETA: 11s - loss: 0.0647 - acc: 0.98 - ETA: 11s - loss: 0.0680 - acc: 0.97 - ETA: 10s - loss: 0.0687 - acc: 0.97 - ETA: 10s - loss: 0.0667 - acc: 0.97 - ETA: 10s - loss: 0.0810 - acc: 0.97 - ETA: 9s - loss: 0.0816 - acc: 0.9696 - ETA: 9s - loss: 0.0800 - acc: 0.971 - ETA: 9s - loss: 0.0918 - acc: 0.965 - ETA: 8s - loss: 0.0910 - acc: 0.966 - ETA: 8s - loss: 0.0910 - acc: 0.966 - ETA: 8s - loss: 0.0882 - acc: 0.968 - ETA: 7s - loss: 0.0840 - acc: 0.969 - ETA: 7s - loss: 0.0824 - acc: 0.970 - ETA: 7s - loss: 0.0859 - acc: 0.969 - ETA: 6s - loss: 0.0844 - acc: 0.970 - ETA: 6s - loss: 0.0823 - acc: 0.970 - ETA: 6s - loss: 0.0810 - acc: 0.971 - ETA: 6s - loss: 0.0819 - acc: 0.971 - ETA: 5s - loss: 0.0794 - acc: 0.971 - ETA: 5s - loss: 0.0765 - acc: 0.972 - ETA: 5s - loss: 0.0746 - acc: 0.974 - ETA: 4s - loss: 0.0733 - acc: 0.974 - ETA: 4s - loss: 0.0711 - acc: 0.975 - ETA: 4s - loss: 0.0741 - acc: 0.975 - ETA: 3s - loss: 0.0819 - acc: 0.972 - ETA: 3s - loss: 0.0819 - acc: 0.972 - ETA: 3s - loss: 0.0805 - acc: 0.973 - ETA: 2s - loss: 0.0816 - acc: 0.973 - ETA: 2s - loss: 0.0812 - acc: 0.973 - ETA: 2s - loss: 0.0808 - acc: 0.973 - ETA: 1s - loss: 0.0795 - acc: 0.974 - ETA: 1s - loss: 0.0788 - acc: 0.974 - ETA: 1s - loss: 0.0788 - acc: 0.973 - ETA: 0s - loss: 0.0797 - acc: 0.973 - ETA: 0s - loss: 0.0795 - acc: 0.973 - ETA: 0s - loss: 0.0806 - acc: 0.973 - 14s 11ms/step - loss: 0.0796 - acc: 0.9739 - val_loss: 0.0257 - val_acc: 0.9882\n",
      "Epoch 7/50\n",
      "1273/1273 [==============================] - ETA: 16s - loss: 0.1348 - acc: 0.96 - ETA: 13s - loss: 0.1276 - acc: 0.95 - ETA: 12s - loss: 0.1099 - acc: 0.96 - ETA: 12s - loss: 0.0950 - acc: 0.96 - ETA: 12s - loss: 0.0859 - acc: 0.96 - ETA: 11s - loss: 0.0740 - acc: 0.97 - ETA: 11s - loss: 0.0665 - acc: 0.97 - ETA: 11s - loss: 0.0664 - acc: 0.97 - ETA: 10s - loss: 0.0702 - acc: 0.97 - ETA: 10s - loss: 0.0653 - acc: 0.97 - ETA: 9s - loss: 0.0674 - acc: 0.9766 - ETA: 9s - loss: 0.0701 - acc: 0.972 - ETA: 9s - loss: 0.0660 - acc: 0.974 - ETA: 8s - loss: 0.0635 - acc: 0.976 - ETA: 8s - loss: 0.0702 - acc: 0.974 - ETA: 8s - loss: 0.0692 - acc: 0.974 - ETA: 7s - loss: 0.0696 - acc: 0.975 - ETA: 7s - loss: 0.0666 - acc: 0.976 - ETA: 7s - loss: 0.0677 - acc: 0.976 - ETA: 6s - loss: 0.0680 - acc: 0.976 - ETA: 6s - loss: 0.0661 - acc: 0.977 - ETA: 6s - loss: 0.0661 - acc: 0.976 - ETA: 5s - loss: 0.0651 - acc: 0.976 - ETA: 5s - loss: 0.0655 - acc: 0.977 - ETA: 4s - loss: 0.0670 - acc: 0.976 - ETA: 4s - loss: 0.0679 - acc: 0.976 - ETA: 4s - loss: 0.0671 - acc: 0.976 - ETA: 3s - loss: 0.0653 - acc: 0.977 - ETA: 3s - loss: 0.0633 - acc: 0.977 - ETA: 3s - loss: 0.0635 - acc: 0.977 - ETA: 2s - loss: 0.0633 - acc: 0.977 - ETA: 2s - loss: 0.0619 - acc: 0.978 - ETA: 2s - loss: 0.0619 - acc: 0.978 - ETA: 1s - loss: 0.0603 - acc: 0.979 - ETA: 1s - loss: 0.0602 - acc: 0.979 - ETA: 1s - loss: 0.0618 - acc: 0.979 - ETA: 0s - loss: 0.0606 - acc: 0.979 - ETA: 0s - loss: 0.0595 - acc: 0.979 - ETA: 0s - loss: 0.0584 - acc: 0.980 - 14s 11ms/step - loss: 0.0575 - acc: 0.9808 - val_loss: 0.0256 - val_acc: 0.9890\n",
      "Epoch 8/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0713 - acc: 0.96 - ETA: 12s - loss: 0.0498 - acc: 0.97 - ETA: 11s - loss: 0.0491 - acc: 0.97 - ETA: 11s - loss: 0.0386 - acc: 0.98 - ETA: 11s - loss: 0.0347 - acc: 0.98 - ETA: 10s - loss: 0.0436 - acc: 0.98 - ETA: 10s - loss: 0.0386 - acc: 0.98 - ETA: 10s - loss: 0.0361 - acc: 0.98 - ETA: 9s - loss: 0.0338 - acc: 0.9887 - ETA: 9s - loss: 0.0324 - acc: 0.989 - ETA: 9s - loss: 0.0430 - acc: 0.987 - ETA: 8s - loss: 0.0402 - acc: 0.988 - ETA: 8s - loss: 0.0393 - acc: 0.989 - ETA: 8s - loss: 0.0433 - acc: 0.988 - ETA: 7s - loss: 0.0433 - acc: 0.988 - ETA: 7s - loss: 0.0433 - acc: 0.987 - ETA: 7s - loss: 0.0415 - acc: 0.988 - ETA: 6s - loss: 0.0403 - acc: 0.989 - ETA: 6s - loss: 0.0396 - acc: 0.988 - ETA: 6s - loss: 0.0387 - acc: 0.989 - ETA: 6s - loss: 0.0388 - acc: 0.988 - ETA: 5s - loss: 0.0444 - acc: 0.987 - ETA: 5s - loss: 0.0429 - acc: 0.988 - ETA: 5s - loss: 0.0442 - acc: 0.987 - ETA: 4s - loss: 0.0459 - acc: 0.987 - ETA: 4s - loss: 0.0443 - acc: 0.987 - ETA: 4s - loss: 0.0435 - acc: 0.987 - ETA: 3s - loss: 0.0424 - acc: 0.988 - ETA: 3s - loss: 0.0417 - acc: 0.987 - ETA: 3s - loss: 0.0409 - acc: 0.988 - ETA: 2s - loss: 0.0416 - acc: 0.988 - ETA: 2s - loss: 0.0437 - acc: 0.986 - ETA: 2s - loss: 0.0430 - acc: 0.986 - ETA: 1s - loss: 0.0433 - acc: 0.986 - ETA: 1s - loss: 0.0465 - acc: 0.985 - ETA: 1s - loss: 0.0455 - acc: 0.986 - ETA: 0s - loss: 0.0472 - acc: 0.986 - ETA: 0s - loss: 0.0482 - acc: 0.985 - ETA: 0s - loss: 0.0481 - acc: 0.985 - 14s 11ms/step - loss: 0.0478 - acc: 0.9855 - val_loss: 0.0499 - val_acc: 0.9859\n",
      "Epoch 9/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0643 - acc: 0.96 - ETA: 11s - loss: 0.0511 - acc: 0.97 - ETA: 11s - loss: 0.0374 - acc: 0.98 - ETA: 11s - loss: 0.0302 - acc: 0.98 - ETA: 10s - loss: 0.0382 - acc: 0.98 - ETA: 10s - loss: 0.0369 - acc: 0.98 - ETA: 10s - loss: 0.0333 - acc: 0.98 - ETA: 10s - loss: 0.0341 - acc: 0.98 - ETA: 9s - loss: 0.0309 - acc: 0.9896 - ETA: 9s - loss: 0.0284 - acc: 0.990 - ETA: 9s - loss: 0.0400 - acc: 0.987 - ETA: 8s - loss: 0.0390 - acc: 0.987 - ETA: 8s - loss: 0.0374 - acc: 0.988 - ETA: 8s - loss: 0.0352 - acc: 0.989 - ETA: 7s - loss: 0.0330 - acc: 0.990 - ETA: 7s - loss: 0.0313 - acc: 0.990 - ETA: 7s - loss: 0.0304 - acc: 0.991 - ETA: 6s - loss: 0.0310 - acc: 0.990 - ETA: 6s - loss: 0.0318 - acc: 0.990 - ETA: 6s - loss: 0.0310 - acc: 0.991 - ETA: 6s - loss: 0.0318 - acc: 0.990 - ETA: 5s - loss: 0.0365 - acc: 0.989 - ETA: 5s - loss: 0.0356 - acc: 0.989 - ETA: 5s - loss: 0.0405 - acc: 0.988 - ETA: 4s - loss: 0.0393 - acc: 0.989 - ETA: 4s - loss: 0.0380 - acc: 0.989 - ETA: 4s - loss: 0.0370 - acc: 0.990 - ETA: 3s - loss: 0.0363 - acc: 0.990 - ETA: 3s - loss: 0.0360 - acc: 0.990 - ETA: 3s - loss: 0.0351 - acc: 0.990 - ETA: 2s - loss: 0.0376 - acc: 0.989 - ETA: 2s - loss: 0.0427 - acc: 0.988 - ETA: 2s - loss: 0.0422 - acc: 0.988 - ETA: 1s - loss: 0.0416 - acc: 0.989 - ETA: 1s - loss: 0.0410 - acc: 0.988 - ETA: 1s - loss: 0.0402 - acc: 0.989 - ETA: 0s - loss: 0.0392 - acc: 0.989 - ETA: 0s - loss: 0.0389 - acc: 0.989 - ETA: 0s - loss: 0.0383 - acc: 0.989 - 14s 11ms/step - loss: 0.0385 - acc: 0.9892 - val_loss: 0.0173 - val_acc: 0.9914\n",
      "Epoch 10/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.1635 - acc: 0.96 - ETA: 11s - loss: 0.0853 - acc: 0.98 - ETA: 11s - loss: 0.0622 - acc: 0.98 - ETA: 11s - loss: 0.0729 - acc: 0.98 - ETA: 10s - loss: 0.0677 - acc: 0.98 - ETA: 10s - loss: 0.0591 - acc: 0.98 - ETA: 10s - loss: 0.0550 - acc: 0.98 - ETA: 10s - loss: 0.0494 - acc: 0.98 - ETA: 9s - loss: 0.0553 - acc: 0.9844 - ETA: 9s - loss: 0.0506 - acc: 0.985 - ETA: 9s - loss: 0.0470 - acc: 0.987 - ETA: 8s - loss: 0.0477 - acc: 0.987 - ETA: 8s - loss: 0.0457 - acc: 0.988 - ETA: 8s - loss: 0.0427 - acc: 0.988 - ETA: 7s - loss: 0.0454 - acc: 0.987 - ETA: 7s - loss: 0.0433 - acc: 0.987 - ETA: 7s - loss: 0.0412 - acc: 0.988 - ETA: 6s - loss: 0.0390 - acc: 0.989 - ETA: 6s - loss: 0.0374 - acc: 0.989 - ETA: 6s - loss: 0.0360 - acc: 0.990 - ETA: 5s - loss: 0.0352 - acc: 0.990 - ETA: 5s - loss: 0.0339 - acc: 0.990 - ETA: 5s - loss: 0.0326 - acc: 0.991 - ETA: 5s - loss: 0.0327 - acc: 0.990 - ETA: 4s - loss: 0.0339 - acc: 0.990 - ETA: 4s - loss: 0.0340 - acc: 0.990 - ETA: 4s - loss: 0.0351 - acc: 0.990 - ETA: 3s - loss: 0.0365 - acc: 0.989 - ETA: 3s - loss: 0.0368 - acc: 0.989 - ETA: 3s - loss: 0.0364 - acc: 0.989 - ETA: 2s - loss: 0.0363 - acc: 0.989 - ETA: 2s - loss: 0.0361 - acc: 0.989 - ETA: 2s - loss: 0.0350 - acc: 0.989 - ETA: 1s - loss: 0.0343 - acc: 0.989 - ETA: 1s - loss: 0.0334 - acc: 0.990 - ETA: 1s - loss: 0.0326 - acc: 0.990 - ETA: 0s - loss: 0.0321 - acc: 0.990 - ETA: 0s - loss: 0.0315 - acc: 0.990 - ETA: 0s - loss: 0.0311 - acc: 0.991 - 14s 11ms/step - loss: 0.0305 - acc: 0.9912 - val_loss: 0.0294 - val_acc: 0.9906\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0067 - acc: 1.00 - ETA: 11s - loss: 0.0050 - acc: 1.00 - ETA: 11s - loss: 0.0046 - acc: 1.00 - ETA: 11s - loss: 0.0080 - acc: 0.99 - ETA: 10s - loss: 0.0160 - acc: 0.99 - ETA: 10s - loss: 0.0158 - acc: 0.99 - ETA: 10s - loss: 0.0183 - acc: 0.99 - ETA: 9s - loss: 0.0178 - acc: 0.9941 - ETA: 9s - loss: 0.0173 - acc: 0.994 - ETA: 9s - loss: 0.0237 - acc: 0.993 - ETA: 9s - loss: 0.0216 - acc: 0.994 - ETA: 8s - loss: 0.0206 - acc: 0.994 - ETA: 8s - loss: 0.0202 - acc: 0.994 - ETA: 8s - loss: 0.0203 - acc: 0.993 - ETA: 7s - loss: 0.0231 - acc: 0.992 - ETA: 7s - loss: 0.0219 - acc: 0.993 - ETA: 7s - loss: 0.0213 - acc: 0.993 - ETA: 6s - loss: 0.0208 - acc: 0.993 - ETA: 6s - loss: 0.0216 - acc: 0.992 - ETA: 6s - loss: 0.0217 - acc: 0.993 - ETA: 5s - loss: 0.0208 - acc: 0.993 - ETA: 5s - loss: 0.0225 - acc: 0.992 - ETA: 5s - loss: 0.0219 - acc: 0.993 - ETA: 4s - loss: 0.0211 - acc: 0.993 - ETA: 4s - loss: 0.0206 - acc: 0.993 - ETA: 4s - loss: 0.0202 - acc: 0.993 - ETA: 4s - loss: 0.0201 - acc: 0.993 - ETA: 3s - loss: 0.0207 - acc: 0.992 - ETA: 3s - loss: 0.0222 - acc: 0.991 - ETA: 3s - loss: 0.0232 - acc: 0.991 - ETA: 2s - loss: 0.0244 - acc: 0.990 - ETA: 2s - loss: 0.0240 - acc: 0.991 - ETA: 2s - loss: 0.0255 - acc: 0.990 - ETA: 1s - loss: 0.0255 - acc: 0.990 - ETA: 1s - loss: 0.0248 - acc: 0.990 - ETA: 1s - loss: 0.0258 - acc: 0.990 - ETA: 0s - loss: 0.0289 - acc: 0.988 - ETA: 0s - loss: 0.0323 - acc: 0.988 - ETA: 0s - loss: 0.0318 - acc: 0.988 - 14s 11ms/step - loss: 0.0312 - acc: 0.9886 - val_loss: 0.0280 - val_acc: 0.9922\n",
      "Epoch 12/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0094 - acc: 1.00 - ETA: 11s - loss: 0.0079 - acc: 1.00 - ETA: 11s - loss: 0.0064 - acc: 1.00 - ETA: 10s - loss: 0.0058 - acc: 1.00 - ETA: 10s - loss: 0.0134 - acc: 0.99 - ETA: 10s - loss: 0.0116 - acc: 0.99 - ETA: 10s - loss: 0.0305 - acc: 0.99 - ETA: 9s - loss: 0.0300 - acc: 0.9902 - ETA: 9s - loss: 0.0300 - acc: 0.989 - ETA: 9s - loss: 0.0280 - acc: 0.990 - ETA: 8s - loss: 0.0263 - acc: 0.991 - ETA: 8s - loss: 0.0248 - acc: 0.992 - ETA: 8s - loss: 0.0248 - acc: 0.991 - ETA: 8s - loss: 0.0233 - acc: 0.992 - ETA: 7s - loss: 0.0227 - acc: 0.992 - ETA: 7s - loss: 0.0214 - acc: 0.992 - ETA: 7s - loss: 0.0213 - acc: 0.992 - ETA: 6s - loss: 0.0202 - acc: 0.992 - ETA: 6s - loss: 0.0219 - acc: 0.992 - ETA: 6s - loss: 0.0210 - acc: 0.992 - ETA: 5s - loss: 0.0202 - acc: 0.992 - ETA: 5s - loss: 0.0194 - acc: 0.993 - ETA: 5s - loss: 0.0188 - acc: 0.993 - ETA: 4s - loss: 0.0181 - acc: 0.993 - ETA: 4s - loss: 0.0177 - acc: 0.994 - ETA: 4s - loss: 0.0172 - acc: 0.994 - ETA: 4s - loss: 0.0187 - acc: 0.993 - ETA: 3s - loss: 0.0273 - acc: 0.992 - ETA: 3s - loss: 0.0271 - acc: 0.992 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 2s - loss: 0.0279 - acc: 0.992 - ETA: 2s - loss: 0.0272 - acc: 0.992 - ETA: 2s - loss: 0.0290 - acc: 0.991 - ETA: 1s - loss: 0.0283 - acc: 0.992 - ETA: 1s - loss: 0.0285 - acc: 0.991 - ETA: 1s - loss: 0.0278 - acc: 0.991 - ETA: 0s - loss: 0.0273 - acc: 0.992 - ETA: 0s - loss: 0.0267 - acc: 0.992 - ETA: 0s - loss: 0.0272 - acc: 0.992 - 13s 10ms/step - loss: 0.0285 - acc: 0.9918 - val_loss: 0.0294 - val_acc: 0.9906\n",
      "Epoch 13/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0576 - acc: 0.98 - ETA: 12s - loss: 0.0340 - acc: 0.99 - ETA: 11s - loss: 0.0240 - acc: 0.99 - ETA: 11s - loss: 0.0535 - acc: 0.98 - ETA: 11s - loss: 0.0496 - acc: 0.98 - ETA: 10s - loss: 0.0420 - acc: 0.98 - ETA: 10s - loss: 0.0389 - acc: 0.99 - ETA: 10s - loss: 0.0433 - acc: 0.98 - ETA: 9s - loss: 0.0396 - acc: 0.9896 - ETA: 9s - loss: 0.0365 - acc: 0.990 - ETA: 9s - loss: 0.0346 - acc: 0.990 - ETA: 8s - loss: 0.0353 - acc: 0.989 - ETA: 8s - loss: 0.0328 - acc: 0.990 - ETA: 8s - loss: 0.0307 - acc: 0.991 - ETA: 7s - loss: 0.0289 - acc: 0.991 - ETA: 7s - loss: 0.0272 - acc: 0.992 - ETA: 7s - loss: 0.0267 - acc: 0.991 - ETA: 6s - loss: 0.0254 - acc: 0.992 - ETA: 6s - loss: 0.0244 - acc: 0.992 - ETA: 6s - loss: 0.0235 - acc: 0.993 - ETA: 6s - loss: 0.0224 - acc: 0.993 - ETA: 5s - loss: 0.0215 - acc: 0.993 - ETA: 5s - loss: 0.0206 - acc: 0.993 - ETA: 5s - loss: 0.0205 - acc: 0.993 - ETA: 4s - loss: 0.0246 - acc: 0.992 - ETA: 4s - loss: 0.0238 - acc: 0.992 - ETA: 4s - loss: 0.0231 - acc: 0.993 - ETA: 3s - loss: 0.0226 - acc: 0.993 - ETA: 3s - loss: 0.0235 - acc: 0.993 - ETA: 3s - loss: 0.0270 - acc: 0.992 - ETA: 2s - loss: 0.0264 - acc: 0.992 - ETA: 2s - loss: 0.0258 - acc: 0.992 - ETA: 2s - loss: 0.0274 - acc: 0.992 - ETA: 1s - loss: 0.0273 - acc: 0.992 - ETA: 1s - loss: 0.0267 - acc: 0.992 - ETA: 1s - loss: 0.0268 - acc: 0.992 - ETA: 0s - loss: 0.0261 - acc: 0.992 - ETA: 0s - loss: 0.0255 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - 14s 11ms/step - loss: 0.0279 - acc: 0.9925 - val_loss: 0.0411 - val_acc: 0.9875\n",
      "Epoch 14/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0033 - acc: 1.00 - ETA: 12s - loss: 0.0092 - acc: 0.99 - ETA: 11s - loss: 0.0201 - acc: 0.98 - ETA: 11s - loss: 0.0157 - acc: 0.99 - ETA: 10s - loss: 0.0130 - acc: 0.99 - ETA: 10s - loss: 0.0111 - acc: 0.99 - ETA: 10s - loss: 0.0098 - acc: 0.99 - ETA: 10s - loss: 0.0091 - acc: 0.99 - ETA: 9s - loss: 0.0083 - acc: 0.9957 - ETA: 9s - loss: 0.0077 - acc: 0.996 - ETA: 9s - loss: 0.0085 - acc: 0.995 - ETA: 8s - loss: 0.0079 - acc: 0.995 - ETA: 8s - loss: 0.0074 - acc: 0.995 - ETA: 8s - loss: 0.0071 - acc: 0.996 - ETA: 7s - loss: 0.0075 - acc: 0.996 - ETA: 7s - loss: 0.0093 - acc: 0.995 - ETA: 7s - loss: 0.0091 - acc: 0.995 - ETA: 6s - loss: 0.0087 - acc: 0.996 - ETA: 6s - loss: 0.0138 - acc: 0.995 - ETA: 6s - loss: 0.0133 - acc: 0.995 - ETA: 5s - loss: 0.0142 - acc: 0.995 - ETA: 5s - loss: 0.0139 - acc: 0.995 - ETA: 5s - loss: 0.0170 - acc: 0.994 - ETA: 4s - loss: 0.0200 - acc: 0.994 - ETA: 4s - loss: 0.0215 - acc: 0.993 - ETA: 4s - loss: 0.0223 - acc: 0.993 - ETA: 4s - loss: 0.0220 - acc: 0.993 - ETA: 3s - loss: 0.0214 - acc: 0.993 - ETA: 3s - loss: 0.0210 - acc: 0.994 - ETA: 3s - loss: 0.0206 - acc: 0.994 - ETA: 2s - loss: 0.0199 - acc: 0.994 - ETA: 2s - loss: 0.0209 - acc: 0.994 - ETA: 2s - loss: 0.0227 - acc: 0.993 - ETA: 1s - loss: 0.0221 - acc: 0.993 - ETA: 1s - loss: 0.0219 - acc: 0.993 - ETA: 1s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0229 - acc: 0.993 - ETA: 0s - loss: 0.0224 - acc: 0.993 - 13s 10ms/step - loss: 0.0233 - acc: 0.9933 - val_loss: 0.0204 - val_acc: 0.9906\n",
      "Epoch 15/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0188 - acc: 0.98 - ETA: 11s - loss: 0.0104 - acc: 0.99 - ETA: 11s - loss: 0.0082 - acc: 0.99 - ETA: 11s - loss: 0.0171 - acc: 0.99 - ETA: 11s - loss: 0.0212 - acc: 0.99 - ETA: 10s - loss: 0.0195 - acc: 0.99 - ETA: 10s - loss: 0.0179 - acc: 0.99 - ETA: 10s - loss: 0.0188 - acc: 0.99 - ETA: 9s - loss: 0.0168 - acc: 0.9931 - ETA: 9s - loss: 0.0154 - acc: 0.993 - ETA: 9s - loss: 0.0180 - acc: 0.992 - ETA: 9s - loss: 0.0173 - acc: 0.993 - ETA: 8s - loss: 0.0161 - acc: 0.994 - ETA: 8s - loss: 0.0152 - acc: 0.994 - ETA: 8s - loss: 0.0142 - acc: 0.994 - ETA: 7s - loss: 0.0133 - acc: 0.995 - ETA: 7s - loss: 0.0172 - acc: 0.994 - ETA: 7s - loss: 0.0175 - acc: 0.993 - ETA: 6s - loss: 0.0166 - acc: 0.994 - ETA: 6s - loss: 0.0166 - acc: 0.994 - ETA: 6s - loss: 0.0159 - acc: 0.994 - ETA: 5s - loss: 0.0153 - acc: 0.994 - ETA: 5s - loss: 0.0147 - acc: 0.994 - ETA: 5s - loss: 0.0143 - acc: 0.995 - ETA: 4s - loss: 0.0152 - acc: 0.994 - ETA: 4s - loss: 0.0161 - acc: 0.994 - ETA: 4s - loss: 0.0175 - acc: 0.993 - ETA: 3s - loss: 0.0172 - acc: 0.993 - ETA: 3s - loss: 0.0199 - acc: 0.992 - ETA: 3s - loss: 0.0229 - acc: 0.991 - ETA: 2s - loss: 0.0223 - acc: 0.991 - ETA: 2s - loss: 0.0217 - acc: 0.992 - ETA: 2s - loss: 0.0212 - acc: 0.992 - ETA: 1s - loss: 0.0206 - acc: 0.992 - ETA: 1s - loss: 0.0210 - acc: 0.992 - ETA: 1s - loss: 0.0206 - acc: 0.992 - ETA: 0s - loss: 0.0204 - acc: 0.992 - ETA: 0s - loss: 0.0199 - acc: 0.993 - ETA: 0s - loss: 0.0217 - acc: 0.992 - 14s 11ms/step - loss: 0.0213 - acc: 0.9929 - val_loss: 0.0493 - val_acc: 0.9875\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0137 - acc: 1.00 - ETA: 11s - loss: 0.0098 - acc: 1.00 - ETA: 11s - loss: 0.0241 - acc: 0.99 - ETA: 11s - loss: 0.0336 - acc: 0.98 - ETA: 11s - loss: 0.0332 - acc: 0.98 - ETA: 10s - loss: 0.0308 - acc: 0.98 - ETA: 10s - loss: 0.0269 - acc: 0.98 - ETA: 10s - loss: 0.0238 - acc: 0.98 - ETA: 9s - loss: 0.0212 - acc: 0.9905 - ETA: 9s - loss: 0.0204 - acc: 0.991 - ETA: 9s - loss: 0.0226 - acc: 0.990 - ETA: 9s - loss: 0.0209 - acc: 0.991 - ETA: 8s - loss: 0.0195 - acc: 0.992 - ETA: 8s - loss: 0.0183 - acc: 0.992 - ETA: 8s - loss: 0.0172 - acc: 0.993 - ETA: 7s - loss: 0.0163 - acc: 0.993 - ETA: 7s - loss: 0.0154 - acc: 0.994 - ETA: 7s - loss: 0.0177 - acc: 0.993 - ETA: 7s - loss: 0.0177 - acc: 0.993 - ETA: 6s - loss: 0.0177 - acc: 0.992 - ETA: 6s - loss: 0.0186 - acc: 0.992 - ETA: 6s - loss: 0.0181 - acc: 0.992 - ETA: 5s - loss: 0.0223 - acc: 0.991 - ETA: 5s - loss: 0.0239 - acc: 0.991 - ETA: 5s - loss: 0.0230 - acc: 0.991 - ETA: 4s - loss: 0.0223 - acc: 0.991 - ETA: 4s - loss: 0.0217 - acc: 0.992 - ETA: 4s - loss: 0.0233 - acc: 0.991 - ETA: 3s - loss: 0.0228 - acc: 0.992 - ETA: 3s - loss: 0.0224 - acc: 0.992 - ETA: 3s - loss: 0.0218 - acc: 0.992 - ETA: 2s - loss: 0.0212 - acc: 0.992 - ETA: 2s - loss: 0.0205 - acc: 0.993 - ETA: 2s - loss: 0.0205 - acc: 0.993 - ETA: 1s - loss: 0.0200 - acc: 0.993 - ETA: 1s - loss: 0.0197 - acc: 0.993 - ETA: 1s - loss: 0.0192 - acc: 0.993 - ETA: 0s - loss: 0.0187 - acc: 0.993 - ETA: 0s - loss: 0.0185 - acc: 0.994 - 15s 12ms/step - loss: 0.0181 - acc: 0.9941 - val_loss: 0.1204 - val_acc: 0.9726\n",
      "Epoch 17/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 5.8150e-04 - acc: 1.00 - ETA: 12s - loss: 6.6189e-04 - acc: 1.00 - ETA: 12s - loss: 0.0015 - acc: 1.0000   - ETA: 12s - loss: 0.0163 - acc: 0.99 - ETA: 11s - loss: 0.0132 - acc: 0.99 - ETA: 11s - loss: 0.0119 - acc: 0.99 - ETA: 11s - loss: 0.0103 - acc: 0.99 - ETA: 10s - loss: 0.0156 - acc: 0.99 - ETA: 10s - loss: 0.0140 - acc: 0.99 - ETA: 10s - loss: 0.0127 - acc: 0.99 - ETA: 9s - loss: 0.0116 - acc: 0.9957 - ETA: 9s - loss: 0.0135 - acc: 0.994 - ETA: 8s - loss: 0.0125 - acc: 0.995 - ETA: 8s - loss: 0.0120 - acc: 0.995 - ETA: 8s - loss: 0.0114 - acc: 0.995 - ETA: 7s - loss: 0.0147 - acc: 0.995 - ETA: 7s - loss: 0.0156 - acc: 0.994 - ETA: 7s - loss: 0.0148 - acc: 0.994 - ETA: 6s - loss: 0.0171 - acc: 0.993 - ETA: 6s - loss: 0.0166 - acc: 0.994 - ETA: 6s - loss: 0.0160 - acc: 0.994 - ETA: 5s - loss: 0.0154 - acc: 0.994 - ETA: 5s - loss: 0.0148 - acc: 0.994 - ETA: 5s - loss: 0.0153 - acc: 0.994 - ETA: 4s - loss: 0.0152 - acc: 0.994 - ETA: 4s - loss: 0.0173 - acc: 0.993 - ETA: 4s - loss: 0.0193 - acc: 0.992 - ETA: 3s - loss: 0.0189 - acc: 0.993 - ETA: 3s - loss: 0.0192 - acc: 0.992 - ETA: 3s - loss: 0.0194 - acc: 0.992 - ETA: 2s - loss: 0.0188 - acc: 0.992 - ETA: 2s - loss: 0.0182 - acc: 0.992 - ETA: 2s - loss: 0.0216 - acc: 0.992 - ETA: 1s - loss: 0.0210 - acc: 0.992 - ETA: 1s - loss: 0.0208 - acc: 0.992 - ETA: 1s - loss: 0.0203 - acc: 0.993 - ETA: 0s - loss: 0.0198 - acc: 0.993 - ETA: 0s - loss: 0.0195 - acc: 0.993 - ETA: 0s - loss: 0.0191 - acc: 0.993 - 14s 11ms/step - loss: 0.0187 - acc: 0.9937 - val_loss: 0.0873 - val_acc: 0.9757\n",
      "Epoch 18/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0039 - acc: 1.00 - ETA: 11s - loss: 0.0195 - acc: 0.99 - ETA: 11s - loss: 0.0134 - acc: 0.99 - ETA: 11s - loss: 0.0104 - acc: 0.99 - ETA: 10s - loss: 0.0089 - acc: 0.99 - ETA: 10s - loss: 0.0138 - acc: 0.99 - ETA: 10s - loss: 0.0132 - acc: 0.99 - ETA: 9s - loss: 0.0122 - acc: 0.9961 - ETA: 9s - loss: 0.0119 - acc: 0.996 - ETA: 9s - loss: 0.0108 - acc: 0.996 - ETA: 9s - loss: 0.0099 - acc: 0.997 - ETA: 9s - loss: 0.0130 - acc: 0.996 - ETA: 8s - loss: 0.0122 - acc: 0.996 - ETA: 8s - loss: 0.0153 - acc: 0.994 - ETA: 8s - loss: 0.0155 - acc: 0.993 - ETA: 8s - loss: 0.0145 - acc: 0.994 - ETA: 7s - loss: 0.0150 - acc: 0.993 - ETA: 7s - loss: 0.0142 - acc: 0.993 - ETA: 7s - loss: 0.0135 - acc: 0.994 - ETA: 6s - loss: 0.0129 - acc: 0.994 - ETA: 6s - loss: 0.0154 - acc: 0.994 - ETA: 6s - loss: 0.0165 - acc: 0.993 - ETA: 5s - loss: 0.0159 - acc: 0.993 - ETA: 5s - loss: 0.0153 - acc: 0.994 - ETA: 5s - loss: 0.0158 - acc: 0.993 - ETA: 4s - loss: 0.0154 - acc: 0.994 - ETA: 4s - loss: 0.0173 - acc: 0.993 - ETA: 4s - loss: 0.0169 - acc: 0.993 - ETA: 3s - loss: 0.0163 - acc: 0.994 - ETA: 3s - loss: 0.0158 - acc: 0.994 - ETA: 2s - loss: 0.0160 - acc: 0.994 - ETA: 2s - loss: 0.0226 - acc: 0.993 - ETA: 2s - loss: 0.0221 - acc: 0.993 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0211 - acc: 0.993 - ETA: 1s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0230 - acc: 0.993 - ETA: 0s - loss: 0.0225 - acc: 0.993 - ETA: 0s - loss: 0.0221 - acc: 0.993 - 14s 11ms/step - loss: 0.0217 - acc: 0.9937 - val_loss: 0.1119 - val_acc: 0.9757\n",
      "Epoch 19/50\n",
      "1273/1273 [==============================] - ETA: 13s - loss: 0.0427 - acc: 0.98 - ETA: 12s - loss: 0.0217 - acc: 0.99 - ETA: 12s - loss: 0.0164 - acc: 0.99 - ETA: 11s - loss: 0.0129 - acc: 0.99 - ETA: 11s - loss: 0.0109 - acc: 0.99 - ETA: 11s - loss: 0.0096 - acc: 0.99 - ETA: 10s - loss: 0.0095 - acc: 0.99 - ETA: 10s - loss: 0.0084 - acc: 0.99 - ETA: 10s - loss: 0.0075 - acc: 0.99 - ETA: 9s - loss: 0.0070 - acc: 0.9984 - ETA: 9s - loss: 0.0070 - acc: 0.998 - ETA: 9s - loss: 0.0066 - acc: 0.998 - ETA: 8s - loss: 0.0064 - acc: 0.998 - ETA: 8s - loss: 0.0112 - acc: 0.997 - ETA: 8s - loss: 0.0108 - acc: 0.997 - ETA: 7s - loss: 0.0156 - acc: 0.996 - ETA: 7s - loss: 0.0182 - acc: 0.995 - ETA: 7s - loss: 0.0188 - acc: 0.994 - ETA: 6s - loss: 0.0182 - acc: 0.995 - ETA: 6s - loss: 0.0175 - acc: 0.995 - ETA: 6s - loss: 0.0190 - acc: 0.994 - ETA: 5s - loss: 0.0199 - acc: 0.993 - ETA: 5s - loss: 0.0198 - acc: 0.993 - ETA: 5s - loss: 0.0191 - acc: 0.993 - ETA: 4s - loss: 0.0202 - acc: 0.993 - ETA: 4s - loss: 0.0194 - acc: 0.993 - ETA: 4s - loss: 0.0188 - acc: 0.993 - ETA: 3s - loss: 0.0182 - acc: 0.993 - ETA: 3s - loss: 0.0182 - acc: 0.993 - ETA: 3s - loss: 0.0178 - acc: 0.994 - ETA: 2s - loss: 0.0173 - acc: 0.994 - ETA: 2s - loss: 0.0168 - acc: 0.994 - ETA: 2s - loss: 0.0163 - acc: 0.994 - ETA: 1s - loss: 0.0159 - acc: 0.994 - ETA: 1s - loss: 0.0155 - acc: 0.994 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0146 - acc: 0.995 - ETA: 0s - loss: 0.0143 - acc: 0.995 - 14s 11ms/step - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0810 - val_acc: 0.9796\n",
      "Epoch 20/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 2.1612e-04 - acc: 1.00 - ETA: 13s - loss: 9.7570e-04 - acc: 1.00 - ETA: 12s - loss: 8.5895e-04 - acc: 1.00 - ETA: 12s - loss: 0.0089 - acc: 0.9961   - ETA: 11s - loss: 0.0072 - acc: 0.99 - ETA: 11s - loss: 0.0061 - acc: 0.99 - ETA: 11s - loss: 0.0053 - acc: 0.99 - ETA: 10s - loss: 0.0102 - acc: 0.99 - ETA: 10s - loss: 0.0097 - acc: 0.99 - ETA: 9s - loss: 0.0095 - acc: 0.9969 - ETA: 9s - loss: 0.0088 - acc: 0.997 - ETA: 9s - loss: 0.0087 - acc: 0.997 - ETA: 8s - loss: 0.0081 - acc: 0.997 - ETA: 8s - loss: 0.0093 - acc: 0.996 - ETA: 8s - loss: 0.0151 - acc: 0.994 - ETA: 7s - loss: 0.0143 - acc: 0.995 - ETA: 7s - loss: 0.0134 - acc: 0.995 - ETA: 7s - loss: 0.0128 - acc: 0.995 - ETA: 6s - loss: 0.0122 - acc: 0.995 - ETA: 6s - loss: 0.0116 - acc: 0.996 - ETA: 6s - loss: 0.0123 - acc: 0.995 - ETA: 5s - loss: 0.0122 - acc: 0.995 - ETA: 5s - loss: 0.0117 - acc: 0.995 - ETA: 5s - loss: 0.0117 - acc: 0.995 - ETA: 4s - loss: 0.0113 - acc: 0.995 - ETA: 4s - loss: 0.0111 - acc: 0.996 - ETA: 4s - loss: 0.0107 - acc: 0.996 - ETA: 3s - loss: 0.0121 - acc: 0.995 - ETA: 3s - loss: 0.0117 - acc: 0.996 - ETA: 3s - loss: 0.0113 - acc: 0.996 - ETA: 2s - loss: 0.0112 - acc: 0.996 - ETA: 2s - loss: 0.0108 - acc: 0.996 - ETA: 2s - loss: 0.0106 - acc: 0.996 - ETA: 1s - loss: 0.0110 - acc: 0.996 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 0s - loss: 0.0107 - acc: 0.996 - ETA: 0s - loss: 0.0104 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.995 - 14s 11ms/step - loss: 0.0117 - acc: 0.9955 - val_loss: 1.0404 - val_acc: 0.8676\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0030 - acc: 1.00 - ETA: 12s - loss: 0.0016 - acc: 1.00 - ETA: 11s - loss: 0.0074 - acc: 0.99 - ETA: 11s - loss: 0.0069 - acc: 0.99 - ETA: 11s - loss: 0.0065 - acc: 0.99 - ETA: 10s - loss: 0.0105 - acc: 0.99 - ETA: 10s - loss: 0.0103 - acc: 0.99 - ETA: 10s - loss: 0.0091 - acc: 0.99 - ETA: 10s - loss: 0.0085 - acc: 0.99 - ETA: 10s - loss: 0.0077 - acc: 0.99 - ETA: 10s - loss: 0.0071 - acc: 0.99 - ETA: 9s - loss: 0.0093 - acc: 0.9961 - ETA: 9s - loss: 0.0086 - acc: 0.996 - ETA: 9s - loss: 0.0080 - acc: 0.996 - ETA: 8s - loss: 0.0076 - acc: 0.996 - ETA: 8s - loss: 0.0072 - acc: 0.997 - ETA: 8s - loss: 0.0098 - acc: 0.996 - ETA: 7s - loss: 0.0095 - acc: 0.996 - ETA: 7s - loss: 0.0101 - acc: 0.995 - ETA: 7s - loss: 0.0102 - acc: 0.995 - ETA: 6s - loss: 0.0097 - acc: 0.995 - ETA: 6s - loss: 0.0093 - acc: 0.996 - ETA: 5s - loss: 0.0090 - acc: 0.996 - ETA: 5s - loss: 0.0087 - acc: 0.996 - ETA: 5s - loss: 0.0083 - acc: 0.996 - ETA: 4s - loss: 0.0089 - acc: 0.996 - ETA: 4s - loss: 0.0115 - acc: 0.995 - ETA: 4s - loss: 0.0122 - acc: 0.994 - ETA: 3s - loss: 0.0118 - acc: 0.994 - ETA: 3s - loss: 0.0115 - acc: 0.995 - ETA: 3s - loss: 0.0117 - acc: 0.995 - ETA: 2s - loss: 0.0114 - acc: 0.995 - ETA: 2s - loss: 0.0111 - acc: 0.995 - ETA: 2s - loss: 0.0109 - acc: 0.995 - ETA: 1s - loss: 0.0134 - acc: 0.995 - ETA: 1s - loss: 0.0132 - acc: 0.995 - ETA: 0s - loss: 0.0129 - acc: 0.995 - ETA: 0s - loss: 0.0157 - acc: 0.994 - ETA: 0s - loss: 0.0164 - acc: 0.994 - 15s 11ms/step - loss: 0.0161 - acc: 0.9947 - val_loss: 0.1773 - val_acc: 0.9624\n",
      "Epoch 22/50\n",
      "1273/1273 [==============================] - ETA: 13s - loss: 0.0308 - acc: 0.98 - ETA: 12s - loss: 0.0178 - acc: 0.99 - ETA: 12s - loss: 0.0133 - acc: 0.99 - ETA: 11s - loss: 0.0103 - acc: 0.99 - ETA: 11s - loss: 0.0164 - acc: 0.99 - ETA: 11s - loss: 0.0141 - acc: 0.99 - ETA: 10s - loss: 0.0124 - acc: 0.99 - ETA: 10s - loss: 0.0122 - acc: 0.99 - ETA: 10s - loss: 0.0126 - acc: 0.99 - ETA: 9s - loss: 0.0114 - acc: 0.9945 - ETA: 9s - loss: 0.0105 - acc: 0.995 - ETA: 9s - loss: 0.0097 - acc: 0.995 - ETA: 8s - loss: 0.0091 - acc: 0.995 - ETA: 8s - loss: 0.0085 - acc: 0.996 - ETA: 8s - loss: 0.0086 - acc: 0.996 - ETA: 7s - loss: 0.0081 - acc: 0.996 - ETA: 7s - loss: 0.0114 - acc: 0.994 - ETA: 7s - loss: 0.0208 - acc: 0.992 - ETA: 6s - loss: 0.0232 - acc: 0.992 - ETA: 6s - loss: 0.0224 - acc: 0.992 - ETA: 6s - loss: 0.0218 - acc: 0.992 - ETA: 5s - loss: 0.0208 - acc: 0.993 - ETA: 5s - loss: 0.0205 - acc: 0.993 - ETA: 5s - loss: 0.0201 - acc: 0.993 - ETA: 4s - loss: 0.0194 - acc: 0.994 - ETA: 4s - loss: 0.0187 - acc: 0.994 - ETA: 4s - loss: 0.0183 - acc: 0.994 - ETA: 3s - loss: 0.0198 - acc: 0.993 - ETA: 3s - loss: 0.0195 - acc: 0.993 - ETA: 3s - loss: 0.0195 - acc: 0.993 - ETA: 2s - loss: 0.0203 - acc: 0.993 - ETA: 2s - loss: 0.0201 - acc: 0.993 - ETA: 2s - loss: 0.0199 - acc: 0.993 - ETA: 1s - loss: 0.0193 - acc: 0.993 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.994 - ETA: 0s - loss: 0.0178 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0170 - acc: 0.994 - 14s 11ms/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.0867 - val_acc: 0.9773\n",
      "Epoch 23/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 7.6460e-04 - acc: 1.00 - ETA: 14s - loss: 0.0074 - acc: 0.9922   - ETA: 13s - loss: 0.0082 - acc: 0.99 - ETA: 12s - loss: 0.0064 - acc: 0.99 - ETA: 12s - loss: 0.0060 - acc: 0.99 - ETA: 11s - loss: 0.0080 - acc: 0.99 - ETA: 11s - loss: 0.0149 - acc: 0.99 - ETA: 10s - loss: 0.0131 - acc: 0.99 - ETA: 10s - loss: 0.0123 - acc: 0.99 - ETA: 10s - loss: 0.0114 - acc: 0.99 - ETA: 9s - loss: 0.0104 - acc: 0.9972 - ETA: 9s - loss: 0.0095 - acc: 0.997 - ETA: 9s - loss: 0.0123 - acc: 0.996 - ETA: 8s - loss: 0.0118 - acc: 0.996 - ETA: 8s - loss: 0.0116 - acc: 0.996 - ETA: 8s - loss: 0.0109 - acc: 0.997 - ETA: 7s - loss: 0.0104 - acc: 0.997 - ETA: 7s - loss: 0.0099 - acc: 0.997 - ETA: 6s - loss: 0.0094 - acc: 0.997 - ETA: 6s - loss: 0.0096 - acc: 0.996 - ETA: 6s - loss: 0.0129 - acc: 0.995 - ETA: 5s - loss: 0.0127 - acc: 0.995 - ETA: 5s - loss: 0.0155 - acc: 0.995 - ETA: 5s - loss: 0.0149 - acc: 0.995 - ETA: 4s - loss: 0.0147 - acc: 0.995 - ETA: 4s - loss: 0.0141 - acc: 0.995 - ETA: 4s - loss: 0.0137 - acc: 0.995 - ETA: 3s - loss: 0.0133 - acc: 0.996 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0125 - acc: 0.996 - ETA: 2s - loss: 0.0122 - acc: 0.996 - ETA: 2s - loss: 0.0127 - acc: 0.996 - ETA: 2s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 0s - loss: 0.0114 - acc: 0.996 - ETA: 0s - loss: 0.0113 - acc: 0.996 - ETA: 0s - loss: 0.0112 - acc: 0.996 - 14s 11ms/step - loss: 0.0110 - acc: 0.9969 - val_loss: 0.1449 - val_acc: 0.9710\n",
      "Epoch 24/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0233 - acc: 0.98 - ETA: 11s - loss: 0.0120 - acc: 0.99 - ETA: 11s - loss: 0.0105 - acc: 0.99 - ETA: 11s - loss: 0.0079 - acc: 0.99 - ETA: 11s - loss: 0.0064 - acc: 0.99 - ETA: 10s - loss: 0.0054 - acc: 0.99 - ETA: 10s - loss: 0.0051 - acc: 0.99 - ETA: 10s - loss: 0.0045 - acc: 0.99 - ETA: 9s - loss: 0.0083 - acc: 0.9965 - ETA: 9s - loss: 0.0082 - acc: 0.996 - ETA: 9s - loss: 0.0087 - acc: 0.995 - ETA: 8s - loss: 0.0080 - acc: 0.996 - ETA: 8s - loss: 0.0074 - acc: 0.996 - ETA: 8s - loss: 0.0070 - acc: 0.996 - ETA: 7s - loss: 0.0065 - acc: 0.996 - ETA: 7s - loss: 0.0061 - acc: 0.997 - ETA: 7s - loss: 0.0058 - acc: 0.997 - ETA: 6s - loss: 0.0055 - acc: 0.997 - ETA: 6s - loss: 0.0054 - acc: 0.997 - ETA: 6s - loss: 0.0052 - acc: 0.997 - ETA: 5s - loss: 0.0069 - acc: 0.997 - ETA: 5s - loss: 0.0066 - acc: 0.997 - ETA: 5s - loss: 0.0063 - acc: 0.997 - ETA: 5s - loss: 0.0061 - acc: 0.997 - ETA: 4s - loss: 0.0073 - acc: 0.996 - ETA: 4s - loss: 0.0071 - acc: 0.997 - ETA: 4s - loss: 0.0082 - acc: 0.996 - ETA: 3s - loss: 0.0090 - acc: 0.996 - ETA: 3s - loss: 0.0092 - acc: 0.995 - ETA: 3s - loss: 0.0091 - acc: 0.995 - ETA: 2s - loss: 0.0089 - acc: 0.996 - ETA: 2s - loss: 0.0087 - acc: 0.996 - ETA: 2s - loss: 0.0085 - acc: 0.996 - ETA: 1s - loss: 0.0083 - acc: 0.996 - ETA: 1s - loss: 0.0081 - acc: 0.996 - ETA: 1s - loss: 0.0094 - acc: 0.996 - ETA: 0s - loss: 0.0104 - acc: 0.995 - ETA: 0s - loss: 0.0133 - acc: 0.995 - ETA: 0s - loss: 0.0133 - acc: 0.995 - 14s 11ms/step - loss: 0.0131 - acc: 0.9955 - val_loss: 0.5439 - val_acc: 0.9075\n",
      "Epoch 25/50\n",
      "1273/1273 [==============================] - ETA: 13s - loss: 0.0024 - acc: 1.00 - ETA: 12s - loss: 0.0014 - acc: 1.00 - ETA: 11s - loss: 0.0030 - acc: 1.00 - ETA: 11s - loss: 0.0024 - acc: 1.00 - ETA: 11s - loss: 0.0062 - acc: 0.99 - ETA: 10s - loss: 0.0052 - acc: 0.99 - ETA: 10s - loss: 0.0046 - acc: 0.99 - ETA: 10s - loss: 0.0041 - acc: 0.99 - ETA: 9s - loss: 0.0037 - acc: 0.9983 - ETA: 9s - loss: 0.0092 - acc: 0.996 - ETA: 9s - loss: 0.0098 - acc: 0.996 - ETA: 8s - loss: 0.0098 - acc: 0.996 - ETA: 8s - loss: 0.0140 - acc: 0.995 - ETA: 8s - loss: 0.0134 - acc: 0.996 - ETA: 7s - loss: 0.0126 - acc: 0.996 - ETA: 7s - loss: 0.0119 - acc: 0.996 - ETA: 7s - loss: 0.0112 - acc: 0.996 - ETA: 6s - loss: 0.0106 - acc: 0.997 - ETA: 6s - loss: 0.0181 - acc: 0.995 - ETA: 6s - loss: 0.0173 - acc: 0.996 - ETA: 5s - loss: 0.0177 - acc: 0.995 - ETA: 5s - loss: 0.0171 - acc: 0.995 - ETA: 5s - loss: 0.0164 - acc: 0.995 - ETA: 5s - loss: 0.0157 - acc: 0.996 - ETA: 4s - loss: 0.0151 - acc: 0.996 - ETA: 4s - loss: 0.0146 - acc: 0.996 - ETA: 4s - loss: 0.0143 - acc: 0.996 - ETA: 3s - loss: 0.0150 - acc: 0.996 - ETA: 3s - loss: 0.0145 - acc: 0.996 - ETA: 3s - loss: 0.0164 - acc: 0.995 - ETA: 2s - loss: 0.0159 - acc: 0.996 - ETA: 2s - loss: 0.0155 - acc: 0.996 - ETA: 2s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0148 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - 14s 11ms/step - loss: 0.0144 - acc: 0.9961 - val_loss: 0.1085 - val_acc: 0.9734\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 15s - loss: 2.8716e-04 - acc: 1.00 - ETA: 15s - loss: 3.8608e-04 - acc: 1.00 - ETA: 14s - loss: 0.0021 - acc: 1.0000   - ETA: 14s - loss: 0.0019 - acc: 1.00 - ETA: 13s - loss: 0.0029 - acc: 1.00 - ETA: 13s - loss: 0.0028 - acc: 1.00 - ETA: 12s - loss: 0.0024 - acc: 1.00 - ETA: 12s - loss: 0.0021 - acc: 1.00 - ETA: 11s - loss: 0.0019 - acc: 1.00 - ETA: 11s - loss: 0.0017 - acc: 1.00 - ETA: 11s - loss: 0.0016 - acc: 1.00 - ETA: 10s - loss: 0.0015 - acc: 1.00 - ETA: 10s - loss: 0.0014 - acc: 1.00 - ETA: 9s - loss: 0.0020 - acc: 1.0000 - ETA: 9s - loss: 0.0021 - acc: 1.000 - ETA: 8s - loss: 0.0020 - acc: 1.000 - ETA: 8s - loss: 0.0022 - acc: 1.000 - ETA: 8s - loss: 0.0020 - acc: 1.000 - ETA: 7s - loss: 0.0050 - acc: 0.999 - ETA: 7s - loss: 0.0050 - acc: 0.999 - ETA: 6s - loss: 0.0071 - acc: 0.998 - ETA: 6s - loss: 0.0067 - acc: 0.998 - ETA: 6s - loss: 0.0065 - acc: 0.998 - ETA: 5s - loss: 0.0062 - acc: 0.998 - ETA: 5s - loss: 0.0063 - acc: 0.998 - ETA: 4s - loss: 0.0061 - acc: 0.998 - ETA: 4s - loss: 0.0059 - acc: 0.998 - ETA: 4s - loss: 0.0059 - acc: 0.998 - ETA: 3s - loss: 0.0058 - acc: 0.998 - ETA: 3s - loss: 0.0056 - acc: 0.999 - ETA: 3s - loss: 0.0068 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 2s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - 14s 11ms/step - loss: 0.0063 - acc: 0.9986 - val_loss: 0.0659 - val_acc: 0.9843\n",
      "Epoch 27/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0015 - acc: 1.00 - ETA: 12s - loss: 7.9682e-04 - acc: 1.00 - ETA: 11s - loss: 0.0010 - acc: 1.0000   - ETA: 11s - loss: 8.2480e-04 - acc: 1.00 - ETA: 11s - loss: 0.0014 - acc: 1.0000   - ETA: 10s - loss: 0.0012 - acc: 1.00 - ETA: 10s - loss: 0.0011 - acc: 1.00 - ETA: 10s - loss: 9.9197e-04 - acc: 1.00 - ETA: 10s - loss: 0.0209 - acc: 0.9965   - ETA: 9s - loss: 0.0196 - acc: 0.9969 - ETA: 9s - loss: 0.0184 - acc: 0.997 - ETA: 9s - loss: 0.0170 - acc: 0.997 - ETA: 8s - loss: 0.0159 - acc: 0.997 - ETA: 8s - loss: 0.0148 - acc: 0.997 - ETA: 8s - loss: 0.0138 - acc: 0.997 - ETA: 7s - loss: 0.0131 - acc: 0.998 - ETA: 7s - loss: 0.0123 - acc: 0.998 - ETA: 7s - loss: 0.0116 - acc: 0.998 - ETA: 6s - loss: 0.0120 - acc: 0.997 - ETA: 6s - loss: 0.0115 - acc: 0.997 - ETA: 6s - loss: 0.0110 - acc: 0.997 - ETA: 5s - loss: 0.0112 - acc: 0.997 - ETA: 5s - loss: 0.0136 - acc: 0.996 - ETA: 5s - loss: 0.0133 - acc: 0.997 - ETA: 4s - loss: 0.0142 - acc: 0.996 - ETA: 4s - loss: 0.0152 - acc: 0.996 - ETA: 4s - loss: 0.0151 - acc: 0.996 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0141 - acc: 0.996 - ETA: 3s - loss: 0.0137 - acc: 0.996 - ETA: 2s - loss: 0.0148 - acc: 0.996 - ETA: 2s - loss: 0.0155 - acc: 0.995 - ETA: 2s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0148 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0133 - acc: 0.996 - 14s 11ms/step - loss: 0.0130 - acc: 0.9963 - val_loss: 0.0469 - val_acc: 0.9882\n",
      "Epoch 28/50\n",
      "1273/1273 [==============================] - ETA: 15s - loss: 1.3720e-04 - acc: 1.00 - ETA: 13s - loss: 0.0029 - acc: 1.0000   - ETA: 12s - loss: 0.0022 - acc: 1.00 - ETA: 12s - loss: 0.0018 - acc: 1.00 - ETA: 12s - loss: 0.0022 - acc: 1.00 - ETA: 12s - loss: 0.0018 - acc: 1.00 - ETA: 11s - loss: 0.0017 - acc: 1.00 - ETA: 11s - loss: 0.0015 - acc: 1.00 - ETA: 10s - loss: 0.0014 - acc: 1.00 - ETA: 10s - loss: 0.0013 - acc: 1.00 - ETA: 10s - loss: 0.0012 - acc: 1.00 - ETA: 9s - loss: 0.0011 - acc: 1.0000 - ETA: 9s - loss: 0.0032 - acc: 0.998 - ETA: 8s - loss: 0.0036 - acc: 0.998 - ETA: 8s - loss: 0.0034 - acc: 0.999 - ETA: 8s - loss: 0.0032 - acc: 0.999 - ETA: 7s - loss: 0.0031 - acc: 0.999 - ETA: 7s - loss: 0.0060 - acc: 0.998 - ETA: 7s - loss: 0.0057 - acc: 0.998 - ETA: 6s - loss: 0.0054 - acc: 0.998 - ETA: 6s - loss: 0.0053 - acc: 0.998 - ETA: 5s - loss: 0.0051 - acc: 0.998 - ETA: 5s - loss: 0.0049 - acc: 0.998 - ETA: 5s - loss: 0.0049 - acc: 0.998 - ETA: 4s - loss: 0.0047 - acc: 0.998 - ETA: 4s - loss: 0.0079 - acc: 0.997 - ETA: 4s - loss: 0.0082 - acc: 0.997 - ETA: 3s - loss: 0.0079 - acc: 0.997 - ETA: 3s - loss: 0.0077 - acc: 0.997 - ETA: 3s - loss: 0.0076 - acc: 0.997 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 2s - loss: 0.0085 - acc: 0.997 - ETA: 2s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0090 - acc: 0.996 - ETA: 1s - loss: 0.0091 - acc: 0.996 - ETA: 0s - loss: 0.0089 - acc: 0.996 - ETA: 0s - loss: 0.0088 - acc: 0.996 - ETA: 0s - loss: 0.0095 - acc: 0.996 - 14s 11ms/step - loss: 0.0093 - acc: 0.9967 - val_loss: 0.2299 - val_acc: 0.9538\n",
      "Epoch 29/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0027 - acc: 1.00 - ETA: 11s - loss: 0.0059 - acc: 1.00 - ETA: 11s - loss: 0.0040 - acc: 1.00 - ETA: 11s - loss: 0.0030 - acc: 1.00 - ETA: 10s - loss: 0.0027 - acc: 1.00 - ETA: 10s - loss: 0.0029 - acc: 1.00 - ETA: 10s - loss: 0.0032 - acc: 1.00 - ETA: 10s - loss: 0.0037 - acc: 1.00 - ETA: 9s - loss: 0.0033 - acc: 1.0000 - ETA: 9s - loss: 0.0036 - acc: 1.000 - ETA: 9s - loss: 0.0033 - acc: 1.000 - ETA: 8s - loss: 0.0056 - acc: 0.998 - ETA: 8s - loss: 0.0053 - acc: 0.998 - ETA: 8s - loss: 0.0056 - acc: 0.998 - ETA: 7s - loss: 0.0053 - acc: 0.999 - ETA: 7s - loss: 0.0050 - acc: 0.999 - ETA: 7s - loss: 0.0047 - acc: 0.999 - ETA: 6s - loss: 0.0046 - acc: 0.999 - ETA: 6s - loss: 0.0043 - acc: 0.999 - ETA: 6s - loss: 0.0042 - acc: 0.999 - ETA: 6s - loss: 0.0048 - acc: 0.998 - ETA: 5s - loss: 0.0046 - acc: 0.998 - ETA: 5s - loss: 0.0069 - acc: 0.998 - ETA: 5s - loss: 0.0066 - acc: 0.998 - ETA: 4s - loss: 0.0064 - acc: 0.998 - ETA: 4s - loss: 0.0061 - acc: 0.998 - ETA: 4s - loss: 0.0059 - acc: 0.998 - ETA: 3s - loss: 0.0057 - acc: 0.998 - ETA: 3s - loss: 0.0066 - acc: 0.997 - ETA: 3s - loss: 0.0064 - acc: 0.997 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0087 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - 14s 11ms/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0185 - val_acc: 0.9953\n",
      "Epoch 30/50\n",
      "1273/1273 [==============================] - ETA: 13s - loss: 2.5133e-04 - acc: 1.00 - ETA: 12s - loss: 2.1764e-04 - acc: 1.00 - ETA: 12s - loss: 1.6136e-04 - acc: 1.00 - ETA: 11s - loss: 3.0799e-04 - acc: 1.00 - ETA: 11s - loss: 3.6642e-04 - acc: 1.00 - ETA: 11s - loss: 3.2909e-04 - acc: 1.00 - ETA: 10s - loss: 0.0042 - acc: 0.9978   - ETA: 10s - loss: 0.0037 - acc: 0.99 - ETA: 10s - loss: 0.0035 - acc: 0.99 - ETA: 9s - loss: 0.0032 - acc: 0.9984 - ETA: 9s - loss: 0.0060 - acc: 0.997 - ETA: 8s - loss: 0.0055 - acc: 0.997 - ETA: 8s - loss: 0.0051 - acc: 0.997 - ETA: 8s - loss: 0.0048 - acc: 0.997 - ETA: 7s - loss: 0.0060 - acc: 0.996 - ETA: 7s - loss: 0.0056 - acc: 0.997 - ETA: 7s - loss: 0.0056 - acc: 0.997 - ETA: 7s - loss: 0.0056 - acc: 0.997 - ETA: 6s - loss: 0.0079 - acc: 0.996 - ETA: 6s - loss: 0.0078 - acc: 0.996 - ETA: 6s - loss: 0.0074 - acc: 0.997 - ETA: 5s - loss: 0.0071 - acc: 0.997 - ETA: 5s - loss: 0.0068 - acc: 0.997 - ETA: 5s - loss: 0.0078 - acc: 0.996 - ETA: 4s - loss: 0.0075 - acc: 0.996 - ETA: 4s - loss: 0.0073 - acc: 0.997 - ETA: 4s - loss: 0.0070 - acc: 0.997 - ETA: 3s - loss: 0.0068 - acc: 0.997 - ETA: 3s - loss: 0.0078 - acc: 0.996 - ETA: 3s - loss: 0.0096 - acc: 0.995 - ETA: 2s - loss: 0.0096 - acc: 0.996 - ETA: 2s - loss: 0.0093 - acc: 0.996 - ETA: 2s - loss: 0.0111 - acc: 0.995 - ETA: 1s - loss: 0.0108 - acc: 0.995 - ETA: 1s - loss: 0.0111 - acc: 0.995 - ETA: 1s - loss: 0.0109 - acc: 0.995 - ETA: 0s - loss: 0.0106 - acc: 0.995 - ETA: 0s - loss: 0.0104 - acc: 0.995 - ETA: 0s - loss: 0.0101 - acc: 0.996 - 14s 11ms/step - loss: 0.0104 - acc: 0.9957 - val_loss: 0.0698 - val_acc: 0.9851\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 13s - loss: 3.1332e-04 - acc: 1.00 - ETA: 14s - loss: 1.8670e-04 - acc: 1.00 - ETA: 13s - loss: 2.3043e-04 - acc: 1.00 - ETA: 12s - loss: 2.8757e-04 - acc: 1.00 - ETA: 12s - loss: 2.9980e-04 - acc: 1.00 - ETA: 11s - loss: 0.0038 - acc: 0.9974   - ETA: 11s - loss: 0.0033 - acc: 0.99 - ETA: 10s - loss: 0.0029 - acc: 0.99 - ETA: 10s - loss: 0.0029 - acc: 0.99 - ETA: 10s - loss: 0.0065 - acc: 0.99 - ETA: 9s - loss: 0.0059 - acc: 0.9972 - ETA: 9s - loss: 0.0059 - acc: 0.997 - ETA: 9s - loss: 0.0055 - acc: 0.997 - ETA: 8s - loss: 0.0059 - acc: 0.996 - ETA: 8s - loss: 0.0057 - acc: 0.996 - ETA: 8s - loss: 0.0053 - acc: 0.997 - ETA: 7s - loss: 0.0051 - acc: 0.997 - ETA: 7s - loss: 0.0079 - acc: 0.996 - ETA: 7s - loss: 0.0075 - acc: 0.996 - ETA: 6s - loss: 0.0071 - acc: 0.996 - ETA: 6s - loss: 0.0113 - acc: 0.995 - ETA: 6s - loss: 0.0108 - acc: 0.995 - ETA: 5s - loss: 0.0105 - acc: 0.995 - ETA: 5s - loss: 0.0126 - acc: 0.995 - ETA: 4s - loss: 0.0136 - acc: 0.995 - ETA: 4s - loss: 0.0132 - acc: 0.995 - ETA: 4s - loss: 0.0129 - acc: 0.995 - ETA: 3s - loss: 0.0146 - acc: 0.995 - ETA: 3s - loss: 0.0142 - acc: 0.995 - ETA: 3s - loss: 0.0138 - acc: 0.995 - ETA: 2s - loss: 0.0134 - acc: 0.995 - ETA: 2s - loss: 0.0131 - acc: 0.995 - ETA: 2s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0138 - acc: 0.995 - ETA: 1s - loss: 0.0135 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.995 - 14s 11ms/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0236 - val_acc: 0.9937\n",
      "Epoch 32/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0024 - acc: 1.00 - ETA: 11s - loss: 0.0015 - acc: 1.00 - ETA: 11s - loss: 0.0012 - acc: 1.00 - ETA: 11s - loss: 9.5827e-04 - acc: 1.00 - ETA: 10s - loss: 8.3603e-04 - acc: 1.00 - ETA: 10s - loss: 7.7349e-04 - acc: 1.00 - ETA: 10s - loss: 7.5354e-04 - acc: 1.00 - ETA: 10s - loss: 0.0013 - acc: 1.0000   - ETA: 9s - loss: 0.0011 - acc: 1.0000 - ETA: 9s - loss: 0.0011 - acc: 1.000 - ETA: 9s - loss: 0.0011 - acc: 1.000 - ETA: 8s - loss: 0.0037 - acc: 0.998 - ETA: 8s - loss: 0.0068 - acc: 0.997 - ETA: 8s - loss: 0.0064 - acc: 0.997 - ETA: 7s - loss: 0.0080 - acc: 0.996 - ETA: 7s - loss: 0.0075 - acc: 0.997 - ETA: 7s - loss: 0.0094 - acc: 0.996 - ETA: 6s - loss: 0.0096 - acc: 0.996 - ETA: 6s - loss: 0.0091 - acc: 0.996 - ETA: 6s - loss: 0.0088 - acc: 0.996 - ETA: 5s - loss: 0.0084 - acc: 0.996 - ETA: 5s - loss: 0.0081 - acc: 0.996 - ETA: 5s - loss: 0.0077 - acc: 0.996 - ETA: 5s - loss: 0.0076 - acc: 0.997 - ETA: 4s - loss: 0.0077 - acc: 0.997 - ETA: 4s - loss: 0.0090 - acc: 0.996 - ETA: 4s - loss: 0.0098 - acc: 0.995 - ETA: 3s - loss: 0.0096 - acc: 0.996 - ETA: 3s - loss: 0.0095 - acc: 0.996 - ETA: 3s - loss: 0.0092 - acc: 0.996 - ETA: 2s - loss: 0.0090 - acc: 0.996 - ETA: 2s - loss: 0.0087 - acc: 0.996 - ETA: 2s - loss: 0.0085 - acc: 0.996 - ETA: 1s - loss: 0.0083 - acc: 0.996 - ETA: 1s - loss: 0.0081 - acc: 0.996 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0075 - acc: 0.997 - ETA: 0s - loss: 0.0073 - acc: 0.997 - 14s 11ms/step - loss: 0.0071 - acc: 0.9973 - val_loss: 0.0360 - val_acc: 0.9937\n",
      "Epoch 33/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0119 - acc: 0.99 - ETA: 12s - loss: 0.0060 - acc: 0.99 - ETA: 11s - loss: 0.0098 - acc: 0.99 - ETA: 11s - loss: 0.0077 - acc: 0.99 - ETA: 11s - loss: 0.0074 - acc: 0.99 - ETA: 10s - loss: 0.0062 - acc: 0.99 - ETA: 10s - loss: 0.0069 - acc: 0.99 - ETA: 10s - loss: 0.0112 - acc: 0.99 - ETA: 9s - loss: 0.0099 - acc: 0.9957 - ETA: 9s - loss: 0.0096 - acc: 0.996 - ETA: 9s - loss: 0.0087 - acc: 0.996 - ETA: 8s - loss: 0.0083 - acc: 0.996 - ETA: 8s - loss: 0.0077 - acc: 0.997 - ETA: 8s - loss: 0.0072 - acc: 0.997 - ETA: 7s - loss: 0.0070 - acc: 0.997 - ETA: 7s - loss: 0.0066 - acc: 0.997 - ETA: 7s - loss: 0.0062 - acc: 0.997 - ETA: 6s - loss: 0.0059 - acc: 0.997 - ETA: 6s - loss: 0.0056 - acc: 0.997 - ETA: 6s - loss: 0.0054 - acc: 0.998 - ETA: 6s - loss: 0.0051 - acc: 0.998 - ETA: 5s - loss: 0.0050 - acc: 0.998 - ETA: 5s - loss: 0.0048 - acc: 0.998 - ETA: 5s - loss: 0.0047 - acc: 0.998 - ETA: 4s - loss: 0.0108 - acc: 0.997 - ETA: 4s - loss: 0.0119 - acc: 0.997 - ETA: 4s - loss: 0.0114 - acc: 0.997 - ETA: 3s - loss: 0.0115 - acc: 0.997 - ETA: 3s - loss: 0.0112 - acc: 0.997 - ETA: 3s - loss: 0.0108 - acc: 0.997 - ETA: 2s - loss: 0.0108 - acc: 0.997 - ETA: 2s - loss: 0.0105 - acc: 0.997 - ETA: 2s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0124 - acc: 0.997 - ETA: 1s - loss: 0.0121 - acc: 0.997 - ETA: 0s - loss: 0.0123 - acc: 0.997 - ETA: 0s - loss: 0.0120 - acc: 0.997 - ETA: 0s - loss: 0.0118 - acc: 0.997 - 14s 11ms/step - loss: 0.0115 - acc: 0.9974 - val_loss: 0.1731 - val_acc: 0.9694\n",
      "Epoch 34/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0507 - acc: 0.98 - ETA: 11s - loss: 0.0263 - acc: 0.99 - ETA: 11s - loss: 0.0176 - acc: 0.99 - ETA: 11s - loss: 0.0195 - acc: 0.99 - ETA: 11s - loss: 0.0156 - acc: 0.99 - ETA: 11s - loss: 0.0130 - acc: 0.99 - ETA: 11s - loss: 0.0112 - acc: 0.99 - ETA: 10s - loss: 0.0103 - acc: 0.99 - ETA: 10s - loss: 0.0092 - acc: 0.99 - ETA: 10s - loss: 0.0083 - acc: 0.99 - ETA: 9s - loss: 0.0084 - acc: 0.9972 - ETA: 9s - loss: 0.0078 - acc: 0.997 - ETA: 9s - loss: 0.0072 - acc: 0.997 - ETA: 8s - loss: 0.0084 - acc: 0.996 - ETA: 8s - loss: 0.0079 - acc: 0.996 - ETA: 8s - loss: 0.0074 - acc: 0.997 - ETA: 7s - loss: 0.0074 - acc: 0.997 - ETA: 7s - loss: 0.0070 - acc: 0.997 - ETA: 7s - loss: 0.0067 - acc: 0.997 - ETA: 6s - loss: 0.0063 - acc: 0.997 - ETA: 6s - loss: 0.0060 - acc: 0.997 - ETA: 5s - loss: 0.0059 - acc: 0.997 - ETA: 5s - loss: 0.0066 - acc: 0.997 - ETA: 5s - loss: 0.0064 - acc: 0.997 - ETA: 4s - loss: 0.0063 - acc: 0.997 - ETA: 4s - loss: 0.0061 - acc: 0.997 - ETA: 4s - loss: 0.0058 - acc: 0.997 - ETA: 3s - loss: 0.0056 - acc: 0.997 - ETA: 3s - loss: 0.0054 - acc: 0.997 - ETA: 3s - loss: 0.0054 - acc: 0.997 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0051 - acc: 0.998 - ETA: 2s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0049 - acc: 0.997 - ETA: 0s - loss: 0.0048 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - 14s 11ms/step - loss: 0.0077 - acc: 0.9976 - val_loss: 0.1046 - val_acc: 0.9812\n",
      "Epoch 35/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 1.6165e-04 - acc: 1.00 - ETA: 11s - loss: 4.3077e-04 - acc: 1.00 - ETA: 11s - loss: 3.2082e-04 - acc: 1.00 - ETA: 11s - loss: 5.1414e-04 - acc: 1.00 - ETA: 11s - loss: 4.5246e-04 - acc: 1.00 - ETA: 10s - loss: 5.8497e-04 - acc: 1.00 - ETA: 10s - loss: 5.3082e-04 - acc: 1.00 - ETA: 10s - loss: 0.0059 - acc: 0.9971   - ETA: 9s - loss: 0.0053 - acc: 0.9974 - ETA: 9s - loss: 0.0047 - acc: 0.997 - ETA: 9s - loss: 0.0055 - acc: 0.996 - ETA: 8s - loss: 0.0050 - acc: 0.996 - ETA: 8s - loss: 0.0056 - acc: 0.996 - ETA: 8s - loss: 0.0052 - acc: 0.996 - ETA: 7s - loss: 0.0071 - acc: 0.995 - ETA: 7s - loss: 0.0079 - acc: 0.995 - ETA: 7s - loss: 0.0074 - acc: 0.995 - ETA: 6s - loss: 0.0072 - acc: 0.996 - ETA: 6s - loss: 0.0070 - acc: 0.996 - ETA: 6s - loss: 0.0066 - acc: 0.996 - ETA: 6s - loss: 0.0063 - acc: 0.996 - ETA: 5s - loss: 0.0061 - acc: 0.996 - ETA: 5s - loss: 0.0077 - acc: 0.996 - ETA: 5s - loss: 0.0074 - acc: 0.996 - ETA: 4s - loss: 0.0085 - acc: 0.995 - ETA: 4s - loss: 0.0084 - acc: 0.996 - ETA: 4s - loss: 0.0081 - acc: 0.996 - ETA: 3s - loss: 0.0079 - acc: 0.996 - ETA: 3s - loss: 0.0077 - acc: 0.996 - ETA: 3s - loss: 0.0074 - acc: 0.996 - ETA: 2s - loss: 0.0072 - acc: 0.996 - ETA: 2s - loss: 0.0070 - acc: 0.996 - ETA: 2s - loss: 0.0068 - acc: 0.996 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.997 - 14s 11ms/step - loss: 0.0057 - acc: 0.9974 - val_loss: 0.0497 - val_acc: 0.9906\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0041 - acc: 1.00 - ETA: 11s - loss: 0.0023 - acc: 1.00 - ETA: 11s - loss: 0.0015 - acc: 1.00 - ETA: 11s - loss: 0.0012 - acc: 1.00 - ETA: 10s - loss: 0.0010 - acc: 1.00 - ETA: 10s - loss: 8.8053e-04 - acc: 1.00 - ETA: 10s - loss: 0.0040 - acc: 0.9978   - ETA: 10s - loss: 0.0035 - acc: 0.99 - ETA: 9s - loss: 0.0031 - acc: 0.9983 - ETA: 9s - loss: 0.0028 - acc: 0.998 - ETA: 9s - loss: 0.0026 - acc: 0.998 - ETA: 8s - loss: 0.0024 - acc: 0.998 - ETA: 8s - loss: 0.0022 - acc: 0.998 - ETA: 8s - loss: 0.0022 - acc: 0.998 - ETA: 7s - loss: 0.2376 - acc: 0.981 - ETA: 7s - loss: 0.4932 - acc: 0.961 - ETA: 7s - loss: 0.6664 - acc: 0.946 - ETA: 6s - loss: 0.7757 - acc: 0.934 - ETA: 6s - loss: 0.8949 - acc: 0.918 - ETA: 6s - loss: 0.9655 - acc: 0.907 - ETA: 5s - loss: 0.9927 - acc: 0.898 - ETA: 5s - loss: 1.0304 - acc: 0.887 - ETA: 5s - loss: 1.0384 - acc: 0.880 - ETA: 4s - loss: 1.0494 - acc: 0.868 - ETA: 4s - loss: 1.0403 - acc: 0.860 - ETA: 4s - loss: 1.0360 - acc: 0.853 - ETA: 4s - loss: 1.0192 - acc: 0.848 - ETA: 3s - loss: 1.0041 - acc: 0.843 - ETA: 3s - loss: 0.9934 - acc: 0.839 - ETA: 3s - loss: 0.9823 - acc: 0.837 - ETA: 2s - loss: 0.9814 - acc: 0.831 - ETA: 2s - loss: 0.9816 - acc: 0.826 - ETA: 2s - loss: 0.9831 - acc: 0.822 - ETA: 1s - loss: 0.9810 - acc: 0.816 - ETA: 1s - loss: 0.9730 - acc: 0.814 - ETA: 1s - loss: 0.9645 - acc: 0.809 - ETA: 0s - loss: 0.9566 - acc: 0.807 - ETA: 0s - loss: 0.9543 - acc: 0.804 - ETA: 0s - loss: 0.9450 - acc: 0.801 - 13s 11ms/step - loss: 0.9407 - acc: 0.8001 - val_loss: 0.2949 - val_acc: 0.9922\n",
      "Epoch 37/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.6785 - acc: 0.70 - ETA: 12s - loss: 0.6828 - acc: 0.68 - ETA: 12s - loss: 0.6380 - acc: 0.69 - ETA: 11s - loss: 0.6540 - acc: 0.69 - ETA: 11s - loss: 0.6513 - acc: 0.69 - ETA: 10s - loss: 0.6387 - acc: 0.70 - ETA: 10s - loss: 0.6259 - acc: 0.71 - ETA: 10s - loss: 0.6274 - acc: 0.71 - ETA: 9s - loss: 0.6194 - acc: 0.7179 - ETA: 9s - loss: 0.6187 - acc: 0.711 - ETA: 9s - loss: 0.6046 - acc: 0.713 - ETA: 8s - loss: 0.5968 - acc: 0.712 - ETA: 8s - loss: 0.5878 - acc: 0.712 - ETA: 8s - loss: 0.5728 - acc: 0.716 - ETA: 7s - loss: 0.5992 - acc: 0.715 - ETA: 7s - loss: 0.5933 - acc: 0.717 - ETA: 7s - loss: 0.5862 - acc: 0.721 - ETA: 6s - loss: 0.5771 - acc: 0.723 - ETA: 6s - loss: 0.5692 - acc: 0.723 - ETA: 6s - loss: 0.5614 - acc: 0.726 - ETA: 6s - loss: 0.5531 - acc: 0.730 - ETA: 5s - loss: 0.5479 - acc: 0.729 - ETA: 5s - loss: 0.5463 - acc: 0.731 - ETA: 5s - loss: 0.5380 - acc: 0.735 - ETA: 4s - loss: 0.5322 - acc: 0.735 - ETA: 4s - loss: 0.5258 - acc: 0.740 - ETA: 4s - loss: 0.5216 - acc: 0.741 - ETA: 3s - loss: 0.5145 - acc: 0.746 - ETA: 3s - loss: 0.5107 - acc: 0.746 - ETA: 3s - loss: 0.5061 - acc: 0.747 - ETA: 2s - loss: 0.5032 - acc: 0.749 - ETA: 2s - loss: 0.4997 - acc: 0.752 - ETA: 2s - loss: 0.5001 - acc: 0.752 - ETA: 1s - loss: 0.4950 - acc: 0.755 - ETA: 1s - loss: 0.4888 - acc: 0.759 - ETA: 1s - loss: 0.4835 - acc: 0.761 - ETA: 0s - loss: 0.4793 - acc: 0.762 - ETA: 0s - loss: 0.4758 - acc: 0.763 - ETA: 0s - loss: 0.4718 - acc: 0.766 - 14s 11ms/step - loss: 0.4672 - acc: 0.7692 - val_loss: 0.6873 - val_acc: 0.5415\n",
      "Epoch 38/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.4584 - acc: 0.74 - ETA: 12s - loss: 0.4538 - acc: 0.76 - ETA: 12s - loss: 0.3789 - acc: 0.80 - ETA: 11s - loss: 0.3633 - acc: 0.81 - ETA: 11s - loss: 0.3445 - acc: 0.83 - ETA: 10s - loss: 0.3252 - acc: 0.84 - ETA: 10s - loss: 0.3098 - acc: 0.85 - ETA: 10s - loss: 0.3012 - acc: 0.86 - ETA: 9s - loss: 0.3037 - acc: 0.8672 - ETA: 9s - loss: 0.3080 - acc: 0.860 - ETA: 9s - loss: 0.3260 - acc: 0.855 - ETA: 8s - loss: 0.3285 - acc: 0.850 - ETA: 8s - loss: 0.3240 - acc: 0.849 - ETA: 8s - loss: 0.3135 - acc: 0.858 - ETA: 7s - loss: 0.3109 - acc: 0.860 - ETA: 7s - loss: 0.3090 - acc: 0.859 - ETA: 7s - loss: 0.3029 - acc: 0.864 - ETA: 6s - loss: 0.2981 - acc: 0.867 - ETA: 6s - loss: 0.2927 - acc: 0.870 - ETA: 6s - loss: 0.2917 - acc: 0.869 - ETA: 6s - loss: 0.2883 - acc: 0.870 - ETA: 5s - loss: 0.2849 - acc: 0.871 - ETA: 5s - loss: 0.2832 - acc: 0.871 - ETA: 5s - loss: 0.2810 - acc: 0.871 - ETA: 4s - loss: 0.2780 - acc: 0.871 - ETA: 4s - loss: 0.2745 - acc: 0.871 - ETA: 4s - loss: 0.2706 - acc: 0.873 - ETA: 3s - loss: 0.2654 - acc: 0.877 - ETA: 3s - loss: 0.2602 - acc: 0.880 - ETA: 3s - loss: 0.2557 - acc: 0.883 - ETA: 2s - loss: 0.2517 - acc: 0.886 - ETA: 2s - loss: 0.2476 - acc: 0.888 - ETA: 2s - loss: 0.2433 - acc: 0.890 - ETA: 1s - loss: 0.2388 - acc: 0.893 - ETA: 1s - loss: 0.2356 - acc: 0.895 - ETA: 1s - loss: 0.2341 - acc: 0.896 - ETA: 0s - loss: 0.2341 - acc: 0.896 - ETA: 0s - loss: 0.2351 - acc: 0.895 - ETA: 0s - loss: 0.2317 - acc: 0.897 - 13s 11ms/step - loss: 0.2285 - acc: 0.8989 - val_loss: 0.0679 - val_acc: 0.9773\n",
      "Epoch 39/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0611 - acc: 0.98 - ETA: 11s - loss: 0.0586 - acc: 0.98 - ETA: 11s - loss: 0.0521 - acc: 0.99 - ETA: 11s - loss: 0.0470 - acc: 0.99 - ETA: 11s - loss: 0.1058 - acc: 0.96 - ETA: 11s - loss: 0.1693 - acc: 0.95 - ETA: 10s - loss: 0.1920 - acc: 0.94 - ETA: 10s - loss: 0.2184 - acc: 0.92 - ETA: 10s - loss: 0.2098 - acc: 0.93 - ETA: 9s - loss: 0.2078 - acc: 0.9289 - ETA: 9s - loss: 0.2084 - acc: 0.926 - ETA: 9s - loss: 0.1948 - acc: 0.931 - ETA: 8s - loss: 0.1888 - acc: 0.931 - ETA: 8s - loss: 0.1784 - acc: 0.935 - ETA: 8s - loss: 0.1678 - acc: 0.940 - ETA: 7s - loss: 0.1595 - acc: 0.943 - ETA: 7s - loss: 0.1517 - acc: 0.946 - ETA: 7s - loss: 0.1448 - acc: 0.949 - ETA: 6s - loss: 0.1412 - acc: 0.951 - ETA: 6s - loss: 0.1357 - acc: 0.953 - ETA: 6s - loss: 0.1323 - acc: 0.953 - ETA: 5s - loss: 0.1283 - acc: 0.955 - ETA: 5s - loss: 0.1280 - acc: 0.955 - ETA: 5s - loss: 0.1246 - acc: 0.956 - ETA: 4s - loss: 0.1217 - acc: 0.957 - ETA: 4s - loss: 0.1174 - acc: 0.959 - ETA: 4s - loss: 0.1155 - acc: 0.960 - ETA: 3s - loss: 0.1120 - acc: 0.961 - ETA: 3s - loss: 0.1098 - acc: 0.962 - ETA: 3s - loss: 0.1123 - acc: 0.960 - ETA: 2s - loss: 0.1106 - acc: 0.961 - ETA: 2s - loss: 0.1088 - acc: 0.962 - ETA: 2s - loss: 0.1066 - acc: 0.962 - ETA: 1s - loss: 0.1040 - acc: 0.963 - ETA: 1s - loss: 0.1031 - acc: 0.964 - ETA: 1s - loss: 0.1010 - acc: 0.965 - ETA: 0s - loss: 0.0992 - acc: 0.965 - ETA: 0s - loss: 0.0989 - acc: 0.965 - ETA: 0s - loss: 0.0977 - acc: 0.965 - 13s 11ms/step - loss: 0.0964 - acc: 0.9662 - val_loss: 0.0654 - val_acc: 0.9843\n",
      "Epoch 40/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0193 - acc: 0.99 - ETA: 11s - loss: 0.0160 - acc: 0.99 - ETA: 11s - loss: 0.0114 - acc: 0.99 - ETA: 11s - loss: 0.0215 - acc: 0.99 - ETA: 11s - loss: 0.0289 - acc: 0.98 - ETA: 10s - loss: 0.0252 - acc: 0.99 - ETA: 10s - loss: 0.0234 - acc: 0.99 - ETA: 10s - loss: 0.0216 - acc: 0.99 - ETA: 9s - loss: 0.0201 - acc: 0.9939 - ETA: 9s - loss: 0.0205 - acc: 0.993 - ETA: 9s - loss: 0.0431 - acc: 0.989 - ETA: 8s - loss: 0.0404 - acc: 0.990 - ETA: 8s - loss: 0.0413 - acc: 0.989 - ETA: 8s - loss: 0.0506 - acc: 0.988 - ETA: 8s - loss: 0.0509 - acc: 0.988 - ETA: 7s - loss: 0.0490 - acc: 0.988 - ETA: 7s - loss: 0.0470 - acc: 0.988 - ETA: 7s - loss: 0.0461 - acc: 0.988 - ETA: 7s - loss: 0.0515 - acc: 0.985 - ETA: 6s - loss: 0.0577 - acc: 0.982 - ETA: 6s - loss: 0.0571 - acc: 0.982 - ETA: 6s - loss: 0.0547 - acc: 0.983 - ETA: 5s - loss: 0.0525 - acc: 0.984 - ETA: 5s - loss: 0.0505 - acc: 0.985 - ETA: 5s - loss: 0.0488 - acc: 0.985 - ETA: 4s - loss: 0.0472 - acc: 0.986 - ETA: 4s - loss: 0.0455 - acc: 0.986 - ETA: 4s - loss: 0.0440 - acc: 0.987 - ETA: 3s - loss: 0.0426 - acc: 0.987 - ETA: 3s - loss: 0.0413 - acc: 0.988 - ETA: 3s - loss: 0.0400 - acc: 0.988 - ETA: 2s - loss: 0.0391 - acc: 0.988 - ETA: 2s - loss: 0.0379 - acc: 0.988 - ETA: 2s - loss: 0.0369 - acc: 0.989 - ETA: 1s - loss: 0.0377 - acc: 0.989 - ETA: 1s - loss: 0.0367 - acc: 0.989 - ETA: 0s - loss: 0.0358 - acc: 0.989 - ETA: 0s - loss: 0.0472 - acc: 0.988 - ETA: 0s - loss: 0.0460 - acc: 0.989 - 14s 11ms/step - loss: 0.0463 - acc: 0.9884 - val_loss: 0.0332 - val_acc: 0.9875\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0281 - acc: 0.98 - ETA: 12s - loss: 0.0146 - acc: 0.99 - ETA: 11s - loss: 0.0101 - acc: 0.99 - ETA: 11s - loss: 0.0165 - acc: 0.99 - ETA: 11s - loss: 0.0134 - acc: 0.99 - ETA: 10s - loss: 0.0113 - acc: 0.99 - ETA: 10s - loss: 0.0099 - acc: 0.99 - ETA: 10s - loss: 0.0087 - acc: 0.99 - ETA: 9s - loss: 0.0111 - acc: 0.9948 - ETA: 9s - loss: 0.0104 - acc: 0.995 - ETA: 9s - loss: 0.0097 - acc: 0.995 - ETA: 8s - loss: 0.0099 - acc: 0.995 - ETA: 8s - loss: 0.0100 - acc: 0.995 - ETA: 8s - loss: 0.0097 - acc: 0.996 - ETA: 7s - loss: 0.0119 - acc: 0.995 - ETA: 7s - loss: 0.0112 - acc: 0.995 - ETA: 7s - loss: 0.0120 - acc: 0.994 - ETA: 6s - loss: 0.0116 - acc: 0.995 - ETA: 6s - loss: 0.0112 - acc: 0.995 - ETA: 6s - loss: 0.0107 - acc: 0.995 - ETA: 5s - loss: 0.0102 - acc: 0.995 - ETA: 5s - loss: 0.0104 - acc: 0.995 - ETA: 5s - loss: 0.0100 - acc: 0.995 - ETA: 5s - loss: 0.0096 - acc: 0.995 - ETA: 4s - loss: 0.0094 - acc: 0.995 - ETA: 4s - loss: 0.0091 - acc: 0.996 - ETA: 4s - loss: 0.0093 - acc: 0.995 - ETA: 3s - loss: 0.0090 - acc: 0.995 - ETA: 3s - loss: 0.0102 - acc: 0.995 - ETA: 3s - loss: 0.0101 - acc: 0.995 - ETA: 2s - loss: 0.0098 - acc: 0.995 - ETA: 2s - loss: 0.0097 - acc: 0.995 - ETA: 2s - loss: 0.0094 - acc: 0.996 - ETA: 1s - loss: 0.0091 - acc: 0.996 - ETA: 1s - loss: 0.0089 - acc: 0.996 - ETA: 1s - loss: 0.0087 - acc: 0.996 - ETA: 0s - loss: 0.0085 - acc: 0.996 - ETA: 0s - loss: 0.0083 - acc: 0.996 - ETA: 0s - loss: 0.0083 - acc: 0.996 - 13s 10ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 0.0354 - val_acc: 0.9875\n",
      "Epoch 42/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0030 - acc: 1.00 - ETA: 12s - loss: 0.0085 - acc: 0.99 - ETA: 11s - loss: 0.0066 - acc: 0.99 - ETA: 11s - loss: 0.0050 - acc: 0.99 - ETA: 10s - loss: 0.0041 - acc: 0.99 - ETA: 10s - loss: 0.0035 - acc: 0.99 - ETA: 10s - loss: 0.0030 - acc: 0.99 - ETA: 10s - loss: 0.0027 - acc: 0.99 - ETA: 9s - loss: 0.0024 - acc: 0.9991 - ETA: 9s - loss: 0.0022 - acc: 0.999 - ETA: 9s - loss: 0.0021 - acc: 0.999 - ETA: 8s - loss: 0.0020 - acc: 0.999 - ETA: 8s - loss: 0.0019 - acc: 0.999 - ETA: 8s - loss: 0.0018 - acc: 0.999 - ETA: 8s - loss: 0.0017 - acc: 0.999 - ETA: 7s - loss: 0.0016 - acc: 0.999 - ETA: 7s - loss: 0.0015 - acc: 0.999 - ETA: 7s - loss: 0.0017 - acc: 0.999 - ETA: 6s - loss: 0.0016 - acc: 0.999 - ETA: 6s - loss: 0.0015 - acc: 0.999 - ETA: 6s - loss: 0.0015 - acc: 0.999 - ETA: 5s - loss: 0.0018 - acc: 0.999 - ETA: 5s - loss: 0.0018 - acc: 0.999 - ETA: 5s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0017 - acc: 0.999 - ETA: 4s - loss: 0.0016 - acc: 0.999 - ETA: 4s - loss: 0.0016 - acc: 0.999 - ETA: 3s - loss: 0.0015 - acc: 0.999 - ETA: 3s - loss: 0.0018 - acc: 0.999 - ETA: 3s - loss: 0.0017 - acc: 0.999 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0079 - acc: 0.998 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0103 - acc: 0.997 - 14s 11ms/step - loss: 0.0101 - acc: 0.9974 - val_loss: 0.0175 - val_acc: 0.9969\n",
      "Epoch 43/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 0.0046 - acc: 1.00 - ETA: 11s - loss: 0.0117 - acc: 0.99 - ETA: 11s - loss: 0.0078 - acc: 0.99 - ETA: 11s - loss: 0.0063 - acc: 0.99 - ETA: 10s - loss: 0.0051 - acc: 0.99 - ETA: 10s - loss: 0.0043 - acc: 0.99 - ETA: 10s - loss: 0.0043 - acc: 0.99 - ETA: 10s - loss: 0.0037 - acc: 0.99 - ETA: 9s - loss: 0.0033 - acc: 0.9983 - ETA: 9s - loss: 0.0030 - acc: 0.998 - ETA: 9s - loss: 0.0039 - acc: 0.998 - ETA: 8s - loss: 0.0039 - acc: 0.998 - ETA: 8s - loss: 0.0039 - acc: 0.998 - ETA: 8s - loss: 0.0036 - acc: 0.998 - ETA: 7s - loss: 0.0034 - acc: 0.999 - ETA: 7s - loss: 0.0034 - acc: 0.999 - ETA: 7s - loss: 0.0053 - acc: 0.998 - ETA: 6s - loss: 0.0050 - acc: 0.998 - ETA: 6s - loss: 0.0058 - acc: 0.997 - ETA: 6s - loss: 0.0063 - acc: 0.996 - ETA: 5s - loss: 0.0060 - acc: 0.997 - ETA: 5s - loss: 0.0057 - acc: 0.997 - ETA: 5s - loss: 0.0055 - acc: 0.997 - ETA: 4s - loss: 0.0053 - acc: 0.997 - ETA: 4s - loss: 0.0051 - acc: 0.997 - ETA: 4s - loss: 0.0049 - acc: 0.997 - ETA: 4s - loss: 0.0060 - acc: 0.997 - ETA: 3s - loss: 0.0063 - acc: 0.996 - ETA: 3s - loss: 0.0061 - acc: 0.997 - ETA: 3s - loss: 0.0059 - acc: 0.997 - ETA: 2s - loss: 0.0057 - acc: 0.997 - ETA: 2s - loss: 0.0057 - acc: 0.997 - ETA: 2s - loss: 0.0055 - acc: 0.997 - ETA: 1s - loss: 0.0053 - acc: 0.997 - ETA: 1s - loss: 0.0055 - acc: 0.997 - ETA: 1s - loss: 0.0055 - acc: 0.997 - ETA: 0s - loss: 0.0062 - acc: 0.997 - ETA: 0s - loss: 0.0061 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - 14s 11ms/step - loss: 0.0058 - acc: 0.9974 - val_loss: 0.0408 - val_acc: 0.9922\n",
      "Epoch 44/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 3.4117e-04 - acc: 1.00 - ETA: 11s - loss: 7.1463e-04 - acc: 1.00 - ETA: 11s - loss: 0.0013 - acc: 1.0000   - ETA: 11s - loss: 9.6583e-04 - acc: 1.00 - ETA: 10s - loss: 7.8975e-04 - acc: 1.00 - ETA: 10s - loss: 6.6761e-04 - acc: 1.00 - ETA: 10s - loss: 0.0010 - acc: 1.0000   - ETA: 10s - loss: 8.9261e-04 - acc: 1.00 - ETA: 9s - loss: 8.2082e-04 - acc: 1.0000 - ETA: 9s - loss: 7.8028e-04 - acc: 1.000 - ETA: 9s - loss: 7.2672e-04 - acc: 1.000 - ETA: 8s - loss: 7.3664e-04 - acc: 1.000 - ETA: 8s - loss: 6.8022e-04 - acc: 1.000 - ETA: 8s - loss: 6.3291e-04 - acc: 1.000 - ETA: 7s - loss: 5.9625e-04 - acc: 1.000 - ETA: 7s - loss: 0.0017 - acc: 0.9990    - ETA: 7s - loss: 0.0022 - acc: 0.999 - ETA: 6s - loss: 0.0021 - acc: 0.999 - ETA: 6s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0022 - acc: 0.999 - ETA: 5s - loss: 0.0021 - acc: 0.999 - ETA: 5s - loss: 0.0020 - acc: 0.999 - ETA: 5s - loss: 0.0020 - acc: 0.999 - ETA: 4s - loss: 0.0024 - acc: 0.998 - ETA: 4s - loss: 0.0024 - acc: 0.998 - ETA: 4s - loss: 0.0023 - acc: 0.998 - ETA: 4s - loss: 0.0022 - acc: 0.998 - ETA: 3s - loss: 0.0021 - acc: 0.998 - ETA: 3s - loss: 0.0021 - acc: 0.998 - ETA: 3s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0031 - acc: 0.998 - ETA: 2s - loss: 0.0034 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.997 - ETA: 1s - loss: 0.0036 - acc: 0.997 - ETA: 1s - loss: 0.0036 - acc: 0.997 - ETA: 1s - loss: 0.0036 - acc: 0.997 - ETA: 0s - loss: 0.0035 - acc: 0.997 - ETA: 0s - loss: 0.0034 - acc: 0.997 - ETA: 0s - loss: 0.0046 - acc: 0.997 - 13s 10ms/step - loss: 0.0045 - acc: 0.9973 - val_loss: 0.0179 - val_acc: 0.9969\n",
      "Epoch 45/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 1.3539e-04 - acc: 1.00 - ETA: 11s - loss: 0.0043 - acc: 1.0000   - ETA: 11s - loss: 0.0030 - acc: 1.00 - ETA: 11s - loss: 0.0022 - acc: 1.00 - ETA: 11s - loss: 0.0018 - acc: 1.00 - ETA: 11s - loss: 0.0015 - acc: 1.00 - ETA: 10s - loss: 0.0016 - acc: 1.00 - ETA: 10s - loss: 0.0014 - acc: 1.00 - ETA: 10s - loss: 0.0042 - acc: 0.99 - ETA: 10s - loss: 0.0038 - acc: 0.99 - ETA: 9s - loss: 0.0034 - acc: 0.9986 - ETA: 9s - loss: 0.0031 - acc: 0.998 - ETA: 9s - loss: 0.0030 - acc: 0.998 - ETA: 8s - loss: 0.0028 - acc: 0.998 - ETA: 8s - loss: 0.0032 - acc: 0.999 - ETA: 8s - loss: 0.0030 - acc: 0.999 - ETA: 7s - loss: 0.0028 - acc: 0.999 - ETA: 7s - loss: 0.0030 - acc: 0.999 - ETA: 7s - loss: 0.0029 - acc: 0.999 - ETA: 6s - loss: 0.0028 - acc: 0.999 - ETA: 6s - loss: 0.0059 - acc: 0.998 - ETA: 6s - loss: 0.0056 - acc: 0.998 - ETA: 5s - loss: 0.0054 - acc: 0.998 - ETA: 5s - loss: 0.0052 - acc: 0.998 - ETA: 5s - loss: 0.0050 - acc: 0.998 - ETA: 4s - loss: 0.0048 - acc: 0.998 - ETA: 4s - loss: 0.0046 - acc: 0.998 - ETA: 4s - loss: 0.0045 - acc: 0.998 - ETA: 3s - loss: 0.0045 - acc: 0.998 - ETA: 3s - loss: 0.0044 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0040 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - 14s 11ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.1070 - val_acc: 0.9812\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1273/1273 [==============================] - ETA: 12s - loss: 0.0355 - acc: 0.96 - ETA: 11s - loss: 0.0178 - acc: 0.98 - ETA: 11s - loss: 0.0119 - acc: 0.98 - ETA: 11s - loss: 0.0092 - acc: 0.99 - ETA: 10s - loss: 0.0074 - acc: 0.99 - ETA: 10s - loss: 0.0062 - acc: 0.99 - ETA: 10s - loss: 0.0053 - acc: 0.99 - ETA: 9s - loss: 0.0053 - acc: 0.9961 - ETA: 9s - loss: 0.0047 - acc: 0.996 - ETA: 9s - loss: 0.0042 - acc: 0.996 - ETA: 9s - loss: 0.0039 - acc: 0.997 - ETA: 8s - loss: 0.0061 - acc: 0.996 - ETA: 8s - loss: 0.0056 - acc: 0.996 - ETA: 8s - loss: 0.0052 - acc: 0.996 - ETA: 7s - loss: 0.0055 - acc: 0.996 - ETA: 7s - loss: 0.0052 - acc: 0.997 - ETA: 7s - loss: 0.0052 - acc: 0.997 - ETA: 6s - loss: 0.0050 - acc: 0.997 - ETA: 6s - loss: 0.0047 - acc: 0.997 - ETA: 6s - loss: 0.0045 - acc: 0.997 - ETA: 5s - loss: 0.0042 - acc: 0.997 - ETA: 5s - loss: 0.0041 - acc: 0.997 - ETA: 5s - loss: 0.0039 - acc: 0.998 - ETA: 5s - loss: 0.0037 - acc: 0.998 - ETA: 4s - loss: 0.0036 - acc: 0.998 - ETA: 4s - loss: 0.0035 - acc: 0.998 - ETA: 4s - loss: 0.0034 - acc: 0.998 - ETA: 3s - loss: 0.0033 - acc: 0.998 - ETA: 3s - loss: 0.0031 - acc: 0.998 - ETA: 3s - loss: 0.0033 - acc: 0.998 - ETA: 2s - loss: 0.0032 - acc: 0.998 - ETA: 2s - loss: 0.0031 - acc: 0.998 - ETA: 2s - loss: 0.0030 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 1s - loss: 0.0057 - acc: 0.997 - ETA: 1s - loss: 0.0056 - acc: 0.997 - ETA: 0s - loss: 0.0054 - acc: 0.997 - ETA: 0s - loss: 0.0053 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.997 - 13s 10ms/step - loss: 0.0057 - acc: 0.9976 - val_loss: 0.0213 - val_acc: 0.9969\n",
      "Epoch 47/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 3.0444e-04 - acc: 1.00 - ETA: 12s - loss: 2.3678e-04 - acc: 1.00 - ETA: 11s - loss: 1.8192e-04 - acc: 1.00 - ETA: 11s - loss: 1.4192e-04 - acc: 1.00 - ETA: 11s - loss: 1.8417e-04 - acc: 1.00 - ETA: 10s - loss: 1.6009e-04 - acc: 1.00 - ETA: 10s - loss: 1.3741e-04 - acc: 1.00 - ETA: 10s - loss: 1.2766e-04 - acc: 1.00 - ETA: 10s - loss: 1.1634e-04 - acc: 1.00 - ETA: 9s - loss: 1.0480e-04 - acc: 1.0000 - ETA: 9s - loss: 0.0032 - acc: 0.9986    - ETA: 9s - loss: 0.0030 - acc: 0.998 - ETA: 8s - loss: 0.0028 - acc: 0.998 - ETA: 8s - loss: 0.0026 - acc: 0.998 - ETA: 8s - loss: 0.0025 - acc: 0.999 - ETA: 7s - loss: 0.0024 - acc: 0.999 - ETA: 7s - loss: 0.0025 - acc: 0.999 - ETA: 7s - loss: 0.0023 - acc: 0.999 - ETA: 6s - loss: 0.0053 - acc: 0.998 - ETA: 6s - loss: 0.0050 - acc: 0.998 - ETA: 6s - loss: 0.0048 - acc: 0.998 - ETA: 5s - loss: 0.0046 - acc: 0.998 - ETA: 5s - loss: 0.0044 - acc: 0.998 - ETA: 5s - loss: 0.0042 - acc: 0.998 - ETA: 4s - loss: 0.0040 - acc: 0.998 - ETA: 4s - loss: 0.0039 - acc: 0.998 - ETA: 4s - loss: 0.0038 - acc: 0.998 - ETA: 3s - loss: 0.0037 - acc: 0.998 - ETA: 3s - loss: 0.0037 - acc: 0.998 - ETA: 3s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 0s - loss: 0.0031 - acc: 0.999 - ETA: 0s - loss: 0.0030 - acc: 0.999 - ETA: 0s - loss: 0.0037 - acc: 0.998 - 14s 11ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0427 - val_acc: 0.9937\n",
      "Epoch 48/50\n",
      "1273/1273 [==============================] - ETA: 11s - loss: 1.9041e-05 - acc: 1.00 - ETA: 11s - loss: 2.0902e-05 - acc: 1.00 - ETA: 11s - loss: 0.0054 - acc: 0.9948   - ETA: 11s - loss: 0.0047 - acc: 0.99 - ETA: 10s - loss: 0.0040 - acc: 0.99 - ETA: 10s - loss: 0.0035 - acc: 0.99 - ETA: 10s - loss: 0.0030 - acc: 0.99 - ETA: 9s - loss: 0.0026 - acc: 0.9980 - ETA: 9s - loss: 0.0023 - acc: 0.998 - ETA: 9s - loss: 0.0021 - acc: 0.998 - ETA: 9s - loss: 0.0019 - acc: 0.998 - ETA: 8s - loss: 0.0018 - acc: 0.998 - ETA: 8s - loss: 0.0017 - acc: 0.998 - ETA: 8s - loss: 0.0016 - acc: 0.998 - ETA: 7s - loss: 0.0015 - acc: 0.999 - ETA: 7s - loss: 0.0014 - acc: 0.999 - ETA: 7s - loss: 0.0013 - acc: 0.999 - ETA: 6s - loss: 0.0012 - acc: 0.999 - ETA: 6s - loss: 0.0012 - acc: 0.999 - ETA: 6s - loss: 0.0011 - acc: 0.999 - ETA: 5s - loss: 0.0011 - acc: 0.999 - ETA: 5s - loss: 0.0010 - acc: 0.999 - ETA: 5s - loss: 0.0021 - acc: 0.998 - ETA: 4s - loss: 0.0020 - acc: 0.998 - ETA: 4s - loss: 0.0019 - acc: 0.998 - ETA: 4s - loss: 0.0018 - acc: 0.998 - ETA: 4s - loss: 0.0018 - acc: 0.998 - ETA: 3s - loss: 0.0018 - acc: 0.998 - ETA: 3s - loss: 0.0025 - acc: 0.998 - ETA: 3s - loss: 0.0024 - acc: 0.998 - ETA: 2s - loss: 0.0024 - acc: 0.998 - ETA: 2s - loss: 0.0032 - acc: 0.997 - ETA: 2s - loss: 0.0031 - acc: 0.997 - ETA: 1s - loss: 0.0030 - acc: 0.997 - ETA: 1s - loss: 0.0029 - acc: 0.997 - ETA: 1s - loss: 0.0037 - acc: 0.997 - ETA: 0s - loss: 0.0036 - acc: 0.997 - ETA: 0s - loss: 0.0035 - acc: 0.997 - ETA: 0s - loss: 0.0035 - acc: 0.997 - 13s 11ms/step - loss: 0.0034 - acc: 0.9976 - val_loss: 0.0348 - val_acc: 0.9953\n",
      "Epoch 49/50\n",
      "1273/1273 [==============================] - ETA: 12s - loss: 9.5233e-04 - acc: 1.00 - ETA: 11s - loss: 4.8328e-04 - acc: 1.00 - ETA: 11s - loss: 3.2779e-04 - acc: 1.00 - ETA: 11s - loss: 0.0021 - acc: 1.0000   - ETA: 10s - loss: 0.0019 - acc: 1.00 - ETA: 10s - loss: 0.0017 - acc: 1.00 - ETA: 10s - loss: 0.0015 - acc: 1.00 - ETA: 10s - loss: 0.0013 - acc: 1.00 - ETA: 9s - loss: 0.0011 - acc: 1.0000 - ETA: 9s - loss: 0.0010 - acc: 1.000 - ETA: 9s - loss: 0.0011 - acc: 1.000 - ETA: 8s - loss: 9.8650e-04 - acc: 1.000 - ETA: 8s - loss: 9.1076e-04 - acc: 1.000 - ETA: 8s - loss: 8.4600e-04 - acc: 1.000 - ETA: 7s - loss: 8.6252e-04 - acc: 1.000 - ETA: 7s - loss: 8.1280e-04 - acc: 1.000 - ETA: 7s - loss: 7.6752e-04 - acc: 1.000 - ETA: 6s - loss: 8.8003e-04 - acc: 1.000 - ETA: 6s - loss: 8.3415e-04 - acc: 1.000 - ETA: 6s - loss: 7.9289e-04 - acc: 1.000 - ETA: 5s - loss: 7.9292e-04 - acc: 1.000 - ETA: 5s - loss: 7.7701e-04 - acc: 1.000 - ETA: 5s - loss: 7.4324e-04 - acc: 1.000 - ETA: 4s - loss: 7.1514e-04 - acc: 1.000 - ETA: 4s - loss: 0.0017 - acc: 0.9994    - ETA: 4s - loss: 0.0019 - acc: 0.999 - ETA: 4s - loss: 0.0019 - acc: 0.999 - ETA: 3s - loss: 0.0020 - acc: 0.999 - ETA: 3s - loss: 0.0019 - acc: 0.999 - ETA: 3s - loss: 0.0019 - acc: 0.999 - ETA: 2s - loss: 0.0019 - acc: 0.999 - ETA: 2s - loss: 0.0023 - acc: 0.999 - ETA: 2s - loss: 0.0022 - acc: 0.999 - ETA: 1s - loss: 0.0021 - acc: 0.999 - ETA: 1s - loss: 0.0021 - acc: 0.999 - ETA: 1s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - ETA: 0s - loss: 0.0019 - acc: 0.999 - 14s 11ms/step - loss: 0.0018 - acc: 0.9992 - val_loss: 0.0707 - val_acc: 0.9890\n",
      "Epoch 50/50\n",
      "1273/1273 [==============================] - ETA: 13s - loss: 4.5823e-05 - acc: 1.00 - ETA: 12s - loss: 0.0015 - acc: 1.0000   - ETA: 13s - loss: 9.9948e-04 - acc: 1.00 - ETA: 12s - loss: 0.0032 - acc: 1.0000   - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0233 - acc: 0.99 - ETA: 11s - loss: 0.0200 - acc: 0.99 - ETA: 11s - loss: 0.0203 - acc: 0.99 - ETA: 11s - loss: 0.0180 - acc: 0.99 - ETA: 11s - loss: 0.0163 - acc: 0.99 - ETA: 10s - loss: 0.0148 - acc: 0.99 - ETA: 10s - loss: 0.0137 - acc: 0.99 - ETA: 10s - loss: 0.0127 - acc: 0.99 - ETA: 9s - loss: 0.0118 - acc: 0.9978 - ETA: 9s - loss: 0.0111 - acc: 0.997 - ETA: 8s - loss: 0.0104 - acc: 0.998 - ETA: 8s - loss: 0.0098 - acc: 0.998 - ETA: 7s - loss: 0.0092 - acc: 0.998 - ETA: 7s - loss: 0.0088 - acc: 0.998 - ETA: 7s - loss: 0.0083 - acc: 0.998 - ETA: 6s - loss: 0.0079 - acc: 0.998 - ETA: 6s - loss: 0.0076 - acc: 0.998 - ETA: 6s - loss: 0.0078 - acc: 0.998 - ETA: 5s - loss: 0.0075 - acc: 0.998 - ETA: 5s - loss: 0.0072 - acc: 0.998 - ETA: 4s - loss: 0.0069 - acc: 0.998 - ETA: 4s - loss: 0.0067 - acc: 0.998 - ETA: 4s - loss: 0.0064 - acc: 0.998 - ETA: 3s - loss: 0.0062 - acc: 0.998 - ETA: 3s - loss: 0.0060 - acc: 0.998 - ETA: 3s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - 15s 12ms/step - loss: 0.0067 - acc: 0.9984 - val_loss: 0.1005 - val_acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22281977860>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478/478 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = c.evaluate(test_sequences_matrix,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "c.save('Leave_Model_final_test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quer For Leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m = load_model('Leave_Model_final_test.h5')\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how many half days have i availed\"\n",
    "#query = \"do i have any absent for last month\"\n",
    "#query = \"how many leaves have i availed\"\n",
    "#query = \"inform me about how many leaves have i taken\"\n",
    "#query = \"is my leave approved\"\n",
    "#INQUIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i want leave for today\"\n",
    "query = \"i want sick leaves for 2 days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i am feeling sick\"\n",
    "query = \"i dont feel i can work today\"\n",
    "query = \"i dont want to come \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Approve all leaves on my behalf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'is office off on 9th november'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'offic', 'off', 'on', '9th', 'novemb']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = token_stems(sent)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test = ([list(sen)])\n",
    "sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.9025746e-10 4.3433911e-07 2.6337027e-08 9.9999952e-01]]\n",
      "0.9999995\n"
     ]
    }
   ],
   "source": [
    "score = m.predict(sen_sequences_matrix)\n",
    "print(score)\n",
    "print(max(score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inquiry 3\n",
      "Request 0\n",
      "Emottional 2\n",
      "Approval 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Inquiry 3\")\n",
    "print(\"Request 0\")\n",
    "print(\"Emottional 2\")\n",
    "print(\"Approval 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "m = model()\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=model, epochs=15, verbose=2)\n",
    "kfold = ShuffleSplit(n_splits=15, test_size=0.3, random_state=100)\n",
    "results = cross_val_score(estimator,sequences_matrix,y, cv=kfold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_intent = leave(\"i have to go to doctor\") \n",
    "print(leave_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave(name):\n",
    "    sub_intent = leave_module(name)\n",
    "                #for sub_intents\n",
    "    if(sub_intent == 'EmotionlLeave'):\n",
    "        sub_intent_two = emotional_leave(name)\n",
    "                    #return sub_intent_two\n",
    "    elif(sub_intent == 'LeaveApproval'):\n",
    "        sub_intent_two = leave_approval(name)\n",
    "                    #return sub_intent_two\n",
    "    elif(sub_intent == 'LeaveRequest'):\n",
    "        sub_intent_two = classify_leaverequest(name)\n",
    "    else:\n",
    "        sub_intent_two = classify_leaveinquiry(name)\n",
    "    return(sub_intent_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_module(name):\n",
    "        sub_intent = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        docs= train['Data']\n",
    "        tokens = []\n",
    "        for i in docs:\n",
    "            temp = token_stems(i)\n",
    "            tokens.append(temp)\n",
    "\n",
    "        x, y = np.asarray(tokens) , np.asarray(train['Type'])\n",
    "        xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)\n",
    "        max_len=200\n",
    "        max_words = 20000\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "        tok.fit_on_texts(x)\n",
    "        sequences = tok.texts_to_sequences(x)\n",
    "        test_sequences = tok.texts_to_sequences(xtest)\n",
    "\n",
    "        \n",
    "        score = leave_model(name,tok)\n",
    "        print(score)\n",
    "        if((score[0][0] > score [0][1])&(score[0][0] > score [0][2])&(score[0][0] > score [0][3])):\n",
    "            sub_intent = 'LeaveRequest'\n",
    "        elif((score[0][1] > score [0][0])&(score[0][1] > score [0][2])&(score[0][1] > score [0][3])):\n",
    "            sub_intent = 'LeaveApproval'\n",
    "        elif((score[0][2] > score [0][1])&(score[0][2] > score [0][0])&(score[0][2] > score [0][3])):\n",
    "            sub_intent = 'EmotionlLeave'\n",
    "        else:\n",
    "            sub_intent = 'LeaveInquiry'\n",
    "        return sub_intent\n",
    "\n",
    "\n",
    "\n",
    "def leave_model(name,tok):\n",
    "        top_intent = ''\n",
    "        user_response = name\n",
    "        max_len=200\n",
    "        sen = token_stems(user_response)\n",
    "        sen_test = ([list(sen)])\n",
    "        print(sen_test)\n",
    "        sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "        sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)\n",
    "        m = load_model('Leave_Model_final.h5')\n",
    "        score = m.predict(sen_sequences_matrix)\n",
    "        K.clear_session()\n",
    "            #return(\"reach till here\")\n",
    "        print(user_response)\n",
    "        user_response=\"\"\n",
    "        sen=\"\"\n",
    "        sen_test=\"\"\n",
    "        intent=\"\"\n",
    "        Score=\"\"\n",
    "        print(score)\n",
    "        return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def classify_leaverequest(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['Type']== 'LeaveRequest']\n",
    "        text = train['Data']\n",
    "        \n",
    "        half_day = train[train['sub_type_two'] == 'half_day']\n",
    "        leave = train[train['sub_type_two'] == 'leave']\n",
    "        \n",
    "       \n",
    "        gen_half_day = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in half_day['Data']]\n",
    "        gen_leave = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in leave['Data']]\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "        dictionary_half_day = gensim.corpora.Dictionary(gen_half_day)\n",
    "        dictionary_leave = gensim.corpora.Dictionary(gen_leave)\n",
    "       \n",
    "        \n",
    "        corpus_half_day = [dictionary_half_day.doc2bow(gen_half_day) for gen_half_day in gen_half_day]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_half_day  = gensim.models.TfidfModel(corpus_half_day)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_half_day = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\half_day.txt',tf_idf_half_day[corpus_half_day],\n",
    "                                              num_features=len(dictionary_half_day))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_leave = [dictionary_leave.doc2bow(gen_leave) for gen_leave in gen_leave]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave  = gensim.models.TfidfModel(corpus_leave)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave.txt',tf_idf_leave[corpus_leave],\n",
    "                                              num_features=len(dictionary_leave))\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_half_day.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_half_day[query_doc_bow]\n",
    "        half_day = np.max(sims_half_day[query_doc_tf_idf])\n",
    "        print(half_day)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_leave.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave[query_doc_bow]\n",
    "        leave = np.max(sims_leave[query_doc_tf_idf])\n",
    "        print(leave)\n",
    "        \n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "        if(half_day>leave):\n",
    "            return \"half_day\"\n",
    "        else:\n",
    "            return \"leave\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def classify_leaveinquiry(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['Type']== 'leave_inquiry']\n",
    "        text = train['Data']\n",
    "        \n",
    "        status = train[train['sub_type_two'] == 'status']\n",
    "        specific = train[train['sub_type_two'] == 'specific']\n",
    "        encashment = train[train['sub_type_two'] == 'encashment']\n",
    "        reporting = train[train['sub_type_two'] == 'reporting']\n",
    "        genral = train[train['sub_type_two'] == 'genral']\n",
    "       \n",
    "        gen_status = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in status['Data']]\n",
    "        gen_specific = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in specific['Data']]\n",
    "        gen_encashment = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in encashment['Data']]\n",
    "        gen_reporting = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in reporting['Data']]\n",
    "        gen_genral = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in genral['Data']]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        dictionary_status = gensim.corpora.Dictionary(gen_status)\n",
    "        dictionary_specific = gensim.corpora.Dictionary(gen_specific)\n",
    "        dictionary_encashment = gensim.corpora.Dictionary(gen_encashment)\n",
    "        dictionary_reporting = gensim.corpora.Dictionary(gen_reporting)\n",
    "        dictionary_genral = gensim.corpora.Dictionary(gen_genral)\n",
    "        \n",
    "        corpus_status = [dictionary_status.doc2bow(gen_status) for gen_status in gen_status]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_status  = gensim.models.TfidfModel(corpus_status)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_status = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\status.txt',tf_idf_status[corpus_status],\n",
    "                                              num_features=len(dictionary_status))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_specific = [dictionary_specific.doc2bow(gen_specific) for gen_specific in gen_specific]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_specific  = gensim.models.TfidfModel(corpus_specific)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_specific = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\specific.txt',tf_idf_specific[corpus_specific],\n",
    "                                              num_features=len(dictionary_specific))\n",
    "\n",
    "        \n",
    "        \n",
    "        corpus_encashment = [dictionary_encashment.doc2bow(gen_encashment) for gen_encashment in gen_encashment]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_encashment  = gensim.models.TfidfModel(corpus_encashment)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_encashment = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\encashment.txt',tf_idf_encashment[corpus_encashment],\n",
    "                                              num_features=len(dictionary_encashment))\n",
    "        \n",
    "        \n",
    "        corpus_reporting = [dictionary_reporting.doc2bow(gen_reporting) for gen_reporting in gen_reporting]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_reporting  = gensim.models.TfidfModel(corpus_reporting)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_reporting = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\reporting.txt',tf_idf_reporting[corpus_reporting],\n",
    "                                              num_features=len(dictionary_reporting))\n",
    "        \n",
    "        \n",
    "        corpus_genral = [dictionary_genral.doc2bow(gen_genral) for gen_genral in gen_genral]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_genral  = gensim.models.TfidfModel(corpus_genral)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_genral = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\genral.txt',tf_idf_genral[corpus_genral],\n",
    "                                              num_features=len(dictionary_genral))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_status.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_status[query_doc_bow]\n",
    "        status = np.max(sims_status[query_doc_tf_idf])\n",
    "        print(status)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_specific.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_specific[query_doc_bow]\n",
    "        specific = np.max(sims_specific[query_doc_tf_idf])\n",
    "        print(specific)\n",
    "        \n",
    "        query_doc_bow = dictionary_encashment.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_encashment[query_doc_bow]\n",
    "        encashment = np.max(sims_encashment[query_doc_tf_idf])\n",
    "        print(encashment)\n",
    "        \n",
    "        query_doc_bow = dictionary_reporting.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_reporting[query_doc_bow]\n",
    "        reporting = np.max(sims_reporting[query_doc_tf_idf])\n",
    "        print(reporting)\n",
    "        \n",
    "        query_doc_bow = dictionary_genral.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_genral[query_doc_bow]\n",
    "        genral = np.max(sims_genral[query_doc_tf_idf])\n",
    "        print(genral)\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "        if((status>specific)&(status>encashment)&(status>reporting)&(status>genral)):\n",
    "            return \"Leave_status\"\n",
    "        elif((specific>status)&(specific>encashment)&(specific>reporting)&(specific>genral)):\n",
    "            subintent = leave_inquiry_specific(name)\n",
    "            return subintent\n",
    "            #here specific work will come\n",
    "        elif((encashment>status)&(specific<encashment)&(encashment>reporting)&(encashment>genral)):\n",
    "            return \"Leave_encashment\"\n",
    "        elif((reporting>status)&(reporting>specific)&(encashment<reporting)&(reporting>genral)):\n",
    "            return \"Leave_reporting\"\n",
    "        else:\n",
    "            return \"Genral_holiday\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_inquiry_specific(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['sub_type_two']== 'specific']\n",
    "        text = train['Data']\n",
    "        \n",
    "        leave_update = train[train['TypeLeave'] == 'leave_update']\n",
    "        leave_taken = train[train['TypeLeave'] == 'leave_taken']\n",
    "        \n",
    "       \n",
    "        gen_leave_update = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_update['Data']]\n",
    "        gen_leave_taken = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in leave_taken['Data']]\n",
    "\n",
    "        \n",
    "        dictionary_leave_update = gensim.corpora.Dictionary(gen_leave_update)\n",
    "        dictionary_leave_taken = gensim.corpora.Dictionary(gen_leave_taken)\n",
    "       \n",
    "\n",
    "        \n",
    "        corpus_leave_update = [dictionary_leave_update.doc2bow(gen_leave_update) for gen_leave_update in gen_leave_update]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave_update  = gensim.models.TfidfModel(corpus_leave_update)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave_update = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave_update.txt',tf_idf_leave_update[corpus_leave_update],\n",
    "                                              num_features=len(dictionary_leave_update))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_leave_taken = [dictionary_leave_taken.doc2bow(gen_leave_taken) for gen_leave_taken in gen_leave_taken]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave_taken  = gensim.models.TfidfModel(corpus_leave_taken)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave_taken = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave_taken.txt',tf_idf_leave_taken[corpus_leave_taken],\n",
    "                                              num_features=len(dictionary_leave_taken))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_leave_update.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave_update[query_doc_bow]\n",
    "        leave_update = np.max(sims_leave_update[query_doc_tf_idf])\n",
    "        print(leave_update)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_leave_taken.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave_taken[query_doc_bow]\n",
    "        leave_taken = np.max(sims_leave_taken[query_doc_tf_idf])\n",
    "        print(leave_taken)\n",
    "        \n",
    "\n",
    "       \n",
    "        if(leave_update > leave_taken):\n",
    "            sub_type = \"leave_update\"\n",
    "        else:\n",
    "            sub_type = \"leave_taken\"\n",
    "  \n",
    "        return(sub_type)\n",
    "        \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotional_leave(name):\n",
    "        query = name\n",
    "        train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        sick = train[train[\"sub_type_two\"] == 'sick']\n",
    "        casual = train[train[\"sub_type_two\"] == 'casual']\n",
    "\n",
    "\n",
    "        #raw_documents = train['Leave Data Description']\n",
    "        #print(\"Number of sick documents:\",len(sick))\n",
    "        #print(\"Number of casual documents:\",len(casual))\n",
    "        gen_docs_s = [[w.lower() for w in word_tokenize(text)] \n",
    "                for text in sick['Data']]\n",
    "        gen_docs_c = [[w.lower() for w in word_tokenize(text)] \n",
    "                for text in casual['Data']]\n",
    "        dictionary_s = gensim.corpora.Dictionary(gen_docs_s)\n",
    "        dictionary_c = gensim.corpora.Dictionary(gen_docs_c)\n",
    "        corpus_s = [dictionary_s.doc2bow(gen_doc_s) for gen_doc_s in gen_docs_s]\n",
    "        #print(corpus_l)\n",
    "\n",
    "        corpus_c = [dictionary_c.doc2bow(gen_doc_c) for gen_doc_c in gen_docs_c]\n",
    "        #print(corpus_i)\n",
    "        \n",
    "        tf_idf_s = gensim.models.TfidfModel(corpus_s)\n",
    "        tf_idf_c = gensim.models.TfidfModel(corpus_c)\n",
    "        sims_s = gensim.similarities.Similarity('D:\\\\gensim\\\\sick.txt',tf_idf_s[corpus_s],\n",
    "                                          num_features=len(dictionary_s))\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #print(query_doc)\n",
    "        query_doc_bow = dictionary_s.doc2bow(query_doc)\n",
    "        #print(query_doc_bow)\n",
    "        query_doc_tf_idf = tf_idf_s[query_doc_bow]\n",
    "        sick = np.max(sims_s[query_doc_tf_idf])\n",
    "        print(\"sick:\",np.max(sims_s[query_doc_tf_idf]))\n",
    "        sims_c = gensim.similarities.Similarity('D:\\gensim\\\\casual.txt',tf_idf_c[corpus_c],\n",
    "                                          num_features=len(dictionary_c))\n",
    "        query_doc_bow = dictionary_c.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_c[query_doc_bow]\n",
    "        casual = np.max(sims_c[query_doc_tf_idf])\n",
    "        print(\"Casual:\",np.max(sims_c[query_doc_tf_idf]))\n",
    "        if(casual>sick):\n",
    "            return('CasualLeave')\n",
    "        else:\n",
    "            return('Indirect_SickLeave')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_approval(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"leave_final.xlsx\")\n",
    "        train = train[train['Type'] == 'Leave_Approval']\n",
    "        text = train['Data']\n",
    "        approve = train[train['sub_type_two'] == 'approve']\n",
    "        reject = train[train['sub_type_two'] == 'reject']\n",
    "       \n",
    "        gen_approve = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in approve['Data']]\n",
    "        gen_reject = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in reject['Data']]\n",
    "\n",
    "        \n",
    "        dictionary_approve = gensim.corpora.Dictionary(gen_approve)\n",
    "        dictionary_reject = gensim.corpora.Dictionary(gen_reject)\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_approve = [dictionary_approve.doc2bow(gen_approve) for gen_approve in gen_approve]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_approve  = gensim.models.TfidfModel(corpus_approve)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_approve = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\approve.txt',tf_idf_approve[corpus_approve],\n",
    "                                              num_features=len(dictionary_approve))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_reject = [dictionary_reject.doc2bow(gen_reject) for gen_reject in gen_reject]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_reject  = gensim.models.TfidfModel(corpus_reject)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_reject = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\reject.txt',tf_idf_reject[corpus_reject],\n",
    "                                              num_features=len(dictionary_reject))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_approve.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_approve[query_doc_bow]\n",
    "        approve = np.max(sims_approve[query_doc_tf_idf])\n",
    "        print(approve)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_reject.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_reject[query_doc_bow]\n",
    "        reject = np.max(sims_reject[query_doc_tf_idf])\n",
    "        print(reject)\n",
    "\n",
    "       \n",
    "        if(approve > reject):\n",
    "            sub_type = sub_type + \"approve\"\n",
    "        else:\n",
    "            sub_type = sub_type + \"reject\"\n",
    "\n",
    "        return(sub_type)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
