{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import tensorflow as tf\n",
    "    import json\n",
    "    import xlrd\n",
    "    import traceback   \n",
    "    import numpy as np #use to handle numeric data\n",
    "    import nltk #for nlp purpose\n",
    "    import pandas as pd #use for file that we read\n",
    "    import re #to handle regular expression\n",
    "    from keras.models import load_model #To load model\n",
    "    from keras import backend as K #to load the backend library that we are using \n",
    "    from tensorflow.keras.preprocessing import sequence\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from textblob import TextBlob\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from langdetect import detect\n",
    "    import string \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import h5py\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    import datefinder\n",
    "    from dateparser.search import search_dates\n",
    "    import spacy\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import gensim\n",
    "    import inflect\n",
    "    p = inflect.engine()\n",
    "    from dateutil.parser import parse\n",
    "        \n",
    "\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.read_excel(\"D:\\\\bot\\\\botapi\\\\botapi\\\\dataset\\\\Main.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset[Dataset['Module']=='leave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s.no</th>\n",
       "      <th>Data</th>\n",
       "      <th>Module</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>sub_intent_one</th>\n",
       "      <th>sub_intent_two</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>361</td>\n",
       "      <td>Please show my pending leaves</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>362</td>\n",
       "      <td>Please show my all leaves.</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_taken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>363</td>\n",
       "      <td>Please tell me the balance of casual leave</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>364</td>\n",
       "      <td>Please tell me the balance of sick leave</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>365</td>\n",
       "      <td>Please tell me the balance of annual leave</td>\n",
       "      <td>leave</td>\n",
       "      <td>particular</td>\n",
       "      <td>leave_inquiry</td>\n",
       "      <td>specific</td>\n",
       "      <td>leave_update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     s.no                                        Data Module    Quantity  \\\n",
       "360   361               Please show my pending leaves  leave  particular   \n",
       "361   362                  Please show my all leaves.  leave  particular   \n",
       "362   363  Please tell me the balance of casual leave  leave  particular   \n",
       "363   364    Please tell me the balance of sick leave  leave  particular   \n",
       "364   365  Please tell me the balance of annual leave  leave  particular   \n",
       "\n",
       "    sub_intent_one sub_intent_two    Unnamed: 6  \n",
       "360  leave_inquiry       specific  leave_update  \n",
       "361  leave_inquiry       specific   leave_taken  \n",
       "362  leave_inquiry       specific  leave_update  \n",
       "363  leave_inquiry       specific  leave_update  \n",
       "364  leave_inquiry       specific  leave_update  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Dataset['Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first tokenization will be performed then stemming will be performed over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):      \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            #if token not in stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in Data:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs[1],\"\\n\\n\",tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['leave_inquiry', 'LeaveRequest', 'Leave_Approval',\n",
       "       'emotion_LeaveRequest'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset['sub_intent_one'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(Dataset['sub_intent_one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#SVM model \n",
    "from sklearn import svm \n",
    "svm_model = svm.SVC(C=0.4,kernel = 'linear')\n",
    "svm_model.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = \"how many half days have i availed\"\n",
    "query = \"do i have any absent for last month\"\n",
    "query = \"how many leaves have i availed\"\n",
    "query = \"inform me about how many leaves have i taken\"\n",
    "query = \"is my leave approved\"\n",
    "#INQUIRY"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = \"i am feeling sick\"\n",
    "query = \"i dont feel i can work today\"\n",
    "query = \"i dont want to come \"\n",
    "query = \"will office remain close tomorrow\"\n",
    "query = 'who is my leave approving authority'\n",
    "query = 'I wonâ€™t be able to come to office today'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = 'i want sick leave for today'\n",
    "query = 'Shiza is not feeling well'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query = token_stems(query)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sequence_sen = tok.texts_to_sequences([query])\n",
    "sen = sequence.pad_sequences(sequence_sen , maxlen = max_len)\n",
    "y_pr = clf.predict(sen)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(y_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Algo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(sequences_matrix,y,random_state = 100,test_size =0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(svm_model.predict(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    model.add(LSTM(128, input_shape=(x.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(90, input_shape=(x.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    model.add(LSTM(70, input_shape=(x.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(35, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1936 samples, validate on 216 samples\n",
      "Epoch 1/20\n",
      "1936/1936 [==============================] - ETA: 4:28 - loss: 1.3861 - acc: 0.593 - ETA: 2:32 - loss: 1.3821 - acc: 0.515 - ETA: 1:50 - loss: 1.3628 - acc: 0.531 - ETA: 1:29 - loss: 1.3383 - acc: 0.531 - ETA: 1:15 - loss: 1.3734 - acc: 0.487 - ETA: 1:07 - loss: 1.3706 - acc: 0.468 - ETA: 1:00 - loss: 1.3690 - acc: 0.446 - ETA: 55s - loss: 1.3550 - acc: 0.460 - ETA: 51s - loss: 1.3488 - acc: 0.45 - ETA: 48s - loss: 1.3485 - acc: 0.45 - ETA: 46s - loss: 1.3342 - acc: 0.47 - ETA: 43s - loss: 1.3277 - acc: 0.46 - ETA: 41s - loss: 1.3176 - acc: 0.46 - ETA: 39s - loss: 1.3043 - acc: 0.47 - ETA: 38s - loss: 1.3089 - acc: 0.46 - ETA: 36s - loss: 1.3044 - acc: 0.45 - ETA: 35s - loss: 1.3041 - acc: 0.45 - ETA: 34s - loss: 1.3055 - acc: 0.45 - ETA: 32s - loss: 1.3028 - acc: 0.45 - ETA: 31s - loss: 1.2994 - acc: 0.44 - ETA: 30s - loss: 1.3001 - acc: 0.44 - ETA: 29s - loss: 1.2941 - acc: 0.45 - ETA: 28s - loss: 1.2946 - acc: 0.45 - ETA: 27s - loss: 1.2970 - acc: 0.45 - ETA: 26s - loss: 1.2973 - acc: 0.45 - ETA: 25s - loss: 1.2975 - acc: 0.44 - ETA: 24s - loss: 1.2982 - acc: 0.44 - ETA: 23s - loss: 1.2974 - acc: 0.45 - ETA: 23s - loss: 1.2928 - acc: 0.45 - ETA: 22s - loss: 1.2892 - acc: 0.45 - ETA: 21s - loss: 1.2863 - acc: 0.45 - ETA: 20s - loss: 1.2863 - acc: 0.45 - ETA: 19s - loss: 1.2864 - acc: 0.45 - ETA: 18s - loss: 1.2841 - acc: 0.45 - ETA: 18s - loss: 1.2796 - acc: 0.45 - ETA: 17s - loss: 1.2826 - acc: 0.45 - ETA: 16s - loss: 1.2852 - acc: 0.45 - ETA: 15s - loss: 1.2905 - acc: 0.44 - ETA: 15s - loss: 1.2903 - acc: 0.44 - ETA: 14s - loss: 1.2901 - acc: 0.44 - ETA: 13s - loss: 1.2928 - acc: 0.44 - ETA: 13s - loss: 1.2953 - acc: 0.44 - ETA: 12s - loss: 1.2944 - acc: 0.44 - ETA: 11s - loss: 1.2939 - acc: 0.44 - ETA: 10s - loss: 1.2929 - acc: 0.44 - ETA: 10s - loss: 1.2911 - acc: 0.44 - ETA: 9s - loss: 1.2909 - acc: 0.4475 - ETA: 8s - loss: 1.2900 - acc: 0.447 - ETA: 8s - loss: 1.2919 - acc: 0.445 - ETA: 7s - loss: 1.2917 - acc: 0.445 - ETA: 6s - loss: 1.2903 - acc: 0.449 - ETA: 5s - loss: 1.2888 - acc: 0.450 - ETA: 5s - loss: 1.2887 - acc: 0.450 - ETA: 4s - loss: 1.2888 - acc: 0.450 - ETA: 3s - loss: 1.2890 - acc: 0.448 - ETA: 3s - loss: 1.2894 - acc: 0.448 - ETA: 2s - loss: 1.2895 - acc: 0.447 - ETA: 1s - loss: 1.2864 - acc: 0.448 - ETA: 1s - loss: 1.2861 - acc: 0.448 - ETA: 0s - loss: 1.2862 - acc: 0.448 - 44s 23ms/step - loss: 1.2853 - acc: 0.4494 - val_loss: 0.8478 - val_acc: 0.9028\n",
      "Epoch 2/20\n",
      "1936/1936 [==============================] - ETA: 38s - loss: 1.1264 - acc: 0.59 - ETA: 37s - loss: 1.1989 - acc: 0.53 - ETA: 36s - loss: 1.2732 - acc: 0.45 - ETA: 36s - loss: 1.2590 - acc: 0.43 - ETA: 36s - loss: 1.2497 - acc: 0.43 - ETA: 35s - loss: 1.2629 - acc: 0.42 - ETA: 35s - loss: 1.2743 - acc: 0.41 - ETA: 34s - loss: 1.2790 - acc: 0.41 - ETA: 34s - loss: 1.2738 - acc: 0.41 - ETA: 33s - loss: 1.2796 - acc: 0.41 - ETA: 33s - loss: 1.2812 - acc: 0.42 - ETA: 33s - loss: 1.2794 - acc: 0.42 - ETA: 33s - loss: 1.2760 - acc: 0.43 - ETA: 32s - loss: 1.2749 - acc: 0.43 - ETA: 31s - loss: 1.2765 - acc: 0.43 - ETA: 31s - loss: 1.2720 - acc: 0.43 - ETA: 30s - loss: 1.2665 - acc: 0.44 - ETA: 29s - loss: 1.2655 - acc: 0.44 - ETA: 29s - loss: 1.2722 - acc: 0.43 - ETA: 28s - loss: 1.2743 - acc: 0.44 - ETA: 27s - loss: 1.2802 - acc: 0.43 - ETA: 27s - loss: 1.2745 - acc: 0.44 - ETA: 26s - loss: 1.2745 - acc: 0.44 - ETA: 25s - loss: 1.2731 - acc: 0.44 - ETA: 24s - loss: 1.2718 - acc: 0.44 - ETA: 24s - loss: 1.2714 - acc: 0.44 - ETA: 23s - loss: 1.2768 - acc: 0.43 - ETA: 22s - loss: 1.2759 - acc: 0.44 - ETA: 22s - loss: 1.2753 - acc: 0.44 - ETA: 21s - loss: 1.2742 - acc: 0.44 - ETA: 20s - loss: 1.2731 - acc: 0.44 - ETA: 19s - loss: 1.2790 - acc: 0.44 - ETA: 19s - loss: 1.2793 - acc: 0.44 - ETA: 18s - loss: 1.2791 - acc: 0.44 - ETA: 17s - loss: 1.2774 - acc: 0.44 - ETA: 17s - loss: 1.2737 - acc: 0.44 - ETA: 16s - loss: 1.2717 - acc: 0.44 - ETA: 15s - loss: 1.2765 - acc: 0.44 - ETA: 15s - loss: 1.2773 - acc: 0.43 - ETA: 14s - loss: 1.2781 - acc: 0.43 - ETA: 13s - loss: 1.2772 - acc: 0.44 - ETA: 13s - loss: 1.2754 - acc: 0.44 - ETA: 12s - loss: 1.2751 - acc: 0.44 - ETA: 11s - loss: 1.2738 - acc: 0.44 - ETA: 11s - loss: 1.2733 - acc: 0.44 - ETA: 10s - loss: 1.2726 - acc: 0.44 - ETA: 9s - loss: 1.2785 - acc: 0.4441 - ETA: 8s - loss: 1.2803 - acc: 0.442 - ETA: 8s - loss: 1.2791 - acc: 0.440 - ETA: 7s - loss: 1.2783 - acc: 0.441 - ETA: 6s - loss: 1.2780 - acc: 0.443 - ETA: 6s - loss: 1.2758 - acc: 0.445 - ETA: 5s - loss: 1.2737 - acc: 0.448 - ETA: 4s - loss: 1.2743 - acc: 0.446 - ETA: 3s - loss: 1.2734 - acc: 0.447 - ETA: 3s - loss: 1.2743 - acc: 0.445 - ETA: 2s - loss: 1.2738 - acc: 0.447 - ETA: 1s - loss: 1.2738 - acc: 0.449 - ETA: 1s - loss: 1.2766 - acc: 0.446 - ETA: 0s - loss: 1.2741 - acc: 0.449 - 45s 23ms/step - loss: 1.2727 - acc: 0.4504 - val_loss: 0.8496 - val_acc: 0.9028\n",
      "Epoch 3/20\n",
      "1936/1936 [==============================] - ETA: 43s - loss: 1.2562 - acc: 0.46 - ETA: 42s - loss: 1.3241 - acc: 0.40 - ETA: 40s - loss: 1.2847 - acc: 0.41 - ETA: 39s - loss: 1.2787 - acc: 0.42 - ETA: 38s - loss: 1.2601 - acc: 0.44 - ETA: 37s - loss: 1.2720 - acc: 0.44 - ETA: 37s - loss: 1.2817 - acc: 0.44 - ETA: 36s - loss: 1.2679 - acc: 0.44 - ETA: 34s - loss: 1.2887 - acc: 0.43 - ETA: 34s - loss: 1.2739 - acc: 0.45 - ETA: 33s - loss: 1.2694 - acc: 0.45 - ETA: 32s - loss: 1.2747 - acc: 0.44 - ETA: 31s - loss: 1.2705 - acc: 0.44 - ETA: 31s - loss: 1.2686 - acc: 0.45 - ETA: 30s - loss: 1.2751 - acc: 0.44 - ETA: 30s - loss: 1.2760 - acc: 0.43 - ETA: 29s - loss: 1.2745 - acc: 0.43 - ETA: 28s - loss: 1.2721 - acc: 0.44 - ETA: 28s - loss: 1.2765 - acc: 0.43 - ETA: 27s - loss: 1.2769 - acc: 0.43 - ETA: 26s - loss: 1.2725 - acc: 0.44 - ETA: 26s - loss: 1.2695 - acc: 0.44 - ETA: 25s - loss: 1.2675 - acc: 0.44 - ETA: 24s - loss: 1.2679 - acc: 0.43 - ETA: 24s - loss: 1.2599 - acc: 0.44 - ETA: 23s - loss: 1.2501 - acc: 0.45 - ETA: 23s - loss: 1.2466 - acc: 0.45 - ETA: 22s - loss: 1.2416 - acc: 0.45 - ETA: 21s - loss: 1.2415 - acc: 0.45 - ETA: 20s - loss: 1.2380 - acc: 0.44 - ETA: 20s - loss: 1.2337 - acc: 0.44 - ETA: 19s - loss: 1.2285 - acc: 0.44 - ETA: 18s - loss: 1.2235 - acc: 0.44 - ETA: 18s - loss: 1.2179 - acc: 0.44 - ETA: 17s - loss: 1.2176 - acc: 0.44 - ETA: 16s - loss: 1.2122 - acc: 0.44 - ETA: 16s - loss: 1.2105 - acc: 0.43 - ETA: 15s - loss: 1.2054 - acc: 0.43 - ETA: 14s - loss: 1.1955 - acc: 0.44 - ETA: 14s - loss: 1.1903 - acc: 0.43 - ETA: 13s - loss: 1.1829 - acc: 0.44 - ETA: 12s - loss: 1.1712 - acc: 0.44 - ETA: 11s - loss: 1.1619 - acc: 0.44 - ETA: 11s - loss: 1.1542 - acc: 0.44 - ETA: 10s - loss: 1.1475 - acc: 0.44 - ETA: 9s - loss: 1.1404 - acc: 0.4477 - ETA: 9s - loss: 1.1305 - acc: 0.450 - ETA: 8s - loss: 1.1316 - acc: 0.448 - ETA: 7s - loss: 1.1251 - acc: 0.450 - ETA: 7s - loss: 1.1217 - acc: 0.450 - ETA: 6s - loss: 1.1170 - acc: 0.451 - ETA: 5s - loss: 1.1128 - acc: 0.451 - ETA: 5s - loss: 1.1090 - acc: 0.451 - ETA: 4s - loss: 1.1082 - acc: 0.449 - ETA: 3s - loss: 1.1114 - acc: 0.447 - ETA: 3s - loss: 1.1071 - acc: 0.448 - ETA: 2s - loss: 1.1008 - acc: 0.452 - ETA: 1s - loss: 1.0997 - acc: 0.451 - ETA: 1s - loss: 1.0956 - acc: 0.452 - ETA: 0s - loss: 1.0918 - acc: 0.453 - 43s 22ms/step - loss: 1.0913 - acc: 0.4530 - val_loss: 0.4096 - val_acc: 0.9213\n",
      "Epoch 4/20\n",
      "1936/1936 [==============================] - ETA: 43s - loss: 0.9222 - acc: 0.56 - ETA: 45s - loss: 0.9675 - acc: 0.60 - ETA: 44s - loss: 0.9879 - acc: 0.56 - ETA: 43s - loss: 0.9557 - acc: 0.54 - ETA: 42s - loss: 0.9882 - acc: 0.49 - ETA: 41s - loss: 0.9720 - acc: 0.51 - ETA: 40s - loss: 0.9376 - acc: 0.51 - ETA: 39s - loss: 0.9405 - acc: 0.51 - ETA: 38s - loss: 0.9360 - acc: 0.51 - ETA: 37s - loss: 0.9350 - acc: 0.52 - ETA: 36s - loss: 0.9280 - acc: 0.52 - ETA: 35s - loss: 0.9198 - acc: 0.54 - ETA: 34s - loss: 0.9256 - acc: 0.53 - ETA: 33s - loss: 0.9237 - acc: 0.54 - ETA: 32s - loss: 0.9126 - acc: 0.55 - ETA: 32s - loss: 0.9126 - acc: 0.56 - ETA: 31s - loss: 0.9053 - acc: 0.57 - ETA: 30s - loss: 0.8995 - acc: 0.58 - ETA: 29s - loss: 0.9016 - acc: 0.59 - ETA: 29s - loss: 0.9113 - acc: 0.58 - ETA: 28s - loss: 0.9160 - acc: 0.59 - ETA: 28s - loss: 0.9102 - acc: 0.59 - ETA: 27s - loss: 0.9123 - acc: 0.59 - ETA: 26s - loss: 0.9013 - acc: 0.60 - ETA: 25s - loss: 0.8974 - acc: 0.61 - ETA: 25s - loss: 0.8944 - acc: 0.60 - ETA: 24s - loss: 0.8945 - acc: 0.61 - ETA: 23s - loss: 0.8869 - acc: 0.61 - ETA: 22s - loss: 0.8833 - acc: 0.61 - ETA: 22s - loss: 0.8782 - acc: 0.61 - ETA: 21s - loss: 0.8806 - acc: 0.61 - ETA: 20s - loss: 0.8761 - acc: 0.61 - ETA: 19s - loss: 0.8681 - acc: 0.62 - ETA: 19s - loss: 0.8672 - acc: 0.62 - ETA: 18s - loss: 0.8673 - acc: 0.62 - ETA: 17s - loss: 0.8643 - acc: 0.62 - ETA: 16s - loss: 0.8658 - acc: 0.62 - ETA: 16s - loss: 0.8636 - acc: 0.62 - ETA: 15s - loss: 0.8598 - acc: 0.62 - ETA: 14s - loss: 0.8598 - acc: 0.62 - ETA: 13s - loss: 0.8597 - acc: 0.62 - ETA: 13s - loss: 0.8603 - acc: 0.62 - ETA: 12s - loss: 0.8630 - acc: 0.62 - ETA: 11s - loss: 0.8645 - acc: 0.62 - ETA: 10s - loss: 0.8600 - acc: 0.62 - ETA: 10s - loss: 0.8549 - acc: 0.62 - ETA: 9s - loss: 0.8577 - acc: 0.6243 - ETA: 8s - loss: 0.8531 - acc: 0.626 - ETA: 8s - loss: 0.8502 - acc: 0.627 - ETA: 7s - loss: 0.8478 - acc: 0.627 - ETA: 6s - loss: 0.8508 - acc: 0.626 - ETA: 6s - loss: 0.8524 - acc: 0.626 - ETA: 5s - loss: 0.8506 - acc: 0.629 - ETA: 4s - loss: 0.8498 - acc: 0.629 - ETA: 3s - loss: 0.8490 - acc: 0.630 - ETA: 3s - loss: 0.8436 - acc: 0.631 - ETA: 2s - loss: 0.8417 - acc: 0.632 - ETA: 1s - loss: 0.8388 - acc: 0.635 - ETA: 1s - loss: 0.8391 - acc: 0.635 - ETA: 0s - loss: 0.8399 - acc: 0.635 - 44s 23ms/step - loss: 0.8412 - acc: 0.6333 - val_loss: 0.2930 - val_acc: 0.9167\n",
      "Epoch 5/20\n",
      "1936/1936 [==============================] - ETA: 43s - loss: 0.8781 - acc: 0.56 - ETA: 41s - loss: 0.7473 - acc: 0.64 - ETA: 39s - loss: 0.7509 - acc: 0.63 - ETA: 38s - loss: 0.8057 - acc: 0.61 - ETA: 37s - loss: 0.8015 - acc: 0.64 - ETA: 37s - loss: 0.7789 - acc: 0.65 - ETA: 37s - loss: 0.7659 - acc: 0.65 - ETA: 36s - loss: 0.7681 - acc: 0.64 - ETA: 35s - loss: 0.7453 - acc: 0.64 - ETA: 34s - loss: 0.7383 - acc: 0.66 - ETA: 34s - loss: 0.7451 - acc: 0.65 - ETA: 33s - loss: 0.7545 - acc: 0.65 - ETA: 32s - loss: 0.7511 - acc: 0.66 - ETA: 31s - loss: 0.7561 - acc: 0.66 - ETA: 30s - loss: 0.7581 - acc: 0.66 - ETA: 29s - loss: 0.7582 - acc: 0.67 - ETA: 29s - loss: 0.7722 - acc: 0.66 - ETA: 28s - loss: 0.7732 - acc: 0.67 - ETA: 28s - loss: 0.7711 - acc: 0.67 - ETA: 27s - loss: 0.7646 - acc: 0.68 - ETA: 26s - loss: 0.7655 - acc: 0.68 - ETA: 26s - loss: 0.7664 - acc: 0.68 - ETA: 25s - loss: 0.7706 - acc: 0.67 - ETA: 24s - loss: 0.7705 - acc: 0.68 - ETA: 24s - loss: 0.7630 - acc: 0.68 - ETA: 23s - loss: 0.7634 - acc: 0.68 - ETA: 22s - loss: 0.7597 - acc: 0.68 - ETA: 22s - loss: 0.7573 - acc: 0.68 - ETA: 21s - loss: 0.7634 - acc: 0.68 - ETA: 20s - loss: 0.7713 - acc: 0.67 - ETA: 20s - loss: 0.7732 - acc: 0.68 - ETA: 19s - loss: 0.7661 - acc: 0.68 - ETA: 18s - loss: 0.7693 - acc: 0.68 - ETA: 18s - loss: 0.7660 - acc: 0.68 - ETA: 17s - loss: 0.7689 - acc: 0.67 - ETA: 17s - loss: 0.7734 - acc: 0.67 - ETA: 16s - loss: 0.7720 - acc: 0.67 - ETA: 15s - loss: 0.7681 - acc: 0.67 - ETA: 15s - loss: 0.7677 - acc: 0.67 - ETA: 14s - loss: 0.7667 - acc: 0.67 - ETA: 13s - loss: 0.7658 - acc: 0.67 - ETA: 12s - loss: 0.7645 - acc: 0.67 - ETA: 12s - loss: 0.7652 - acc: 0.68 - ETA: 11s - loss: 0.7673 - acc: 0.67 - ETA: 10s - loss: 0.7685 - acc: 0.67 - ETA: 10s - loss: 0.7660 - acc: 0.67 - ETA: 9s - loss: 0.7649 - acc: 0.6789 - ETA: 8s - loss: 0.7699 - acc: 0.675 - ETA: 8s - loss: 0.7731 - acc: 0.675 - ETA: 7s - loss: 0.7803 - acc: 0.669 - ETA: 6s - loss: 0.7773 - acc: 0.671 - ETA: 5s - loss: 0.7755 - acc: 0.673 - ETA: 5s - loss: 0.7742 - acc: 0.673 - ETA: 4s - loss: 0.7694 - acc: 0.675 - ETA: 3s - loss: 0.7680 - acc: 0.675 - ETA: 3s - loss: 0.7665 - acc: 0.675 - ETA: 2s - loss: 0.7645 - acc: 0.676 - ETA: 1s - loss: 0.7671 - acc: 0.673 - ETA: 1s - loss: 0.7674 - acc: 0.674 - ETA: 0s - loss: 0.7665 - acc: 0.675 - 44s 23ms/step - loss: 0.7664 - acc: 0.6751 - val_loss: 0.2500 - val_acc: 0.8472\n",
      "Epoch 6/20\n",
      "1936/1936 [==============================] - ETA: 40s - loss: 0.6978 - acc: 0.62 - ETA: 39s - loss: 0.7008 - acc: 0.62 - ETA: 39s - loss: 0.6693 - acc: 0.63 - ETA: 38s - loss: 0.6578 - acc: 0.68 - ETA: 37s - loss: 0.6712 - acc: 0.68 - ETA: 37s - loss: 0.6789 - acc: 0.66 - ETA: 36s - loss: 0.6648 - acc: 0.66 - ETA: 35s - loss: 0.6890 - acc: 0.65 - ETA: 35s - loss: 0.6660 - acc: 0.67 - ETA: 34s - loss: 0.6580 - acc: 0.67 - ETA: 33s - loss: 0.6503 - acc: 0.68 - ETA: 32s - loss: 0.6316 - acc: 0.69 - ETA: 32s - loss: 0.6388 - acc: 0.68 - ETA: 31s - loss: 0.6539 - acc: 0.68 - ETA: 30s - loss: 0.6535 - acc: 0.68 - ETA: 29s - loss: 0.6593 - acc: 0.68 - ETA: 29s - loss: 0.6646 - acc: 0.68 - ETA: 28s - loss: 0.6542 - acc: 0.68 - ETA: 27s - loss: 0.6586 - acc: 0.68 - ETA: 27s - loss: 0.6647 - acc: 0.68 - ETA: 26s - loss: 0.6643 - acc: 0.68 - ETA: 25s - loss: 0.6744 - acc: 0.68 - ETA: 25s - loss: 0.6788 - acc: 0.67 - ETA: 24s - loss: 0.6785 - acc: 0.67 - ETA: 24s - loss: 0.6778 - acc: 0.67 - ETA: 23s - loss: 0.6693 - acc: 0.68 - ETA: 22s - loss: 0.6715 - acc: 0.68 - ETA: 22s - loss: 0.6742 - acc: 0.67 - ETA: 21s - loss: 0.6763 - acc: 0.68 - ETA: 20s - loss: 0.6774 - acc: 0.67 - ETA: 20s - loss: 0.6708 - acc: 0.68 - ETA: 19s - loss: 0.6664 - acc: 0.68 - ETA: 18s - loss: 0.6692 - acc: 0.68 - ETA: 18s - loss: 0.6612 - acc: 0.68 - ETA: 17s - loss: 0.6644 - acc: 0.68 - ETA: 16s - loss: 0.6651 - acc: 0.68 - ETA: 16s - loss: 0.6688 - acc: 0.68 - ETA: 15s - loss: 0.6645 - acc: 0.68 - ETA: 14s - loss: 0.6604 - acc: 0.68 - ETA: 14s - loss: 0.6619 - acc: 0.68 - ETA: 13s - loss: 0.6623 - acc: 0.68 - ETA: 12s - loss: 0.6681 - acc: 0.68 - ETA: 12s - loss: 0.6718 - acc: 0.67 - ETA: 11s - loss: 0.6685 - acc: 0.68 - ETA: 10s - loss: 0.6683 - acc: 0.68 - ETA: 9s - loss: 0.6726 - acc: 0.6807 - ETA: 9s - loss: 0.6732 - acc: 0.678 - ETA: 8s - loss: 0.6741 - acc: 0.677 - ETA: 7s - loss: 0.6723 - acc: 0.677 - ETA: 7s - loss: 0.6722 - acc: 0.676 - ETA: 6s - loss: 0.6716 - acc: 0.677 - ETA: 5s - loss: 0.6685 - acc: 0.677 - ETA: 5s - loss: 0.6723 - acc: 0.676 - ETA: 4s - loss: 0.6738 - acc: 0.676 - ETA: 3s - loss: 0.6728 - acc: 0.676 - ETA: 3s - loss: 0.6703 - acc: 0.676 - ETA: 2s - loss: 0.6694 - acc: 0.676 - ETA: 1s - loss: 0.6662 - acc: 0.678 - ETA: 1s - loss: 0.6671 - acc: 0.677 - ETA: 0s - loss: 0.6662 - acc: 0.677 - 42s 22ms/step - loss: 0.6672 - acc: 0.6756 - val_loss: 0.2770 - val_acc: 0.8611\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936/1936 [==============================] - ETA: 40s - loss: 0.5655 - acc: 0.68 - ETA: 39s - loss: 0.4913 - acc: 0.75 - ETA: 38s - loss: 0.5077 - acc: 0.72 - ETA: 38s - loss: 0.5684 - acc: 0.71 - ETA: 39s - loss: 0.6294 - acc: 0.68 - ETA: 39s - loss: 0.6309 - acc: 0.67 - ETA: 39s - loss: 0.6247 - acc: 0.66 - ETA: 38s - loss: 0.6005 - acc: 0.68 - ETA: 37s - loss: 0.6026 - acc: 0.69 - ETA: 37s - loss: 0.6100 - acc: 0.69 - ETA: 36s - loss: 0.6104 - acc: 0.70 - ETA: 35s - loss: 0.6298 - acc: 0.70 - ETA: 34s - loss: 0.6404 - acc: 0.69 - ETA: 33s - loss: 0.6321 - acc: 0.70 - ETA: 32s - loss: 0.6345 - acc: 0.71 - ETA: 31s - loss: 0.6211 - acc: 0.72 - ETA: 30s - loss: 0.6281 - acc: 0.72 - ETA: 29s - loss: 0.6247 - acc: 0.72 - ETA: 29s - loss: 0.6248 - acc: 0.72 - ETA: 28s - loss: 0.6239 - acc: 0.72 - ETA: 27s - loss: 0.6307 - acc: 0.72 - ETA: 27s - loss: 0.6279 - acc: 0.73 - ETA: 26s - loss: 0.6338 - acc: 0.73 - ETA: 25s - loss: 0.6259 - acc: 0.73 - ETA: 25s - loss: 0.6256 - acc: 0.73 - ETA: 24s - loss: 0.6311 - acc: 0.73 - ETA: 23s - loss: 0.6368 - acc: 0.73 - ETA: 23s - loss: 0.6393 - acc: 0.73 - ETA: 22s - loss: 0.6314 - acc: 0.74 - ETA: 21s - loss: 0.6304 - acc: 0.74 - ETA: 21s - loss: 0.6299 - acc: 0.74 - ETA: 20s - loss: 0.6327 - acc: 0.74 - ETA: 19s - loss: 0.6307 - acc: 0.74 - ETA: 18s - loss: 0.6312 - acc: 0.74 - ETA: 18s - loss: 0.6320 - acc: 0.75 - ETA: 17s - loss: 0.6301 - acc: 0.75 - ETA: 16s - loss: 0.6283 - acc: 0.75 - ETA: 15s - loss: 0.6250 - acc: 0.75 - ETA: 15s - loss: 0.6233 - acc: 0.75 - ETA: 14s - loss: 0.6189 - acc: 0.75 - ETA: 13s - loss: 0.6208 - acc: 0.75 - ETA: 13s - loss: 0.6220 - acc: 0.75 - ETA: 12s - loss: 0.6195 - acc: 0.76 - ETA: 11s - loss: 0.6186 - acc: 0.76 - ETA: 10s - loss: 0.6164 - acc: 0.76 - ETA: 10s - loss: 0.6174 - acc: 0.76 - ETA: 9s - loss: 0.6180 - acc: 0.7626 - ETA: 8s - loss: 0.6173 - acc: 0.762 - ETA: 8s - loss: 0.6169 - acc: 0.765 - ETA: 7s - loss: 0.6137 - acc: 0.766 - ETA: 6s - loss: 0.6246 - acc: 0.763 - ETA: 5s - loss: 0.6274 - acc: 0.762 - ETA: 5s - loss: 0.6299 - acc: 0.763 - ETA: 4s - loss: 0.6267 - acc: 0.764 - ETA: 3s - loss: 0.6272 - acc: 0.765 - ETA: 3s - loss: 0.6289 - acc: 0.764 - ETA: 2s - loss: 0.6272 - acc: 0.764 - ETA: 1s - loss: 0.6266 - acc: 0.765 - ETA: 1s - loss: 0.6216 - acc: 0.768 - ETA: 0s - loss: 0.6218 - acc: 0.767 - 44s 23ms/step - loss: 0.6210 - acc: 0.7670 - val_loss: 0.2924 - val_acc: 0.8519\n",
      "Epoch 8/20\n",
      "1936/1936 [==============================] - ETA: 40s - loss: 0.6998 - acc: 0.71 - ETA: 40s - loss: 0.6562 - acc: 0.73 - ETA: 40s - loss: 0.6569 - acc: 0.73 - ETA: 40s - loss: 0.5932 - acc: 0.77 - ETA: 39s - loss: 0.5674 - acc: 0.78 - ETA: 38s - loss: 0.5733 - acc: 0.78 - ETA: 38s - loss: 0.5723 - acc: 0.77 - ETA: 37s - loss: 0.5574 - acc: 0.78 - ETA: 36s - loss: 0.5593 - acc: 0.77 - ETA: 36s - loss: 0.5370 - acc: 0.78 - ETA: 35s - loss: 0.5395 - acc: 0.78 - ETA: 34s - loss: 0.5290 - acc: 0.79 - ETA: 34s - loss: 0.5410 - acc: 0.78 - ETA: 33s - loss: 0.5394 - acc: 0.78 - ETA: 32s - loss: 0.5490 - acc: 0.77 - ETA: 31s - loss: 0.5561 - acc: 0.77 - ETA: 30s - loss: 0.5670 - acc: 0.77 - ETA: 29s - loss: 0.5636 - acc: 0.77 - ETA: 29s - loss: 0.5645 - acc: 0.77 - ETA: 28s - loss: 0.5521 - acc: 0.77 - ETA: 27s - loss: 0.5461 - acc: 0.77 - ETA: 26s - loss: 0.5475 - acc: 0.78 - ETA: 26s - loss: 0.5386 - acc: 0.78 - ETA: 25s - loss: 0.5369 - acc: 0.78 - ETA: 24s - loss: 0.5348 - acc: 0.78 - ETA: 23s - loss: 0.5332 - acc: 0.79 - ETA: 23s - loss: 0.5340 - acc: 0.79 - ETA: 22s - loss: 0.5344 - acc: 0.79 - ETA: 21s - loss: 0.5276 - acc: 0.79 - ETA: 21s - loss: 0.5345 - acc: 0.78 - ETA: 20s - loss: 0.5403 - acc: 0.78 - ETA: 19s - loss: 0.5395 - acc: 0.78 - ETA: 19s - loss: 0.5328 - acc: 0.78 - ETA: 18s - loss: 0.5327 - acc: 0.78 - ETA: 17s - loss: 0.5282 - acc: 0.78 - ETA: 17s - loss: 0.5258 - acc: 0.78 - ETA: 16s - loss: 0.5248 - acc: 0.78 - ETA: 15s - loss: 0.5194 - acc: 0.78 - ETA: 15s - loss: 0.5155 - acc: 0.78 - ETA: 14s - loss: 0.5139 - acc: 0.78 - ETA: 13s - loss: 0.5132 - acc: 0.78 - ETA: 13s - loss: 0.5105 - acc: 0.78 - ETA: 12s - loss: 0.5089 - acc: 0.78 - ETA: 11s - loss: 0.5099 - acc: 0.78 - ETA: 11s - loss: 0.5115 - acc: 0.78 - ETA: 10s - loss: 0.5104 - acc: 0.78 - ETA: 9s - loss: 0.5093 - acc: 0.7839 - ETA: 8s - loss: 0.5077 - acc: 0.783 - ETA: 8s - loss: 0.5078 - acc: 0.785 - ETA: 7s - loss: 0.5109 - acc: 0.785 - ETA: 6s - loss: 0.5085 - acc: 0.786 - ETA: 6s - loss: 0.5105 - acc: 0.785 - ETA: 5s - loss: 0.5106 - acc: 0.785 - ETA: 4s - loss: 0.5083 - acc: 0.787 - ETA: 3s - loss: 0.5082 - acc: 0.788 - ETA: 3s - loss: 0.5058 - acc: 0.789 - ETA: 2s - loss: 0.5035 - acc: 0.790 - ETA: 1s - loss: 0.5004 - acc: 0.790 - ETA: 1s - loss: 0.5016 - acc: 0.790 - ETA: 0s - loss: 0.4991 - acc: 0.790 - 45s 23ms/step - loss: 0.4998 - acc: 0.7903 - val_loss: 0.3387 - val_acc: 0.8611\n",
      "Epoch 9/20\n",
      "1936/1936 [==============================] - ETA: 43s - loss: 0.2421 - acc: 0.93 - ETA: 43s - loss: 0.2256 - acc: 0.92 - ETA: 42s - loss: 0.3252 - acc: 0.87 - ETA: 42s - loss: 0.2997 - acc: 0.89 - ETA: 41s - loss: 0.3155 - acc: 0.90 - ETA: 40s - loss: 0.3488 - acc: 0.90 - ETA: 39s - loss: 0.3669 - acc: 0.89 - ETA: 39s - loss: 0.4016 - acc: 0.87 - ETA: 38s - loss: 0.4152 - acc: 0.86 - ETA: 37s - loss: 0.4124 - acc: 0.86 - ETA: 36s - loss: 0.4015 - acc: 0.86 - ETA: 36s - loss: 0.4209 - acc: 0.86 - ETA: 35s - loss: 0.4228 - acc: 0.85 - ETA: 34s - loss: 0.4198 - acc: 0.85 - ETA: 33s - loss: 0.4296 - acc: 0.85 - ETA: 32s - loss: 0.4252 - acc: 0.85 - ETA: 31s - loss: 0.4438 - acc: 0.84 - ETA: 31s - loss: 0.4571 - acc: 0.83 - ETA: 30s - loss: 0.4595 - acc: 0.83 - ETA: 29s - loss: 0.4563 - acc: 0.83 - ETA: 28s - loss: 0.4564 - acc: 0.83 - ETA: 27s - loss: 0.4525 - acc: 0.83 - ETA: 27s - loss: 0.4607 - acc: 0.82 - ETA: 26s - loss: 0.4536 - acc: 0.83 - ETA: 25s - loss: 0.4562 - acc: 0.83 - ETA: 24s - loss: 0.4581 - acc: 0.82 - ETA: 24s - loss: 0.4544 - acc: 0.82 - ETA: 23s - loss: 0.4511 - acc: 0.82 - ETA: 22s - loss: 0.4526 - acc: 0.82 - ETA: 21s - loss: 0.4540 - acc: 0.82 - ETA: 21s - loss: 0.4518 - acc: 0.82 - ETA: 20s - loss: 0.4510 - acc: 0.82 - ETA: 19s - loss: 0.4436 - acc: 0.82 - ETA: 18s - loss: 0.4497 - acc: 0.82 - ETA: 18s - loss: 0.4473 - acc: 0.82 - ETA: 17s - loss: 0.4478 - acc: 0.82 - ETA: 16s - loss: 0.4439 - acc: 0.82 - ETA: 15s - loss: 0.4443 - acc: 0.82 - ETA: 15s - loss: 0.4440 - acc: 0.82 - ETA: 14s - loss: 0.4473 - acc: 0.81 - ETA: 13s - loss: 0.4517 - acc: 0.81 - ETA: 13s - loss: 0.4519 - acc: 0.81 - ETA: 12s - loss: 0.4609 - acc: 0.81 - ETA: 11s - loss: 0.4582 - acc: 0.81 - ETA: 10s - loss: 0.4523 - acc: 0.81 - ETA: 10s - loss: 0.4521 - acc: 0.81 - ETA: 9s - loss: 0.4465 - acc: 0.8211 - ETA: 8s - loss: 0.4463 - acc: 0.819 - ETA: 8s - loss: 0.4451 - acc: 0.821 - ETA: 7s - loss: 0.4430 - acc: 0.821 - ETA: 6s - loss: 0.4455 - acc: 0.821 - ETA: 6s - loss: 0.4478 - acc: 0.819 - ETA: 5s - loss: 0.4457 - acc: 0.821 - ETA: 4s - loss: 0.4448 - acc: 0.820 - ETA: 3s - loss: 0.4428 - acc: 0.819 - ETA: 3s - loss: 0.4416 - acc: 0.820 - ETA: 2s - loss: 0.4395 - acc: 0.822 - ETA: 1s - loss: 0.4361 - acc: 0.824 - ETA: 1s - loss: 0.4355 - acc: 0.823 - ETA: 0s - loss: 0.4355 - acc: 0.824 - 44s 23ms/step - loss: 0.4362 - acc: 0.8233 - val_loss: 0.2715 - val_acc: 0.8750\n",
      "Epoch 10/20\n",
      "1936/1936 [==============================] - ETA: 42s - loss: 0.5071 - acc: 0.78 - ETA: 41s - loss: 0.4274 - acc: 0.82 - ETA: 43s - loss: 0.4059 - acc: 0.84 - ETA: 43s - loss: 0.4284 - acc: 0.83 - ETA: 44s - loss: 0.4282 - acc: 0.83 - ETA: 44s - loss: 0.4589 - acc: 0.81 - ETA: 43s - loss: 0.4531 - acc: 0.82 - ETA: 43s - loss: 0.4355 - acc: 0.81 - ETA: 41s - loss: 0.4343 - acc: 0.81 - ETA: 40s - loss: 0.4398 - acc: 0.81 - ETA: 39s - loss: 0.4466 - acc: 0.82 - ETA: 38s - loss: 0.4435 - acc: 0.81 - ETA: 37s - loss: 0.4372 - acc: 0.82 - ETA: 36s - loss: 0.4300 - acc: 0.82 - ETA: 35s - loss: 0.4225 - acc: 0.82 - ETA: 34s - loss: 0.4229 - acc: 0.83 - ETA: 33s - loss: 0.4168 - acc: 0.83 - ETA: 32s - loss: 0.4214 - acc: 0.83 - ETA: 32s - loss: 0.4240 - acc: 0.83 - ETA: 31s - loss: 0.4233 - acc: 0.83 - ETA: 31s - loss: 0.4170 - acc: 0.83 - ETA: 30s - loss: 0.4199 - acc: 0.83 - ETA: 29s - loss: 0.4198 - acc: 0.83 - ETA: 28s - loss: 0.4171 - acc: 0.83 - ETA: 27s - loss: 0.4156 - acc: 0.83 - ETA: 27s - loss: 0.4131 - acc: 0.84 - ETA: 26s - loss: 0.4074 - acc: 0.84 - ETA: 25s - loss: 0.4147 - acc: 0.83 - ETA: 24s - loss: 0.4163 - acc: 0.83 - ETA: 23s - loss: 0.4136 - acc: 0.84 - ETA: 22s - loss: 0.4112 - acc: 0.84 - ETA: 22s - loss: 0.4125 - acc: 0.84 - ETA: 21s - loss: 0.4120 - acc: 0.84 - ETA: 20s - loss: 0.4080 - acc: 0.84 - ETA: 19s - loss: 0.4098 - acc: 0.84 - ETA: 19s - loss: 0.4077 - acc: 0.84 - ETA: 18s - loss: 0.4066 - acc: 0.84 - ETA: 17s - loss: 0.4068 - acc: 0.84 - ETA: 17s - loss: 0.4186 - acc: 0.84 - ETA: 16s - loss: 0.4173 - acc: 0.84 - ETA: 15s - loss: 0.4130 - acc: 0.84 - ETA: 14s - loss: 0.4120 - acc: 0.83 - ETA: 14s - loss: 0.4101 - acc: 0.83 - ETA: 13s - loss: 0.4171 - acc: 0.83 - ETA: 12s - loss: 0.4146 - acc: 0.83 - ETA: 11s - loss: 0.4104 - acc: 0.83 - ETA: 10s - loss: 0.4081 - acc: 0.84 - ETA: 10s - loss: 0.4074 - acc: 0.84 - ETA: 9s - loss: 0.4076 - acc: 0.8412 - ETA: 8s - loss: 0.4099 - acc: 0.838 - ETA: 7s - loss: 0.4069 - acc: 0.840 - ETA: 6s - loss: 0.4123 - acc: 0.836 - ETA: 6s - loss: 0.4107 - acc: 0.837 - ETA: 5s - loss: 0.4205 - acc: 0.833 - ETA: 4s - loss: 0.4202 - acc: 0.834 - ETA: 3s - loss: 0.4186 - acc: 0.834 - ETA: 2s - loss: 0.4176 - acc: 0.833 - ETA: 2s - loss: 0.4169 - acc: 0.834 - ETA: 1s - loss: 0.4163 - acc: 0.833 - ETA: 0s - loss: 0.4140 - acc: 0.832 - 51s 27ms/step - loss: 0.4122 - acc: 0.8332 - val_loss: 0.3014 - val_acc: 0.8750\n",
      "Epoch 11/20\n",
      "1936/1936 [==============================] - ETA: 47s - loss: 0.3960 - acc: 0.81 - ETA: 45s - loss: 0.3689 - acc: 0.84 - ETA: 44s - loss: 0.3443 - acc: 0.87 - ETA: 43s - loss: 0.3365 - acc: 0.85 - ETA: 43s - loss: 0.3301 - acc: 0.85 - ETA: 42s - loss: 0.3203 - acc: 0.85 - ETA: 41s - loss: 0.3229 - acc: 0.86 - ETA: 40s - loss: 0.3282 - acc: 0.86 - ETA: 39s - loss: 0.3212 - acc: 0.86 - ETA: 39s - loss: 0.3400 - acc: 0.86 - ETA: 38s - loss: 0.3377 - acc: 0.86 - ETA: 37s - loss: 0.4577 - acc: 0.84 - ETA: 36s - loss: 0.4376 - acc: 0.85 - ETA: 35s - loss: 0.4239 - acc: 0.85 - ETA: 35s - loss: 0.4405 - acc: 0.84 - ETA: 34s - loss: 0.4441 - acc: 0.83 - ETA: 33s - loss: 0.4426 - acc: 0.83 - ETA: 32s - loss: 0.4350 - acc: 0.83 - ETA: 31s - loss: 0.4344 - acc: 0.83 - ETA: 31s - loss: 0.4381 - acc: 0.82 - ETA: 30s - loss: 0.4437 - acc: 0.82 - ETA: 30s - loss: 0.4381 - acc: 0.82 - ETA: 29s - loss: 0.4312 - acc: 0.83 - ETA: 28s - loss: 0.4255 - acc: 0.83 - ETA: 27s - loss: 0.4191 - acc: 0.83 - ETA: 26s - loss: 0.4213 - acc: 0.83 - ETA: 26s - loss: 0.4245 - acc: 0.83 - ETA: 25s - loss: 0.4164 - acc: 0.83 - ETA: 24s - loss: 0.4156 - acc: 0.83 - ETA: 23s - loss: 0.4115 - acc: 0.83 - ETA: 22s - loss: 0.4137 - acc: 0.83 - ETA: 22s - loss: 0.4135 - acc: 0.83 - ETA: 21s - loss: 0.4134 - acc: 0.83 - ETA: 20s - loss: 0.4089 - acc: 0.83 - ETA: 19s - loss: 0.4103 - acc: 0.83 - ETA: 18s - loss: 0.4067 - acc: 0.83 - ETA: 18s - loss: 0.4041 - acc: 0.83 - ETA: 17s - loss: 0.4020 - acc: 0.83 - ETA: 16s - loss: 0.3944 - acc: 0.84 - ETA: 16s - loss: 0.3893 - acc: 0.84 - ETA: 15s - loss: 0.3879 - acc: 0.84 - ETA: 14s - loss: 0.3869 - acc: 0.84 - ETA: 13s - loss: 0.3901 - acc: 0.84 - ETA: 13s - loss: 0.3909 - acc: 0.84 - ETA: 12s - loss: 0.3895 - acc: 0.84 - ETA: 11s - loss: 0.3955 - acc: 0.84 - ETA: 10s - loss: 0.3952 - acc: 0.84 - ETA: 9s - loss: 0.3949 - acc: 0.8444 - ETA: 9s - loss: 0.3949 - acc: 0.845 - ETA: 8s - loss: 0.3923 - acc: 0.845 - ETA: 7s - loss: 0.3927 - acc: 0.842 - ETA: 6s - loss: 0.3955 - acc: 0.841 - ETA: 5s - loss: 0.3966 - acc: 0.842 - ETA: 5s - loss: 0.3953 - acc: 0.842 - ETA: 4s - loss: 0.3945 - acc: 0.842 - ETA: 3s - loss: 0.3903 - acc: 0.843 - ETA: 2s - loss: 0.3933 - acc: 0.840 - ETA: 1s - loss: 0.3908 - acc: 0.841 - ETA: 1s - loss: 0.3932 - acc: 0.842 - ETA: 0s - loss: 0.3913 - acc: 0.843 - 48s 25ms/step - loss: 0.3914 - acc: 0.8430 - val_loss: 0.2651 - val_acc: 0.8889\n",
      "Epoch 12/20\n",
      "1936/1936 [==============================] - ETA: 42s - loss: 0.4780 - acc: 0.78 - ETA: 40s - loss: 0.4738 - acc: 0.78 - ETA: 39s - loss: 0.4152 - acc: 0.84 - ETA: 39s - loss: 0.4019 - acc: 0.84 - ETA: 38s - loss: 0.4019 - acc: 0.85 - ETA: 38s - loss: 0.3679 - acc: 0.85 - ETA: 37s - loss: 0.3693 - acc: 0.86 - ETA: 37s - loss: 0.3476 - acc: 0.87 - ETA: 36s - loss: 0.3729 - acc: 0.87 - ETA: 36s - loss: 0.3722 - acc: 0.88 - ETA: 35s - loss: 0.3734 - acc: 0.87 - ETA: 34s - loss: 0.3604 - acc: 0.88 - ETA: 33s - loss: 0.3487 - acc: 0.88 - ETA: 33s - loss: 0.3386 - acc: 0.88 - ETA: 32s - loss: 0.3385 - acc: 0.88 - ETA: 31s - loss: 0.3479 - acc: 0.88 - ETA: 31s - loss: 0.3536 - acc: 0.88 - ETA: 30s - loss: 0.3405 - acc: 0.88 - ETA: 29s - loss: 0.3381 - acc: 0.88 - ETA: 29s - loss: 0.3402 - acc: 0.87 - ETA: 28s - loss: 0.3346 - acc: 0.88 - ETA: 27s - loss: 0.3468 - acc: 0.87 - ETA: 26s - loss: 0.3554 - acc: 0.87 - ETA: 26s - loss: 0.3543 - acc: 0.86 - ETA: 25s - loss: 0.3543 - acc: 0.87 - ETA: 24s - loss: 0.3524 - acc: 0.87 - ETA: 23s - loss: 0.3462 - acc: 0.87 - ETA: 23s - loss: 0.3541 - acc: 0.87 - ETA: 22s - loss: 0.3513 - acc: 0.87 - ETA: 21s - loss: 0.3560 - acc: 0.87 - ETA: 21s - loss: 0.3562 - acc: 0.87 - ETA: 20s - loss: 0.3566 - acc: 0.87 - ETA: 19s - loss: 0.3529 - acc: 0.87 - ETA: 18s - loss: 0.3537 - acc: 0.87 - ETA: 18s - loss: 0.3524 - acc: 0.87 - ETA: 17s - loss: 0.3565 - acc: 0.87 - ETA: 16s - loss: 0.3589 - acc: 0.87 - ETA: 16s - loss: 0.3587 - acc: 0.87 - ETA: 15s - loss: 0.3584 - acc: 0.87 - ETA: 14s - loss: 0.3553 - acc: 0.87 - ETA: 13s - loss: 0.3570 - acc: 0.87 - ETA: 13s - loss: 0.3599 - acc: 0.87 - ETA: 12s - loss: 0.3681 - acc: 0.87 - ETA: 11s - loss: 0.3632 - acc: 0.87 - ETA: 11s - loss: 0.3652 - acc: 0.87 - ETA: 10s - loss: 0.3663 - acc: 0.87 - ETA: 9s - loss: 0.3694 - acc: 0.8750 - ETA: 8s - loss: 0.3687 - acc: 0.875 - ETA: 8s - loss: 0.3665 - acc: 0.876 - ETA: 7s - loss: 0.3644 - acc: 0.878 - ETA: 6s - loss: 0.3622 - acc: 0.878 - ETA: 6s - loss: 0.3712 - acc: 0.876 - ETA: 5s - loss: 0.3707 - acc: 0.877 - ETA: 4s - loss: 0.3687 - acc: 0.878 - ETA: 3s - loss: 0.3687 - acc: 0.877 - ETA: 3s - loss: 0.3701 - acc: 0.876 - ETA: 2s - loss: 0.3698 - acc: 0.875 - ETA: 1s - loss: 0.3705 - acc: 0.875 - ETA: 1s - loss: 0.3673 - acc: 0.876 - ETA: 0s - loss: 0.3690 - acc: 0.875 - 45s 23ms/step - loss: 0.3674 - acc: 0.8760 - val_loss: 0.3382 - val_acc: 0.8843\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936/1936 [==============================] - ETA: 41s - loss: 0.1993 - acc: 0.90 - ETA: 43s - loss: 0.3153 - acc: 0.90 - ETA: 44s - loss: 0.2614 - acc: 0.91 - ETA: 43s - loss: 0.2633 - acc: 0.91 - ETA: 42s - loss: 0.2599 - acc: 0.91 - ETA: 41s - loss: 0.2439 - acc: 0.91 - ETA: 41s - loss: 0.2773 - acc: 0.91 - ETA: 41s - loss: 0.2773 - acc: 0.91 - ETA: 40s - loss: 0.2884 - acc: 0.89 - ETA: 39s - loss: 0.2948 - acc: 0.88 - ETA: 38s - loss: 0.3080 - acc: 0.87 - ETA: 37s - loss: 0.2973 - acc: 0.88 - ETA: 36s - loss: 0.2934 - acc: 0.88 - ETA: 35s - loss: 0.2905 - acc: 0.88 - ETA: 34s - loss: 0.3317 - acc: 0.87 - ETA: 33s - loss: 0.3216 - acc: 0.88 - ETA: 32s - loss: 0.3226 - acc: 0.88 - ETA: 32s - loss: 0.3217 - acc: 0.88 - ETA: 31s - loss: 0.3188 - acc: 0.88 - ETA: 30s - loss: 0.3260 - acc: 0.88 - ETA: 29s - loss: 0.3320 - acc: 0.87 - ETA: 29s - loss: 0.3377 - acc: 0.88 - ETA: 28s - loss: 0.3365 - acc: 0.88 - ETA: 27s - loss: 0.3311 - acc: 0.88 - ETA: 26s - loss: 0.3287 - acc: 0.88 - ETA: 26s - loss: 0.3294 - acc: 0.88 - ETA: 25s - loss: 0.3317 - acc: 0.88 - ETA: 24s - loss: 0.3333 - acc: 0.88 - ETA: 23s - loss: 0.3319 - acc: 0.88 - ETA: 22s - loss: 0.3315 - acc: 0.88 - ETA: 22s - loss: 0.3257 - acc: 0.89 - ETA: 21s - loss: 0.3258 - acc: 0.89 - ETA: 20s - loss: 0.3268 - acc: 0.89 - ETA: 19s - loss: 0.3228 - acc: 0.89 - ETA: 19s - loss: 0.3228 - acc: 0.89 - ETA: 18s - loss: 0.3190 - acc: 0.89 - ETA: 17s - loss: 0.3174 - acc: 0.89 - ETA: 16s - loss: 0.3168 - acc: 0.90 - ETA: 15s - loss: 0.3137 - acc: 0.90 - ETA: 15s - loss: 0.3134 - acc: 0.89 - ETA: 14s - loss: 0.3121 - acc: 0.90 - ETA: 13s - loss: 0.3128 - acc: 0.90 - ETA: 12s - loss: 0.3144 - acc: 0.90 - ETA: 12s - loss: 0.3140 - acc: 0.90 - ETA: 11s - loss: 0.3140 - acc: 0.90 - ETA: 10s - loss: 0.3139 - acc: 0.90 - ETA: 9s - loss: 0.3153 - acc: 0.9003 - ETA: 9s - loss: 0.3163 - acc: 0.901 - ETA: 8s - loss: 0.3150 - acc: 0.901 - ETA: 7s - loss: 0.3189 - acc: 0.901 - ETA: 7s - loss: 0.3168 - acc: 0.903 - ETA: 6s - loss: 0.3142 - acc: 0.904 - ETA: 5s - loss: 0.3141 - acc: 0.905 - ETA: 4s - loss: 0.3151 - acc: 0.905 - ETA: 4s - loss: 0.3141 - acc: 0.905 - ETA: 3s - loss: 0.3138 - acc: 0.905 - ETA: 2s - loss: 0.3193 - acc: 0.903 - ETA: 1s - loss: 0.3207 - acc: 0.903 - ETA: 1s - loss: 0.3251 - acc: 0.900 - ETA: 0s - loss: 0.3234 - acc: 0.900 - 46s 24ms/step - loss: 0.3226 - acc: 0.9013 - val_loss: 0.3389 - val_acc: 0.8935\n",
      "Epoch 14/20\n",
      "1936/1936 [==============================] - ETA: 47s - loss: 0.1783 - acc: 0.96 - ETA: 45s - loss: 0.2371 - acc: 0.93 - ETA: 45s - loss: 0.2416 - acc: 0.93 - ETA: 43s - loss: 0.2818 - acc: 0.92 - ETA: 42s - loss: 0.3240 - acc: 0.91 - ETA: 41s - loss: 0.3238 - acc: 0.92 - ETA: 41s - loss: 0.3113 - acc: 0.92 - ETA: 40s - loss: 0.3026 - acc: 0.92 - ETA: 39s - loss: 0.2988 - acc: 0.93 - ETA: 39s - loss: 0.2974 - acc: 0.93 - ETA: 38s - loss: 0.2957 - acc: 0.93 - ETA: 38s - loss: 0.2844 - acc: 0.94 - ETA: 37s - loss: 0.2941 - acc: 0.93 - ETA: 37s - loss: 0.2828 - acc: 0.93 - ETA: 36s - loss: 0.2796 - acc: 0.93 - ETA: 35s - loss: 0.2948 - acc: 0.93 - ETA: 34s - loss: 0.2940 - acc: 0.93 - ETA: 34s - loss: 0.2920 - acc: 0.93 - ETA: 33s - loss: 0.2833 - acc: 0.93 - ETA: 32s - loss: 0.2817 - acc: 0.93 - ETA: 31s - loss: 0.2752 - acc: 0.94 - ETA: 30s - loss: 0.2678 - acc: 0.94 - ETA: 29s - loss: 0.2636 - acc: 0.94 - ETA: 28s - loss: 0.2578 - acc: 0.94 - ETA: 27s - loss: 0.2558 - acc: 0.94 - ETA: 26s - loss: 0.2576 - acc: 0.94 - ETA: 26s - loss: 0.2534 - acc: 0.94 - ETA: 25s - loss: 0.2503 - acc: 0.94 - ETA: 24s - loss: 0.2482 - acc: 0.94 - ETA: 23s - loss: 0.2461 - acc: 0.95 - ETA: 22s - loss: 0.2529 - acc: 0.94 - ETA: 22s - loss: 0.2618 - acc: 0.94 - ETA: 21s - loss: 0.2641 - acc: 0.94 - ETA: 20s - loss: 0.2598 - acc: 0.94 - ETA: 19s - loss: 0.2602 - acc: 0.94 - ETA: 19s - loss: 0.2611 - acc: 0.94 - ETA: 18s - loss: 0.2680 - acc: 0.94 - ETA: 17s - loss: 0.2653 - acc: 0.94 - ETA: 16s - loss: 0.2624 - acc: 0.94 - ETA: 15s - loss: 0.2615 - acc: 0.94 - ETA: 15s - loss: 0.2620 - acc: 0.94 - ETA: 14s - loss: 0.2589 - acc: 0.94 - ETA: 13s - loss: 0.2600 - acc: 0.94 - ETA: 12s - loss: 0.2624 - acc: 0.94 - ETA: 11s - loss: 0.2636 - acc: 0.94 - ETA: 11s - loss: 0.2664 - acc: 0.94 - ETA: 10s - loss: 0.2657 - acc: 0.94 - ETA: 9s - loss: 0.2641 - acc: 0.9421 - ETA: 8s - loss: 0.2667 - acc: 0.941 - ETA: 7s - loss: 0.2678 - acc: 0.940 - ETA: 7s - loss: 0.2643 - acc: 0.941 - ETA: 6s - loss: 0.2622 - acc: 0.941 - ETA: 5s - loss: 0.2611 - acc: 0.942 - ETA: 4s - loss: 0.2619 - acc: 0.941 - ETA: 4s - loss: 0.2611 - acc: 0.941 - ETA: 3s - loss: 0.2646 - acc: 0.940 - ETA: 2s - loss: 0.2619 - acc: 0.941 - ETA: 1s - loss: 0.2592 - acc: 0.941 - ETA: 1s - loss: 0.2593 - acc: 0.941 - ETA: 0s - loss: 0.2567 - acc: 0.942 - 47s 24ms/step - loss: 0.2553 - acc: 0.9432 - val_loss: 0.4086 - val_acc: 0.8935\n",
      "Epoch 15/20\n",
      "1936/1936 [==============================] - ETA: 41s - loss: 0.2710 - acc: 0.93 - ETA: 40s - loss: 0.2743 - acc: 0.93 - ETA: 39s - loss: 0.2358 - acc: 0.95 - ETA: 39s - loss: 0.2182 - acc: 0.96 - ETA: 38s - loss: 0.2182 - acc: 0.95 - ETA: 37s - loss: 0.2326 - acc: 0.94 - ETA: 36s - loss: 0.2315 - acc: 0.94 - ETA: 35s - loss: 0.2087 - acc: 0.94 - ETA: 35s - loss: 0.2192 - acc: 0.94 - ETA: 34s - loss: 0.2554 - acc: 0.94 - ETA: 33s - loss: 0.2577 - acc: 0.94 - ETA: 33s - loss: 0.2529 - acc: 0.94 - ETA: 32s - loss: 0.2511 - acc: 0.94 - ETA: 32s - loss: 0.2436 - acc: 0.94 - ETA: 31s - loss: 0.2395 - acc: 0.94 - ETA: 30s - loss: 0.2377 - acc: 0.94 - ETA: 30s - loss: 0.2364 - acc: 0.94 - ETA: 29s - loss: 0.2343 - acc: 0.94 - ETA: 28s - loss: 0.2283 - acc: 0.94 - ETA: 28s - loss: 0.2258 - acc: 0.95 - ETA: 27s - loss: 0.2274 - acc: 0.94 - ETA: 27s - loss: 0.2259 - acc: 0.95 - ETA: 26s - loss: 0.2198 - acc: 0.95 - ETA: 25s - loss: 0.2142 - acc: 0.95 - ETA: 25s - loss: 0.2121 - acc: 0.95 - ETA: 24s - loss: 0.2132 - acc: 0.95 - ETA: 23s - loss: 0.2117 - acc: 0.95 - ETA: 23s - loss: 0.2118 - acc: 0.95 - ETA: 22s - loss: 0.2136 - acc: 0.95 - ETA: 21s - loss: 0.2157 - acc: 0.95 - ETA: 20s - loss: 0.2116 - acc: 0.95 - ETA: 20s - loss: 0.2175 - acc: 0.95 - ETA: 19s - loss: 0.2209 - acc: 0.95 - ETA: 18s - loss: 0.2310 - acc: 0.94 - ETA: 18s - loss: 0.2263 - acc: 0.95 - ETA: 17s - loss: 0.2225 - acc: 0.95 - ETA: 16s - loss: 0.2196 - acc: 0.95 - ETA: 16s - loss: 0.2226 - acc: 0.95 - ETA: 15s - loss: 0.2235 - acc: 0.95 - ETA: 14s - loss: 0.2203 - acc: 0.95 - ETA: 14s - loss: 0.2192 - acc: 0.95 - ETA: 13s - loss: 0.2184 - acc: 0.95 - ETA: 12s - loss: 0.2189 - acc: 0.95 - ETA: 11s - loss: 0.2176 - acc: 0.95 - ETA: 11s - loss: 0.2169 - acc: 0.95 - ETA: 10s - loss: 0.2204 - acc: 0.95 - ETA: 9s - loss: 0.2189 - acc: 0.9528 - ETA: 9s - loss: 0.2191 - acc: 0.953 - ETA: 8s - loss: 0.2243 - acc: 0.950 - ETA: 7s - loss: 0.2272 - acc: 0.950 - ETA: 6s - loss: 0.2266 - acc: 0.951 - ETA: 6s - loss: 0.2258 - acc: 0.951 - ETA: 5s - loss: 0.2271 - acc: 0.949 - ETA: 4s - loss: 0.2242 - acc: 0.950 - ETA: 3s - loss: 0.2279 - acc: 0.949 - ETA: 3s - loss: 0.2274 - acc: 0.949 - ETA: 2s - loss: 0.2261 - acc: 0.949 - ETA: 1s - loss: 0.2237 - acc: 0.950 - ETA: 1s - loss: 0.2237 - acc: 0.950 - ETA: 0s - loss: 0.2209 - acc: 0.951 - 45s 23ms/step - loss: 0.2197 - acc: 0.9520 - val_loss: 0.3774 - val_acc: 0.8889\n",
      "Epoch 16/20\n",
      "1936/1936 [==============================] - ETA: 52s - loss: 0.1350 - acc: 1.00 - ETA: 47s - loss: 0.0967 - acc: 1.00 - ETA: 45s - loss: 0.0998 - acc: 1.00 - ETA: 46s - loss: 0.0955 - acc: 1.00 - ETA: 46s - loss: 0.0930 - acc: 1.00 - ETA: 45s - loss: 0.1620 - acc: 0.98 - ETA: 45s - loss: 0.1512 - acc: 0.98 - ETA: 44s - loss: 0.1614 - acc: 0.98 - ETA: 43s - loss: 0.1638 - acc: 0.97 - ETA: 42s - loss: 0.1811 - acc: 0.97 - ETA: 41s - loss: 0.1931 - acc: 0.96 - ETA: 40s - loss: 0.1883 - acc: 0.96 - ETA: 40s - loss: 0.1798 - acc: 0.97 - ETA: 39s - loss: 0.1790 - acc: 0.96 - ETA: 38s - loss: 0.1803 - acc: 0.96 - ETA: 37s - loss: 0.1885 - acc: 0.96 - ETA: 37s - loss: 0.1917 - acc: 0.96 - ETA: 36s - loss: 0.1923 - acc: 0.96 - ETA: 35s - loss: 0.1894 - acc: 0.96 - ETA: 34s - loss: 0.1994 - acc: 0.96 - ETA: 33s - loss: 0.1931 - acc: 0.96 - ETA: 32s - loss: 0.1989 - acc: 0.96 - ETA: 31s - loss: 0.1997 - acc: 0.96 - ETA: 30s - loss: 0.1966 - acc: 0.96 - ETA: 29s - loss: 0.1966 - acc: 0.96 - ETA: 29s - loss: 0.1937 - acc: 0.96 - ETA: 28s - loss: 0.1905 - acc: 0.96 - ETA: 27s - loss: 0.1885 - acc: 0.96 - ETA: 26s - loss: 0.1845 - acc: 0.96 - ETA: 25s - loss: 0.1810 - acc: 0.96 - ETA: 24s - loss: 0.1783 - acc: 0.96 - ETA: 24s - loss: 0.1748 - acc: 0.96 - ETA: 23s - loss: 0.1725 - acc: 0.96 - ETA: 22s - loss: 0.1740 - acc: 0.96 - ETA: 21s - loss: 0.1736 - acc: 0.96 - ETA: 20s - loss: 0.1818 - acc: 0.96 - ETA: 19s - loss: 0.1890 - acc: 0.96 - ETA: 18s - loss: 0.1867 - acc: 0.96 - ETA: 18s - loss: 0.1906 - acc: 0.96 - ETA: 17s - loss: 0.1925 - acc: 0.96 - ETA: 16s - loss: 0.1896 - acc: 0.96 - ETA: 15s - loss: 0.1923 - acc: 0.96 - ETA: 14s - loss: 0.1947 - acc: 0.96 - ETA: 13s - loss: 0.1928 - acc: 0.96 - ETA: 13s - loss: 0.1912 - acc: 0.96 - ETA: 12s - loss: 0.1929 - acc: 0.96 - ETA: 11s - loss: 0.1943 - acc: 0.96 - ETA: 10s - loss: 0.1928 - acc: 0.96 - ETA: 9s - loss: 0.1924 - acc: 0.9630 - ETA: 8s - loss: 0.1905 - acc: 0.963 - ETA: 8s - loss: 0.1915 - acc: 0.962 - ETA: 7s - loss: 0.1905 - acc: 0.962 - ETA: 6s - loss: 0.1885 - acc: 0.962 - ETA: 5s - loss: 0.1924 - acc: 0.961 - ETA: 4s - loss: 0.1900 - acc: 0.961 - ETA: 3s - loss: 0.1896 - acc: 0.962 - ETA: 2s - loss: 0.1899 - acc: 0.961 - ETA: 2s - loss: 0.1877 - acc: 0.962 - ETA: 1s - loss: 0.1853 - acc: 0.962 - ETA: 0s - loss: 0.1847 - acc: 0.963 - 53s 27ms/step - loss: 0.1836 - acc: 0.9633 - val_loss: 0.4030 - val_acc: 0.8935\n",
      "Epoch 17/20\n",
      "1936/1936 [==============================] - ETA: 48s - loss: 0.1277 - acc: 0.96 - ETA: 49s - loss: 2.4998 - acc: 0.79 - ETA: 49s - loss: 1.7327 - acc: 0.85 - ETA: 47s - loss: 1.3544 - acc: 0.87 - ETA: 47s - loss: 1.1220 - acc: 0.89 - ETA: 46s - loss: 0.9773 - acc: 0.90 - ETA: 46s - loss: 0.8532 - acc: 0.91 - ETA: 45s - loss: 0.7720 - acc: 0.92 - ETA: 44s - loss: 0.7093 - acc: 0.92 - ETA: 43s - loss: 0.6595 - acc: 0.92 - ETA: 42s - loss: 0.6096 - acc: 0.92 - ETA: 41s - loss: 0.5711 - acc: 0.92 - ETA: 40s - loss: 0.5469 - acc: 0.93 - ETA: 39s - loss: 0.5353 - acc: 0.93 - ETA: 38s - loss: 0.5094 - acc: 0.93 - ETA: 37s - loss: 0.4833 - acc: 0.93 - ETA: 37s - loss: 0.4733 - acc: 0.93 - ETA: 36s - loss: 0.4586 - acc: 0.93 - ETA: 35s - loss: 0.4477 - acc: 0.93 - ETA: 34s - loss: 0.4314 - acc: 0.94 - ETA: 33s - loss: 0.4229 - acc: 0.94 - ETA: 32s - loss: 0.4112 - acc: 0.94 - ETA: 31s - loss: 0.3969 - acc: 0.94 - ETA: 30s - loss: 0.3873 - acc: 0.94 - ETA: 29s - loss: 0.3753 - acc: 0.94 - ETA: 29s - loss: 0.3697 - acc: 0.94 - ETA: 28s - loss: 0.3579 - acc: 0.95 - ETA: 27s - loss: 0.3477 - acc: 0.95 - ETA: 26s - loss: 0.3441 - acc: 0.95 - ETA: 25s - loss: 0.3339 - acc: 0.95 - ETA: 24s - loss: 0.3254 - acc: 0.95 - ETA: 24s - loss: 0.3191 - acc: 0.95 - ETA: 23s - loss: 0.3104 - acc: 0.95 - ETA: 22s - loss: 0.3026 - acc: 0.95 - ETA: 21s - loss: 0.2961 - acc: 0.95 - ETA: 20s - loss: 0.2981 - acc: 0.95 - ETA: 19s - loss: 0.2917 - acc: 0.95 - ETA: 18s - loss: 0.2950 - acc: 0.95 - ETA: 17s - loss: 0.2905 - acc: 0.95 - ETA: 17s - loss: 0.2860 - acc: 0.95 - ETA: 16s - loss: 0.2830 - acc: 0.95 - ETA: 15s - loss: 0.2839 - acc: 0.95 - ETA: 14s - loss: 0.2839 - acc: 0.95 - ETA: 13s - loss: 0.2862 - acc: 0.95 - ETA: 12s - loss: 0.2832 - acc: 0.95 - ETA: 12s - loss: 0.2830 - acc: 0.95 - ETA: 11s - loss: 0.2827 - acc: 0.95 - ETA: 10s - loss: 0.2802 - acc: 0.95 - ETA: 9s - loss: 0.2763 - acc: 0.9554 - ETA: 8s - loss: 0.2719 - acc: 0.956 - ETA: 7s - loss: 0.2676 - acc: 0.957 - ETA: 7s - loss: 0.2629 - acc: 0.957 - ETA: 6s - loss: 0.2588 - acc: 0.958 - ETA: 5s - loss: 0.2598 - acc: 0.957 - ETA: 4s - loss: 0.2575 - acc: 0.958 - ETA: 3s - loss: 0.2576 - acc: 0.957 - ETA: 2s - loss: 0.2538 - acc: 0.958 - ETA: 2s - loss: 0.2506 - acc: 0.959 - ETA: 1s - loss: 0.2498 - acc: 0.959 - ETA: 0s - loss: 0.2487 - acc: 0.958 - 52s 27ms/step - loss: 0.2483 - acc: 0.9587 - val_loss: 0.2980 - val_acc: 0.9028\n",
      "Epoch 18/20\n",
      "1936/1936 [==============================] - ETA: 50s - loss: 0.0372 - acc: 1.00 - ETA: 51s - loss: 0.0405 - acc: 1.00 - ETA: 50s - loss: 0.0714 - acc: 0.98 - ETA: 49s - loss: 0.1177 - acc: 0.97 - ETA: 48s - loss: 0.1041 - acc: 0.98 - ETA: 47s - loss: 0.1102 - acc: 0.97 - ETA: 47s - loss: 0.1289 - acc: 0.96 - ETA: 45s - loss: 0.1509 - acc: 0.96 - ETA: 44s - loss: 0.1514 - acc: 0.96 - ETA: 43s - loss: 0.1408 - acc: 0.96 - ETA: 42s - loss: 0.1429 - acc: 0.96 - ETA: 41s - loss: 0.1354 - acc: 0.96 - ETA: 40s - loss: 0.1283 - acc: 0.97 - ETA: 39s - loss: 0.1364 - acc: 0.96 - ETA: 38s - loss: 0.1329 - acc: 0.96 - ETA: 37s - loss: 0.1326 - acc: 0.96 - ETA: 36s - loss: 0.1256 - acc: 0.97 - ETA: 35s - loss: 0.1207 - acc: 0.97 - ETA: 35s - loss: 0.1158 - acc: 0.97 - ETA: 34s - loss: 0.1134 - acc: 0.97 - ETA: 33s - loss: 0.1229 - acc: 0.97 - ETA: 32s - loss: 0.1241 - acc: 0.97 - ETA: 31s - loss: 0.1246 - acc: 0.97 - ETA: 31s - loss: 0.1248 - acc: 0.97 - ETA: 30s - loss: 0.1239 - acc: 0.97 - ETA: 29s - loss: 0.1228 - acc: 0.97 - ETA: 28s - loss: 0.1272 - acc: 0.97 - ETA: 27s - loss: 0.1249 - acc: 0.97 - ETA: 27s - loss: 0.1213 - acc: 0.97 - ETA: 26s - loss: 0.1226 - acc: 0.97 - ETA: 25s - loss: 0.1213 - acc: 0.97 - ETA: 24s - loss: 0.1260 - acc: 0.96 - ETA: 23s - loss: 0.1893 - acc: 0.95 - ETA: 22s - loss: 0.3234 - acc: 0.94 - ETA: 22s - loss: 0.3838 - acc: 0.92 - ETA: 21s - loss: 0.3771 - acc: 0.92 - ETA: 20s - loss: 0.3860 - acc: 0.92 - ETA: 19s - loss: 0.3869 - acc: 0.92 - ETA: 18s - loss: 0.3918 - acc: 0.92 - ETA: 17s - loss: 0.3837 - acc: 0.92 - ETA: 16s - loss: 0.3825 - acc: 0.92 - ETA: 15s - loss: 0.3812 - acc: 0.92 - ETA: 14s - loss: 0.3774 - acc: 0.92 - ETA: 14s - loss: 0.3749 - acc: 0.92 - ETA: 13s - loss: 0.3694 - acc: 0.92 - ETA: 12s - loss: 0.3764 - acc: 0.92 - ETA: 11s - loss: 0.3699 - acc: 0.92 - ETA: 10s - loss: 0.3715 - acc: 0.92 - ETA: 9s - loss: 0.3705 - acc: 0.9254 - ETA: 8s - loss: 0.3792 - acc: 0.923 - ETA: 8s - loss: 0.3783 - acc: 0.923 - ETA: 7s - loss: 0.3717 - acc: 0.924 - ETA: 6s - loss: 0.3686 - acc: 0.925 - ETA: 5s - loss: 0.3629 - acc: 0.926 - ETA: 4s - loss: 0.3585 - acc: 0.927 - ETA: 3s - loss: 0.3550 - acc: 0.928 - ETA: 2s - loss: 0.3502 - acc: 0.928 - ETA: 2s - loss: 0.3450 - acc: 0.930 - ETA: 1s - loss: 0.3435 - acc: 0.929 - ETA: 0s - loss: 0.3392 - acc: 0.930 - 53s 27ms/step - loss: 0.3368 - acc: 0.9313 - val_loss: 0.2657 - val_acc: 0.8935\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936/1936 [==============================] - ETA: 51s - loss: 0.3417 - acc: 0.90 - ETA: 50s - loss: 0.3591 - acc: 0.89 - ETA: 48s - loss: 0.2616 - acc: 0.91 - ETA: 48s - loss: 0.2505 - acc: 0.92 - ETA: 48s - loss: 0.2057 - acc: 0.93 - ETA: 47s - loss: 0.2234 - acc: 0.92 - ETA: 46s - loss: 0.1981 - acc: 0.93 - ETA: 46s - loss: 0.1779 - acc: 0.94 - ETA: 45s - loss: 0.1877 - acc: 0.93 - ETA: 44s - loss: 0.1898 - acc: 0.93 - ETA: 43s - loss: 0.1791 - acc: 0.94 - ETA: 42s - loss: 0.1686 - acc: 0.94 - ETA: 41s - loss: 0.1653 - acc: 0.94 - ETA: 40s - loss: 0.1554 - acc: 0.95 - ETA: 39s - loss: 0.1469 - acc: 0.95 - ETA: 38s - loss: 0.1409 - acc: 0.95 - ETA: 37s - loss: 0.1346 - acc: 0.95 - ETA: 36s - loss: 0.1287 - acc: 0.96 - ETA: 35s - loss: 0.1288 - acc: 0.96 - ETA: 34s - loss: 0.1241 - acc: 0.96 - ETA: 33s - loss: 0.1351 - acc: 0.96 - ETA: 32s - loss: 0.1361 - acc: 0.96 - ETA: 31s - loss: 0.1344 - acc: 0.96 - ETA: 30s - loss: 0.1325 - acc: 0.96 - ETA: 29s - loss: 0.1319 - acc: 0.96 - ETA: 29s - loss: 0.1318 - acc: 0.96 - ETA: 28s - loss: 0.1283 - acc: 0.96 - ETA: 27s - loss: 0.1290 - acc: 0.96 - ETA: 26s - loss: 0.1251 - acc: 0.96 - ETA: 25s - loss: 0.1241 - acc: 0.96 - ETA: 24s - loss: 0.1244 - acc: 0.96 - ETA: 24s - loss: 0.1239 - acc: 0.96 - ETA: 23s - loss: 0.1237 - acc: 0.96 - ETA: 22s - loss: 0.1258 - acc: 0.96 - ETA: 21s - loss: 0.1281 - acc: 0.96 - ETA: 20s - loss: 0.1320 - acc: 0.96 - ETA: 20s - loss: 0.1376 - acc: 0.96 - ETA: 19s - loss: 0.1375 - acc: 0.96 - ETA: 18s - loss: 0.1367 - acc: 0.96 - ETA: 17s - loss: 0.1428 - acc: 0.96 - ETA: 16s - loss: 0.1403 - acc: 0.96 - ETA: 15s - loss: 0.1422 - acc: 0.96 - ETA: 14s - loss: 0.1432 - acc: 0.96 - ETA: 13s - loss: 0.1405 - acc: 0.96 - ETA: 13s - loss: 0.1390 - acc: 0.96 - ETA: 12s - loss: 0.1393 - acc: 0.96 - ETA: 11s - loss: 0.1379 - acc: 0.96 - ETA: 10s - loss: 0.1428 - acc: 0.96 - ETA: 9s - loss: 0.1418 - acc: 0.9656 - ETA: 8s - loss: 0.1399 - acc: 0.966 - ETA: 8s - loss: 0.1385 - acc: 0.966 - ETA: 7s - loss: 0.1362 - acc: 0.966 - ETA: 6s - loss: 0.1344 - acc: 0.967 - ETA: 5s - loss: 0.1342 - acc: 0.967 - ETA: 4s - loss: 0.1349 - acc: 0.967 - ETA: 3s - loss: 0.1333 - acc: 0.968 - ETA: 2s - loss: 0.1314 - acc: 0.968 - ETA: 2s - loss: 0.1309 - acc: 0.968 - ETA: 1s - loss: 0.1339 - acc: 0.967 - ETA: 0s - loss: 0.1352 - acc: 0.967 - 53s 27ms/step - loss: 0.1345 - acc: 0.9675 - val_loss: 0.3591 - val_acc: 0.8981\n",
      "Epoch 20/20\n",
      "1936/1936 [==============================] - ETA: 52s - loss: 0.2858 - acc: 0.93 - ETA: 52s - loss: 0.1670 - acc: 0.95 - ETA: 49s - loss: 0.1294 - acc: 0.96 - ETA: 48s - loss: 0.1595 - acc: 0.96 - ETA: 46s - loss: 0.1843 - acc: 0.95 - ETA: 45s - loss: 0.1996 - acc: 0.94 - ETA: 44s - loss: 0.1888 - acc: 0.95 - ETA: 44s - loss: 0.1681 - acc: 0.95 - ETA: 43s - loss: 0.1529 - acc: 0.96 - ETA: 43s - loss: 0.1627 - acc: 0.96 - ETA: 42s - loss: 0.1629 - acc: 0.96 - ETA: 41s - loss: 0.1680 - acc: 0.96 - ETA: 40s - loss: 0.1609 - acc: 0.96 - ETA: 39s - loss: 0.1626 - acc: 0.95 - ETA: 38s - loss: 0.1599 - acc: 0.96 - ETA: 38s - loss: 0.1557 - acc: 0.96 - ETA: 37s - loss: 0.1479 - acc: 0.96 - ETA: 36s - loss: 0.1433 - acc: 0.96 - ETA: 35s - loss: 0.1461 - acc: 0.96 - ETA: 34s - loss: 0.1432 - acc: 0.96 - ETA: 33s - loss: 0.1516 - acc: 0.95 - ETA: 32s - loss: 0.1490 - acc: 0.96 - ETA: 31s - loss: 0.1442 - acc: 0.96 - ETA: 30s - loss: 0.1542 - acc: 0.95 - ETA: 29s - loss: 0.1634 - acc: 0.95 - ETA: 28s - loss: 0.1619 - acc: 0.95 - ETA: 27s - loss: 0.1563 - acc: 0.95 - ETA: 26s - loss: 0.1529 - acc: 0.95 - ETA: 26s - loss: 0.1493 - acc: 0.96 - ETA: 25s - loss: 0.1456 - acc: 0.96 - ETA: 24s - loss: 0.1422 - acc: 0.96 - ETA: 23s - loss: 0.1418 - acc: 0.96 - ETA: 23s - loss: 0.1405 - acc: 0.96 - ETA: 22s - loss: 0.1414 - acc: 0.96 - ETA: 21s - loss: 0.1381 - acc: 0.96 - ETA: 20s - loss: 0.1358 - acc: 0.96 - ETA: 19s - loss: 0.1328 - acc: 0.96 - ETA: 19s - loss: 0.1319 - acc: 0.96 - ETA: 18s - loss: 0.1412 - acc: 0.96 - ETA: 17s - loss: 0.1436 - acc: 0.96 - ETA: 16s - loss: 0.1404 - acc: 0.96 - ETA: 15s - loss: 0.1409 - acc: 0.96 - ETA: 14s - loss: 0.1450 - acc: 0.96 - ETA: 13s - loss: 0.1430 - acc: 0.96 - ETA: 13s - loss: 0.1419 - acc: 0.96 - ETA: 12s - loss: 0.1412 - acc: 0.96 - ETA: 11s - loss: 0.1394 - acc: 0.96 - ETA: 10s - loss: 0.1385 - acc: 0.96 - ETA: 9s - loss: 0.1366 - acc: 0.9656 - ETA: 8s - loss: 0.1343 - acc: 0.966 - ETA: 7s - loss: 0.1321 - acc: 0.966 - ETA: 7s - loss: 0.1298 - acc: 0.967 - ETA: 6s - loss: 0.1316 - acc: 0.967 - ETA: 5s - loss: 0.1298 - acc: 0.967 - ETA: 4s - loss: 0.1288 - acc: 0.967 - ETA: 3s - loss: 0.1352 - acc: 0.966 - ETA: 2s - loss: 0.1338 - acc: 0.967 - ETA: 2s - loss: 0.1350 - acc: 0.967 - ETA: 1s - loss: 0.1335 - acc: 0.967 - ETA: 0s - loss: 0.1317 - acc: 0.968 - 53s 27ms/step - loss: 0.1331 - acc: 0.9680 - val_loss: 0.2803 - val_acc: 0.9074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27e94a02668>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m= model()\n",
    "\n",
    "m.fit(sequences_matrix,y,epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(sequences_matrix,y,random_state = 100,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pd = m.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = m.evaluate(sequences_matrix,y,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13451796508541738, 0.962360594795539]\n"
     ]
    }
   ],
   "source": [
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(ytest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from Keras.util import accuracy_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accuracy = accuracy_score(y_pd,ytest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accuracy = c.evaluate(test_sequences_matrix,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "m.save('LeaveModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quer For Leave"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m = load_model('LeaveModel.h5')\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= Dataset['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(Dataset['sub_intent_one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"update me about leave approving authority\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i want leave for today\"\n",
    "query = \"i want sick leaves for 2 days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Approve all leaves on my behalf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how many half days have i availed\"\n",
    "query = \"do i have any absent for last month\"\n",
    "query = \"how many leaves have i availed\"\n",
    "query = \"inform me about how many leaves have i taken\"\n",
    "query = \"is my leave approved\"\n",
    "#INQUIRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'is office off on 9th november'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is leave encashment'\n",
    "#query = 'can you please tell me about leave encashment procedure'\n",
    "#query = 'i want to encash my leaves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i am feeling sick\"\n",
    "query = \"i dont feel i can work today\"\n",
    "query = \"i dont want to come \"\n",
    "query = \"will office remain close tomorrow\"\n",
    "query = 'who is my leave approving authority'\n",
    "#query = 'I wonâ€™t be able to come to office today'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'i want sick leave for today'\n",
    "query = 'Shiza is not feeling well'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = token_stems(sent)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test = ([list(sen)])\n",
    "sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = m.predict(sen_sequences_matrix)\n",
    "print(score)\n",
    "print(max(score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inquiry 3\")\n",
    "print(\"Request 0\")\n",
    "print(\"Emottional 2\")\n",
    "print(\"Approval 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional Leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_leave = Dataset[Dataset['sub_intent_one']=='emotion_LeaveRequest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = em_leave['Data']\n",
    "x= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    x.append(token_stems(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words = max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "max_len =200\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = em_leave['sub_intent_two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "#y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = svm.SVC(kernel = 'rbf',C=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiza.abid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=50, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(sequences_matrix,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(sequences_matrix,y,random_state=100,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pdr = m.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ac=accuracy_score(ytest, y_pdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981651376146789\n"
     ]
    }
   ],
   "source": [
    "print(lr_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'i am not feeling well'\n",
    "test = 'i am going on picnic tomorrow'\n",
    "test = 'i had an accident yesterday'\n",
    "test = 'my grandmother is in hospital'\n",
    "test = 'i am admitted in hospital'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'admit', 'in', 'hospit']\n",
      "['casual']\n"
     ]
    }
   ],
   "source": [
    "user_response = test\n",
    "sen = token_stems(user_response)\n",
    "#sen_test = ([list(sen)])\n",
    "print(sen)\n",
    "sen_sequences = tok.texts_to_sequences([sen])\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)\n",
    "ans = m.predict(sen_sequences_matrix)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_intent = leave(\"i have to go to doctor\") \n",
    "print(leave_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave(name):\n",
    "    sub_intent = leave_module(name)\n",
    "                #for sub_intents\n",
    "    if(sub_intent == 'EmotionlLeave'):\n",
    "        sub_intent_two = emotional_leave(name)\n",
    "                    #return sub_intent_two\n",
    "    elif(sub_intent == 'LeaveApproval'):\n",
    "        sub_intent_two = leave_approval(name)\n",
    "                    #return sub_intent_two\n",
    "    elif(sub_intent == 'LeaveRequest'):\n",
    "        sub_intent_two = classify_leaverequest(name)\n",
    "    else:\n",
    "        sub_intent_two = classify_leaveinquiry(name)\n",
    "    return(sub_intent_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_module(name):\n",
    "        sub_intent = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        docs= train['Data']\n",
    "        tokens = []\n",
    "        for i in docs:\n",
    "            temp = token_stems(i)\n",
    "            tokens.append(temp)\n",
    "\n",
    "        x, y = np.asarray(tokens) , np.asarray(train['Type'])\n",
    "        xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)\n",
    "        max_len=200\n",
    "        max_words = 20000\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "        tok.fit_on_texts(x)\n",
    "        sequences = tok.texts_to_sequences(x)\n",
    "        test_sequences = tok.texts_to_sequences(xtest)\n",
    "\n",
    "        \n",
    "        score = leave_model(name,tok)\n",
    "        print(score)\n",
    "        if((score[0][0] > score [0][1])&(score[0][0] > score [0][2])&(score[0][0] > score [0][3])):\n",
    "            sub_intent = 'LeaveRequest'\n",
    "        elif((score[0][1] > score [0][0])&(score[0][1] > score [0][2])&(score[0][1] > score [0][3])):\n",
    "            sub_intent = 'LeaveApproval'\n",
    "        elif((score[0][2] > score [0][1])&(score[0][2] > score [0][0])&(score[0][2] > score [0][3])):\n",
    "            sub_intent = 'EmotionlLeave'\n",
    "        else:\n",
    "            sub_intent = 'LeaveInquiry'\n",
    "        return sub_intent\n",
    "\n",
    "\n",
    "\n",
    "def leave_model(name,tok):\n",
    "        top_intent = ''\n",
    "        user_response = name\n",
    "        max_len=200\n",
    "        sen = token_stems(user_response)\n",
    "        sen_test = ([list(sen)])\n",
    "        print(sen_test)\n",
    "        sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "        sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)\n",
    "        m = load_model('Leave_Model_final.h5')\n",
    "        score = m.predict(sen_sequences_matrix)\n",
    "        K.clear_session()\n",
    "            #return(\"reach till here\")\n",
    "        print(user_response)\n",
    "        user_response=\"\"\n",
    "        sen=\"\"\n",
    "        sen_test=\"\"\n",
    "        intent=\"\"\n",
    "        Score=\"\"\n",
    "        print(score)\n",
    "        return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def classify_leaverequest(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['Type']== 'LeaveRequest']\n",
    "        text = train['Data']\n",
    "        \n",
    "        half_day = train[train['sub_type_two'] == 'half_day']\n",
    "        leave = train[train['sub_type_two'] == 'leave']\n",
    "        \n",
    "       \n",
    "        gen_half_day = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in half_day['Data']]\n",
    "        gen_leave = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in leave['Data']]\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "        dictionary_half_day = gensim.corpora.Dictionary(gen_half_day)\n",
    "        dictionary_leave = gensim.corpora.Dictionary(gen_leave)\n",
    "       \n",
    "        \n",
    "        corpus_half_day = [dictionary_half_day.doc2bow(gen_half_day) for gen_half_day in gen_half_day]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_half_day  = gensim.models.TfidfModel(corpus_half_day)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_half_day = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\half_day.txt',tf_idf_half_day[corpus_half_day],\n",
    "                                              num_features=len(dictionary_half_day))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_leave = [dictionary_leave.doc2bow(gen_leave) for gen_leave in gen_leave]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave  = gensim.models.TfidfModel(corpus_leave)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave.txt',tf_idf_leave[corpus_leave],\n",
    "                                              num_features=len(dictionary_leave))\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_half_day.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_half_day[query_doc_bow]\n",
    "        half_day = np.max(sims_half_day[query_doc_tf_idf])\n",
    "        print(half_day)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_leave.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave[query_doc_bow]\n",
    "        leave = np.max(sims_leave[query_doc_tf_idf])\n",
    "        print(leave)\n",
    "        \n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "        if(half_day>leave):\n",
    "            return \"half_day\"\n",
    "        else:\n",
    "            return \"leave\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For emotional leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0ebe1131dc9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sub_intent_one'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'emotion_LeaveRequest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data = Dataset[Dataset['sub_intent_one'] == 'emotion_LeaveRequest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave Inquiry Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "train = train[train['Type'] == 'leave_inquiry']\n",
    "#train = train[train['Module'] == 'Leave']\n",
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer= SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    \n",
    "    stems =[stemmer.stem(t) for t in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(text):\n",
    "    \n",
    "    #breaking each word and making them tokens\n",
    "    tokens=[word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #storing only alpha tokens\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        if (re.search('[a-zA-Z]|\\'', token)):\n",
    "            #if token not in stopwords:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stems(text):  \n",
    "    tokens=tokenizing(text) \n",
    "    stems=stemming(tokens)\n",
    "    return stems       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['sub_type_two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)\n",
    "#y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "\n",
    "#test_sequences = tok.texts_to_sequences(xtest)\n",
    "y = train['sub_type_two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(sequences, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=1,kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(sequences,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#test_sequences_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "\n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    #model.add(LSTM(128, input_shape=(xtrain.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #model.add(LSTM(80, input_shape=(xtrain.shape[1:]), activation='tanh', return_sequences=True))\n",
    "    \n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    \n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your model\n",
    "c.save('Leave_Inquiry_Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "train = train[train['Type'] == 'leave_inquiry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m = load_model('Leave_Inquiry_Model.h5')\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['sub_type_two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"update me about leave approving authority\"\n",
    "#query = 'is today a holiday'\n",
    "#query = 'how many leaves have i taken'\n",
    "#query = \"can you please tell me about my leave encashment\"\n",
    "#query = \"is my leave approved\"\n",
    "#query = \"will office open tomorrow\"\n",
    "#query = \"will office remain close tomorrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENRAL LEAVES 1#\n",
    "query = \"is office open today\"\n",
    "query = \"is this saturday on\"\n",
    "query = \"is tomorrow a working day\"\n",
    "query = \"Can you please inform me if today is working day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAVE APPROVING AUTHORITY 2#\n",
    "query = 'who is my leave approving authroity'\n",
    "query = 'kindly update me about my leave approveing authority'\n",
    "query = \"update me about leave approving authority\"\n",
    "query = 'Can you please update me about my leave approving authority'\n",
    "query = \"inform me who will approve my leaves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECIFIC 3#\n",
    "query = 'how many leaves have i taken'\n",
    "query = 'how many leaves can i take more'\n",
    "query = 'how many sick leaves are left of mine'\n",
    "query = 'can you please tell me how many sick leaves can i take more'\n",
    "query = 'update me about how many sick leaves can i take more'\n",
    "query = 'how many leaves has ms.shiza has taken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAVE ENCASHMENT 0#\n",
    "query = \"can you please tell me about leave encashment procedure\"\n",
    "query = \"how much will i recieve when i will encash my leaves\"\n",
    "query = \"what is the procedure of leave encashment\"\n",
    "query = \"can you please tell me the procedure of leave encashment\"\n",
    "query = \"update me with the procedure of leave encashment\"\n",
    "query = \"inform me about the procedure of leave encashment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAVE STATUS 4#\n",
    "query = \"is my leave approved\"\n",
    "query = \"kindly update me about if my leave is approved or not\"\n",
    "query = \"has my sick leave approved\"\n",
    "query = \"inform me about if my sick leave is approved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECIAL CASES\n",
    "query = 'is my leave approved'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = token_stems(sent)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test = ([list(sen)])\n",
    "sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = m.predict(sen_sequences_matrix)\n",
    "print(score)\n",
    "print(max(score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"leave approve authority 2 2\")\n",
    "print('genral 1 0')\n",
    "print('specific 3 3')\n",
    "print('encashment 0 4 0')\n",
    "print('approval 4 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def leave_inquiry_module(name):\n",
    "        sub_intent = ''\n",
    "        train= pd.read_excel(\"leave_final.xlsx\")\n",
    "        train = train[train['Type'] == 'leave_inquiry']\n",
    "        docs= train['Data']\n",
    "        tokens = []\n",
    "        for i in docs:\n",
    "            temp = token_stems(i)\n",
    "            tokens.append(temp)\n",
    "\n",
    "        x, y = np.asarray(tokens) , np.asarray(train['sub_type_two'])\n",
    "        xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)\n",
    "        max_len=200\n",
    "        max_words = 20000\n",
    "        tok = Tokenizer(num_words=max_words)\n",
    "        tok.fit_on_texts(x)\n",
    "        sequences = tok.texts_to_sequences(x)\n",
    "        test_sequences = tok.texts_to_sequences(xtest)\n",
    "\n",
    "        \n",
    "        score = leave_inquiry_model(name,tok)\n",
    "        print(score)\n",
    "        if((score[0][0] > score [0][1])&(score[0][0] > score [0][2])&(score[0][0] > score [0][3])&(score[0][0] > score [0][4])):\n",
    "            sub_intent = 'LeaveEncashment'\n",
    "        elif((score[0][1] > score [0][0])&(score[0][1] > score [0][2])&(score[0][1] > score [0][3])&(score[0][1] > score [0][4])):\n",
    "            sub_intent = 'LeaveGenral'\n",
    "        elif((score[0][2] > score [0][1])&(score[0][2] > score [0][0])&(score[0][2] > score [0][3])&(score[0][2] > score [0][4])):\n",
    "            sub_intent = 'LeaveReporting'\n",
    "        elif((score[0][3] > score [0][1])&(score[0][3] > score [0][0])&(score[0][3] > score [0][2])&(score[0][3] > score [0][4])):\n",
    "            sub_intent = 'Leave_Specific'+',\"' + leave_inquiry_specific(name) + '\"'\n",
    "        else:\n",
    "            sub_intent = 'LeaveApproval'\n",
    "        return sub_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def leave_inquiry_model(name,tok):\n",
    "        top_intent = ''\n",
    "        user_response = name\n",
    "        max_len=200\n",
    "        sen = token_stems(user_response)\n",
    "        sen_test = ([list(sen)])\n",
    "        print(sen_test)\n",
    "        sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "        sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)\n",
    "        m = load_model('Leave_Inquiry_Model.h5')\n",
    "        score = m.predict(sen_sequences_matrix)\n",
    "        K.clear_session()\n",
    "        print(user_response)\n",
    "        user_response=\"\"\n",
    "        sen=\"\"\n",
    "        sen_test=\"\"\n",
    "        intent=\"\"\n",
    "        Score=\"\"\n",
    "        print(score)\n",
    "        return(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave Inquiry Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def classify_leaveinquiry(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['Type']== 'leave_inquiry']\n",
    "        text = train['Data']\n",
    "        \n",
    "        status = train[train['sub_type_two'] == 'status']\n",
    "        specific = train[train['sub_type_two'] == 'specific']\n",
    "        encashment = train[train['sub_type_two'] == 'encashment']\n",
    "        reporting = train[train['sub_type_two'] == 'reporting']\n",
    "        genral = train[train['sub_type_two'] == 'genral']\n",
    "       \n",
    "        gen_status = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in status['Data']]\n",
    "        gen_specific = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in specific['Data']]\n",
    "        gen_encashment = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in encashment['Data']]\n",
    "        gen_reporting = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in reporting['Data']]\n",
    "        gen_genral = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in genral['Data']]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        dictionary_status = gensim.corpora.Dictionary(gen_status)\n",
    "        dictionary_specific = gensim.corpora.Dictionary(gen_specific)\n",
    "        dictionary_encashment = gensim.corpora.Dictionary(gen_encashment)\n",
    "        dictionary_reporting = gensim.corpora.Dictionary(gen_reporting)\n",
    "        dictionary_genral = gensim.corpora.Dictionary(gen_genral)\n",
    "        \n",
    "        corpus_status = [dictionary_status.doc2bow(gen_status) for gen_status in gen_status]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_status  = gensim.models.TfidfModel(corpus_status)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_status = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\status.txt',tf_idf_status[corpus_status],\n",
    "                                              num_features=len(dictionary_status))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_specific = [dictionary_specific.doc2bow(gen_specific) for gen_specific in gen_specific]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_specific  = gensim.models.TfidfModel(corpus_specific)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_specific = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\specific.txt',tf_idf_specific[corpus_specific],\n",
    "                                              num_features=len(dictionary_specific))\n",
    "\n",
    "        \n",
    "        \n",
    "        corpus_encashment = [dictionary_encashment.doc2bow(gen_encashment) for gen_encashment in gen_encashment]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_encashment  = gensim.models.TfidfModel(corpus_encashment)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_encashment = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\encashment.txt',tf_idf_encashment[corpus_encashment],\n",
    "                                              num_features=len(dictionary_encashment))\n",
    "        \n",
    "        \n",
    "        corpus_reporting = [dictionary_reporting.doc2bow(gen_reporting) for gen_reporting in gen_reporting]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_reporting  = gensim.models.TfidfModel(corpus_reporting)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_reporting = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\reporting.txt',tf_idf_reporting[corpus_reporting],\n",
    "                                              num_features=len(dictionary_reporting))\n",
    "        \n",
    "        \n",
    "        corpus_genral = [dictionary_genral.doc2bow(gen_genral) for gen_genral in gen_genral]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_genral  = gensim.models.TfidfModel(corpus_genral)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_genral = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\genral.txt',tf_idf_genral[corpus_genral],\n",
    "                                              num_features=len(dictionary_genral))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_status.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_status[query_doc_bow]\n",
    "        status = np.max(sims_status[query_doc_tf_idf])\n",
    "        print(status)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_specific.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_specific[query_doc_bow]\n",
    "        specific = np.max(sims_specific[query_doc_tf_idf])\n",
    "        print(specific)\n",
    "        \n",
    "        query_doc_bow = dictionary_encashment.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_encashment[query_doc_bow]\n",
    "        encashment = np.max(sims_encashment[query_doc_tf_idf])\n",
    "        print(encashment)\n",
    "        \n",
    "        query_doc_bow = dictionary_reporting.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_reporting[query_doc_bow]\n",
    "        reporting = np.max(sims_reporting[query_doc_tf_idf])\n",
    "        print(reporting)\n",
    "        \n",
    "        query_doc_bow = dictionary_genral.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_genral[query_doc_bow]\n",
    "        genral = np.max(sims_genral[query_doc_tf_idf])\n",
    "        print(genral)\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "        if((status>specific)&(status>encashment)&(status>reporting)&(status>genral)):\n",
    "            return \"Leave_status\"\n",
    "        elif((specific>status)&(specific>encashment)&(specific>reporting)&(specific>genral)):\n",
    "            subintent = leave_inquiry_specific(name)\n",
    "            return subintent\n",
    "            #here specific work will come\n",
    "        elif((encashment>status)&(specific<encashment)&(encashment>reporting)&(encashment>genral)):\n",
    "            return \"Leave_encashment\"\n",
    "        elif((reporting>status)&(reporting>specific)&(encashment<reporting)&(reporting>genral)):\n",
    "            return \"Leave_reporting\"\n",
    "        else:\n",
    "            return \"Genral_holiday\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'can you please inform me about how many leaves can i take'\n",
    "name = 'do i have sick leaves'\n",
    "name = 'how many sick leaves have left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leave_inquiry_specific(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_inquiry_specific(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        train= train[train['sub_type_two']== 'specific']\n",
    "        text = train['Data']\n",
    "        \n",
    "        leave_update = train[train['TypeLeave'] == 'leave_update']\n",
    "        leave_taken = train[train['TypeLeave'] == 'leave_taken']\n",
    "        \n",
    "       \n",
    "        gen_leave_update = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_update['Data']]\n",
    "        gen_leave_taken = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in leave_taken['Data']]\n",
    "\n",
    "        \n",
    "        dictionary_leave_update = gensim.corpora.Dictionary(gen_leave_update)\n",
    "        dictionary_leave_taken = gensim.corpora.Dictionary(gen_leave_taken)\n",
    "       \n",
    "\n",
    "        \n",
    "        corpus_leave_update = [dictionary_leave_update.doc2bow(gen_leave_update) for gen_leave_update in gen_leave_update]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave_update  = gensim.models.TfidfModel(corpus_leave_update)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave_update = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave_update.txt',tf_idf_leave_update[corpus_leave_update],\n",
    "                                              num_features=len(dictionary_leave_update))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_leave_taken = [dictionary_leave_taken.doc2bow(gen_leave_taken) for gen_leave_taken in gen_leave_taken]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_leave_taken  = gensim.models.TfidfModel(corpus_leave_taken)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_leave_taken = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\leave_taken.txt',tf_idf_leave_taken[corpus_leave_taken],\n",
    "                                              num_features=len(dictionary_leave_taken))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_leave_update.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave_update[query_doc_bow]\n",
    "        leave_update = np.max(sims_leave_update[query_doc_tf_idf])\n",
    "        print(leave_update)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_leave_taken.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_leave_taken[query_doc_bow]\n",
    "        leave_taken = np.max(sims_leave_taken[query_doc_tf_idf])\n",
    "        print(leave_taken)\n",
    "        \n",
    "\n",
    "       \n",
    "        if(leave_update > leave_taken):\n",
    "            sub_type = \"leave_update\"\n",
    "        else:\n",
    "            sub_type = \"leave_taken\"\n",
    "  \n",
    "        return(sub_type)\n",
    "        \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotional_leave(name):\n",
    "        query = name\n",
    "        train= pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "        sick = train[train[\"sub_type_two\"] == 'sick']\n",
    "        casual = train[train[\"sub_type_two\"] == 'casual']\n",
    "\n",
    "\n",
    "        #raw_documents = train['Leave Data Description']\n",
    "        #print(\"Number of sick documents:\",len(sick))\n",
    "        #print(\"Number of casual documents:\",len(casual))\n",
    "        gen_docs_s = [[w.lower() for w in word_tokenize(text)] \n",
    "                for text in sick['Data']]\n",
    "        gen_docs_c = [[w.lower() for w in word_tokenize(text)] \n",
    "                for text in casual['Data']]\n",
    "        dictionary_s = gensim.corpora.Dictionary(gen_docs_s)\n",
    "        dictionary_c = gensim.corpora.Dictionary(gen_docs_c)\n",
    "        corpus_s = [dictionary_s.doc2bow(gen_doc_s) for gen_doc_s in gen_docs_s]\n",
    "        #print(corpus_l)\n",
    "\n",
    "        corpus_c = [dictionary_c.doc2bow(gen_doc_c) for gen_doc_c in gen_docs_c]\n",
    "        #print(corpus_i)\n",
    "        \n",
    "        tf_idf_s = gensim.models.TfidfModel(corpus_s)\n",
    "        tf_idf_c = gensim.models.TfidfModel(corpus_c)\n",
    "        sims_s = gensim.similarities.Similarity('D:\\\\gensim\\\\sick.txt',tf_idf_s[corpus_s],\n",
    "                                          num_features=len(dictionary_s))\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #print(query_doc)\n",
    "        query_doc_bow = dictionary_s.doc2bow(query_doc)\n",
    "        #print(query_doc_bow)\n",
    "        query_doc_tf_idf = tf_idf_s[query_doc_bow]\n",
    "        sick = np.max(sims_s[query_doc_tf_idf])\n",
    "        print(\"sick:\",np.max(sims_s[query_doc_tf_idf]))\n",
    "        sims_c = gensim.similarities.Similarity('D:\\gensim\\\\casual.txt',tf_idf_c[corpus_c],\n",
    "                                          num_features=len(dictionary_c))\n",
    "        query_doc_bow = dictionary_c.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_c[query_doc_bow]\n",
    "        casual = np.max(sims_c[query_doc_tf_idf])\n",
    "        print(\"Casual:\",np.max(sims_c[query_doc_tf_idf]))\n",
    "        if(casual>sick):\n",
    "            return('CasualLeave')\n",
    "        else:\n",
    "            return('Indirect_SickLeave')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_approval(name):\n",
    "        query = name\n",
    "        sub_type = ''\n",
    "        train = pd.read_excel(\"leave_final.xlsx\")\n",
    "        train = train[train['Type'] == 'Leave_Approval']\n",
    "        text = train['Data']\n",
    "        approve = train[train['sub_type_two'] == 'approve']\n",
    "        reject = train[train['sub_type_two'] == 'reject']\n",
    "       \n",
    "        gen_approve = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in approve['Data']]\n",
    "        gen_reject = [[w.lower() for w in word_tokenize(text)] \n",
    "                    for text in reject['Data']]\n",
    "\n",
    "        \n",
    "        dictionary_approve = gensim.corpora.Dictionary(gen_approve)\n",
    "        dictionary_reject = gensim.corpora.Dictionary(gen_reject)\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_approve = [dictionary_approve.doc2bow(gen_approve) for gen_approve in gen_approve]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_approve  = gensim.models.TfidfModel(corpus_approve)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_approve = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\approve.txt',tf_idf_approve[corpus_approve],\n",
    "                                              num_features=len(dictionary_approve))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        corpus_reject = [dictionary_reject.doc2bow(gen_reject) for gen_reject in gen_reject]\n",
    "        #print(corpus_leave)\n",
    "        tf_idf_reject  = gensim.models.TfidfModel(corpus_reject)\n",
    "        #print(tf_idf_leave)\n",
    "        sims_reject = gensim.similarities.Similarity('C:\\\\Users\\\\shiza.abid\\\\Desktop\\\\reject.txt',tf_idf_reject[corpus_reject],\n",
    "                                              num_features=len(dictionary_reject))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        query = name\n",
    "        query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "        #for approval\n",
    "        query_doc_bow = dictionary_approve.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_approve[query_doc_bow]\n",
    "        approve = np.max(sims_approve[query_doc_tf_idf])\n",
    "        print(approve)\n",
    "\n",
    "        #for reject\n",
    "        query_doc_bow = dictionary_reject.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf_reject[query_doc_bow]\n",
    "        reject = np.max(sims_reject[query_doc_tf_idf])\n",
    "        print(reject)\n",
    "\n",
    "       \n",
    "        if(approve > reject):\n",
    "            sub_type = sub_type + \"approve\"\n",
    "        else:\n",
    "            sub_type = sub_type + \"reject\"\n",
    "\n",
    "        return(sub_type)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEAVE ENCASHMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_type = ''\n",
    "train = pd.read_excel(\"D:\\\\virtual\\\\leave_final.xlsx\")\n",
    "leave_encashment = train[train['TypeLeave'] == 'leave_encashment']\n",
    "leave_encashment_policy = train[train['TypeLeave'] == 'leave_encashment_policy']\n",
    "leave_encashment_procedure = train[train['TypeLeave'] == 'leave_encashment_procedure']\n",
    "    \n",
    "gen_docs_leave_encashment = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_encashment['Data']]\n",
    "gen_docs_leave_encashment_policy = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_encashment_policy['Data']]\n",
    "gen_docs_leave_encashment_procedure = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in leave_encashment_procedure['Data']]\n",
    "    \n",
    "dictionary_leave_encashment  = gensim.corpora.Dictionary(gen_docs_leave_encashment )\n",
    "dictionary_leave_encashment_policy = gensim.corpora.Dictionary(gen_docs_leave_encashment_policy)\n",
    "dictionary_leave_encashment_procedure = gensim.corpora.Dictionary(gen_docs_leave_encashment_procedure)\n",
    "    \n",
    "corpus_leave_encashment = [dictionary_leave_encashment.doc2bow(gen_doc_leave_encashment) for gen_doc_leave_encashment in gen_docs_leave_encashment]\n",
    "corpus_leave_encashment_policy = [dictionary_leave_encashment_policy.doc2bow(gen_doc_leave_encashment_policy) for gen_doc_leave_encashment_policy in gen_docs_leave_encashment_policy]\n",
    "corpus_leave_encashment_procedure = [dictionary_leave_encashment_procedure.doc2bow(gen_doc_leave_encashment_procedure) for gen_doc_leave_encashment_procedure in gen_docs_leave_encashment_procedure]\n",
    "\n",
    "tf_idf_leave_encashment = gensim.models.TfidfModel(corpus_leave_encashment)\n",
    "tf_idf_leave_encashment_policy = gensim.models.TfidfModel(corpus_leave_encashment_policy)\n",
    "tf_idf_leave_encashment_procedure = gensim.models.TfidfModel(corpus_leave_encashment_procedure )\n",
    "    \n",
    "sims_leave_encashment = gensim.similarities.Similarity('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\Gensim_data\\\\leave_encashment.txt',tf_idf_leave_encashment[corpus_leave_encashment],\n",
    "                                num_features=len(dictionary_leave_encashment))\n",
    "sims_leave_encashment_policy = gensim.similarities.Similarity('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\Gensim_data\\\\leave_encashment_policy.txt',tf_idf_leave_encashment_policy[corpus_leave_encashment_policy],\n",
    "                                num_features=len(dictionary_leave_encashment_policy))\n",
    "sims_leave_encashment_procedure = gensim.similarities.Similarity('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\Gensim_data\\\\leave_encashment_procedure.txt',tf_idf_leave_encashment_procedure[corpus_leave_encashment_procedure],\n",
    "                                num_features=len(dictionary_leave_encashment_procedure))\n",
    "    \n",
    "leave_encashment_gensim={1:sims_leave_encashment,2:tf_idf_leave_encashment,3:dictionary_leave_encashment,4:sims_leave_encashment_policy,5:tf_idf_leave_encashment_policy,6:dictionary_leave_encashment_policy,7:sims_leave_encashment_procedure,8:tf_idf_leave_encashment_procedure,9:dictionary_leave_encashment_procedure}\n",
    "\n",
    "name = 'how can i encash my leaves'\n",
    "name = 'inform me how can i encash my leaves'\n",
    "name = 'what is leave encashment'\n",
    "name = 'tell me about leave encashment'\n",
    "leave_encashment_classification(name,leave_encashment_gensim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_encashment_classification(name,leave_encashment_gensim):\n",
    "    query = name\n",
    "    sims_leave_encashment = leave_encashment_gensim[1]\n",
    "    tf_idf_leave_encashment  = leave_encashment_gensim[2]\n",
    "    dictionary_leave_encashment = leave_encashment_gensim[3]\n",
    "    sims_leave_encashment_policy = leave_encashment_gensim[4]\n",
    "    tf_idf_leave_encashment_policy = leave_encashment_gensim[5]\n",
    "    dictionary_leave_encashment_policy = leave_encashment_gensim[6]\n",
    "    sims_leave_encashment_procedure = leave_encashment_gensim[7]\n",
    "    tf_idf_leave_encashment_procedure = leave_encashment_gensim[8]\n",
    "    dictionary_leave_encashment_procedure = leave_encashment_gensim[9]\n",
    "    \n",
    "    query_doc = [w.lower() for w in word_tokenize(query)]\n",
    "    \n",
    "    query_doc_bow = dictionary_leave_encashment.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf_leave_encashment[query_doc_bow]\n",
    "    leave_encashment = np.max(sims_leave_encashment[query_doc_tf_idf])\n",
    "    \n",
    "    query_doc_bow = dictionary_leave_encashment_policy.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf_leave_encashment_policy[query_doc_bow]\n",
    "    leave_encashment_policy = np.max(sims_leave_encashment_policy[query_doc_tf_idf])\n",
    "    \n",
    "    query_doc_bow = dictionary_leave_encashment_procedure.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf_leave_encashment_procedure[query_doc_bow]\n",
    "    leave_encashment_procedure = np.max(sims_leave_encashment_procedure[query_doc_tf_idf])\n",
    "    \n",
    "    if((leave_encashment > leave_encashment_policy)&(leave_encashment > leave_encashment_procedure)):\n",
    "        print('leave_encashment')\n",
    "    elif((leave_encashment_policy>leave_encashment)&(leave_encashment_policy>leave_encashment_procedure)):\n",
    "        print('leave_encashment_policy')\n",
    "    else:\n",
    "        print('leave_encashment_procedure')\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave Encashment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\datasets\\\\leave_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['sub_type_two'] == 'encashment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['TypeLeave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y= to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    #Create model by simply calling the Sequential constructor\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #This layer can only be used as the first layer in a model.\n",
    "    \n",
    "    model.add(Embedding(max_words,50,input_length=max_len))\n",
    "    \n",
    "    #specify number of neurons in lstm layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(LSTM(64, activation='tanh',dropout=0.01))\n",
    "    \n",
    "    #specify number of neurons in dense layer and the activation function that we want to use.\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add dropout to prevent overfitting\n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    #and we create our output layer with two nodes as we have 2 class labels.\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    \n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    #Now for training, we need to define an optimizer, loss measure and the error metric.\n",
    "    # We will use the binary_crossentropy as our loss measure. \n",
    "    #As for the minimization algorithm, we will use \"rmsprop\".\n",
    "    #This optimizer is usually a good choice for recurrent neural networks.\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= model()\n",
    "\n",
    "c.fit(sequences_matrix,y,epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.save('Encashment_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('D:\\\\bot_Django\\\\virtual\\\\virtual_bot\\\\bot\\\\datasets\\\\leave_final.xlsx')\n",
    "train = train[train['sub_type_two'] == 'encashment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "m = load_model('Encashment_Model.h5')\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= train['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in docs:\n",
    "    temp = token_stems(i)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.asarray(tokens) , np.asarray(train['TypeLeave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest= train_test_split(x, y, test_size= 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(x)\n",
    "test_sequences = tok.texts_to_sequences(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len =200\n",
    "\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How many Leave allowance will I get?'\n",
    "query = 'Do i have any leaves that can be encashed'\n",
    "query = 'How much i will get if i cash my leaves'\n",
    "query = 'can i encash my sick leaves '\n",
    "query = 'can my sick leaves be encashed '\n",
    "query = 'can i encash my hajj leaves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is leave encashment policy'\n",
    "query = 'tell me about my leave encashment '\n",
    "query = 'what is leave encashment policy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'can you tell me the procedure of leave encashment'\n",
    "query = 'tell me about encashment of leaves '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = token_stems(sent)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_test = ([list(sen)])\n",
    "sen_sequences = tok.texts_to_sequences(sen_test)\n",
    "sen_sequences_matrix = sequence.pad_sequences(sen_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = m.predict(sen_sequences_matrix)\n",
    "print(score)\n",
    "print(max(score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('leave allowance 0')\n",
    "print('policy 1')\n",
    "print('procedure 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
